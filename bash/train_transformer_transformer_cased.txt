nohup: ignoring input
2022-07-11 17:36:16 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11101
2022-07-11 17:36:16 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11101
2022-07-11 17:36:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-11 17:36:16 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11101
2022-07-11 17:36:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-11 17:36:16 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:11101
2022-07-11 17:36:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-11 17:36:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-11 17:36:16 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-11 17:36:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-11 17:36:16 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-11 17:36:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-11 17:36:16 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-11 17:36:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-11 17:36:16 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-11 17:36:16 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-11 17:36:20 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:11101', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_transformer_transformer_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=False, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='transformer', decoder='transformer', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-11 17:36:20 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-11 17:36:20 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-11 17:36:20 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.source
2022-07-11 17:36:20 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.target
2022-07-11 17:36:20 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 valid source-target 13368 examples
2022-07-11 17:36:20 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-11 17:36:20 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
2022-07-11 17:36:23 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): EditorTransformerEncoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): EditorTransformerDecoder(
    (embed_tokens): Embedding(28996, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
    (embed_mask_ins): Embedding(256, 1536)
    (layers_reposition): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-07-11 17:36:23 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-11 17:36:23 | INFO | fairseq_cli.train | num. model params: 151068672 (num. trained: 151068672)
2022-07-11 17:36:23 | INFO | fairseq_cli.train | num. Encoder model params: 55746816 (Encoder num. trained: 55746816)
2022-07-11 17:36:23 | INFO | fairseq_cli.train | num. Decoder model params: 117590784 (Decoder num. trained: 117590784)
Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 412
Trained parameters: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 292
Trained parameters not adapter: ['encoder.embed_tokens.weight', 'encoder.embed_positions.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']2022-07-11 17:36:23 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-11 17:36:23 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-11 17:36:23 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_transformer_transformer_cased/checkpoint_last.pt
2022-07-11 17:36:23 | INFO | fairseq.trainer | loading train data for epoch 1
2022-07-11 17:36:23 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.source
2022-07-11 17:36:23 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.target
2022-07-11 17:36:23 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 train source-target 287112 examples
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-11 17:37:59 | INFO | train_inner | epoch 001:    100 / 1122 loss=24.782, nll_loss=14.013, mask_ins=7.571, word_ins_ml=14.119, word_reposition=3.091, ppl=2.88403e+07, wps=22717.4, ups=1.11, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=18.738, clip=0, loss_scale=128, train_wall=93, wall=96
2022-07-11 17:39:29 | INFO | train_inner | epoch 001:    200 / 1122 loss=18.669, nll_loss=12.499, mask_ins=4.341, word_ins_ml=12.763, word_reposition=1.566, ppl=416910, wps=22914.8, ups=1.11, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=19.139, clip=0, loss_scale=128, train_wall=89, wall=186
2022-07-11 17:40:58 | INFO | train_inner | epoch 001:    300 / 1122 loss=15.327, nll_loss=11.35, mask_ins=2.198, word_ins_ml=11.772, word_reposition=1.357, ppl=41097, wps=22893.3, ups=1.11, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=3.985, clip=0, loss_scale=128, train_wall=89, wall=276
2022-07-11 17:42:29 | INFO | train_inner | epoch 001:    400 / 1122 loss=14.662, nll_loss=10.835, mask_ins=1.971, word_ins_ml=11.381, word_reposition=1.31, ppl=25929.7, wps=22669.1, ups=1.1, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=1.642, clip=0, loss_scale=128, train_wall=90, wall=366
2022-07-11 17:43:59 | INFO | train_inner | epoch 001:    500 / 1122 loss=14.496, nll_loss=10.752, mask_ins=1.864, word_ins_ml=11.33, word_reposition=1.301, ppl=23098.4, wps=22923, ups=1.12, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=1.583, clip=0, loss_scale=128, train_wall=89, wall=456
2022-07-11 17:45:28 | INFO | train_inner | epoch 001:    600 / 1122 loss=14.445, nll_loss=10.712, mask_ins=1.853, word_ins_ml=11.3, word_reposition=1.292, ppl=22304.8, wps=22953.5, ups=1.12, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=1.263, clip=0, loss_scale=242, train_wall=89, wall=545
2022-07-11 17:46:57 | INFO | train_inner | epoch 001:    700 / 1122 loss=14.388, nll_loss=10.658, mask_ins=1.845, word_ins_ml=11.253, word_reposition=1.29, ppl=21442.4, wps=22981.3, ups=1.12, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=1.461, clip=0, loss_scale=256, train_wall=89, wall=635
2022-07-11 17:48:27 | INFO | train_inner | epoch 001:    800 / 1122 loss=14.326, nll_loss=10.597, mask_ins=1.843, word_ins_ml=11.199, word_reposition=1.284, ppl=20531.3, wps=23033.3, ups=1.12, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=1.482, clip=0, loss_scale=256, train_wall=89, wall=724
2022-07-11 17:49:56 | INFO | train_inner | epoch 001:    900 / 1122 loss=14.261, nll_loss=10.537, mask_ins=1.83, word_ins_ml=11.148, word_reposition=1.283, ppl=19630.2, wps=23007.7, ups=1.12, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=1.374, clip=0, loss_scale=256, train_wall=88, wall=813
2022-07-11 17:51:25 | INFO | train_inner | epoch 001:   1000 / 1122 loss=14.226, nll_loss=10.473, mask_ins=1.851, word_ins_ml=11.094, word_reposition=1.281, ppl=19159.4, wps=23043.5, ups=1.12, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=1.364, clip=0, loss_scale=256, train_wall=89, wall=902
2022-07-11 17:52:55 | INFO | train_inner | epoch 001:   1100 / 1122 loss=14.159, nll_loss=10.41, mask_ins=1.839, word_ins_ml=11.04, word_reposition=1.279, ppl=18290.6, wps=22818, ups=1.11, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=1.251, clip=0, loss_scale=453, train_wall=89, wall=992
2022-07-11 17:53:14 | INFO | train | epoch 001 | loss 15.763 | nll_loss 11.152 | mask_ins 2.622 | word_ins_ml 11.661 | word_reposition 1.481 | ppl 55612.4 | wps 22892.3 | ups 1.12 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 4.782 | clip 0 | loss_scale 220 | train_wall 1002 | wall 1011
2022-07-11 17:53:32 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.579 | nll_loss 10.492 | mask_ins 2.201 | word_ins_ml 11.134 | word_reposition 1.244 | ppl 24471.8 | wps 55249.5 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-07-11 17:53:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 14.579) (writing took 3.2972775297239423 seconds)
2022-07-11 17:54:45 | INFO | train_inner | epoch 002:     78 / 1122 loss=14.111, nll_loss=10.354, mask_ins=1.844, word_ins_ml=10.995, word_reposition=1.272, ppl=17692.3, wps=18471.4, ups=0.91, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=1.425, clip=0, loss_scale=512, train_wall=88, wall=1102
2022-07-11 17:56:14 | INFO | train_inner | epoch 002:    178 / 1122 loss=14.07, nll_loss=10.3, mask_ins=1.846, word_ins_ml=10.949, word_reposition=1.275, ppl=17201.2, wps=23099.8, ups=1.12, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=1.305, clip=0, loss_scale=512, train_wall=89, wall=1191
2022-07-11 17:57:44 | INFO | train_inner | epoch 002:    278 / 1122 loss=14.023, nll_loss=10.252, mask_ins=1.839, word_ins_ml=10.908, word_reposition=1.275, ppl=16647.2, wps=22921.5, ups=1.11, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=1.343, clip=0, loss_scale=512, train_wall=89, wall=1281
2022-07-11 17:59:13 | INFO | train_inner | epoch 002:    378 / 1122 loss=13.948, nll_loss=10.196, mask_ins=1.826, word_ins_ml=10.861, word_reposition=1.261, ppl=15802.9, wps=22852.8, ups=1.12, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=1.44, clip=0, loss_scale=512, train_wall=88, wall=1370
2022-07-11 18:00:42 | INFO | train_inner | epoch 002:    478 / 1122 loss=13.915, nll_loss=10.138, mask_ins=1.839, word_ins_ml=10.812, word_reposition=1.264, ppl=15450.7, wps=23044, ups=1.12, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=1.28, clip=0, loss_scale=845, train_wall=89, wall=1459
2022-07-11 18:02:12 | INFO | train_inner | epoch 002:    578 / 1122 loss=13.859, nll_loss=10.088, mask_ins=1.819, word_ins_ml=10.77, word_reposition=1.269, ppl=14854.7, wps=22808.7, ups=1.11, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=1.323, clip=0, loss_scale=1024, train_wall=89, wall=1549
2022-07-11 18:03:42 | INFO | train_inner | epoch 002:    678 / 1122 loss=13.83, nll_loss=10.034, mask_ins=1.842, word_ins_ml=10.723, word_reposition=1.265, ppl=14560.3, wps=22926.2, ups=1.12, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=1.268, clip=0, loss_scale=1024, train_wall=89, wall=1639
2022-07-11 18:05:11 | INFO | train_inner | epoch 002:    778 / 1122 loss=13.75, nll_loss=9.98, mask_ins=1.815, word_ins_ml=10.677, word_reposition=1.259, ppl=13778.2, wps=22982.5, ups=1.12, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=1.253, clip=0, loss_scale=1024, train_wall=89, wall=1728
2022-07-11 18:06:41 | INFO | train_inner | epoch 002:    878 / 1122 loss=13.763, nll_loss=9.94, mask_ins=1.849, word_ins_ml=10.643, word_reposition=1.272, ppl=13905.5, wps=22813.6, ups=1.12, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=1.354, clip=0, loss_scale=1024, train_wall=89, wall=1818
2022-07-11 18:08:10 | INFO | train_inner | epoch 002:    978 / 1122 loss=13.686, nll_loss=9.891, mask_ins=1.817, word_ins_ml=10.601, word_reposition=1.268, ppl=13184, wps=22948.7, ups=1.12, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=1.238, clip=0, loss_scale=1567, train_wall=89, wall=1907
2022-07-11 18:09:40 | INFO | train_inner | epoch 002:   1078 / 1122 loss=13.671, nll_loss=9.842, mask_ins=1.837, word_ins_ml=10.559, word_reposition=1.275, ppl=13042.1, wps=23036.5, ups=1.11, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=1.167, clip=0, loss_scale=2048, train_wall=89, wall=1997
2022-07-11 18:10:19 | INFO | train | epoch 002 | loss 13.861 | nll_loss 10.075 | mask_ins 1.834 | word_ins_ml 10.758 | word_reposition 1.269 | ppl 14882.6 | wps 22466.8 | ups 1.09 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 1.298 | clip 0 | loss_scale 1015 | train_wall 997 | wall 2036
2022-07-11 18:10:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.881 | nll_loss 9.906 | mask_ins 2.049 | word_ins_ml 10.63 | word_reposition 1.203 | ppl 15090.1 | wps 55534.6 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 13.881
2022-07-11 18:10:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 13.881) (writing took 4.803019245155156 seconds)
2022-07-11 18:11:32 | INFO | train_inner | epoch 003:     56 / 1122 loss=13.644, nll_loss=9.789, mask_ins=1.85, word_ins_ml=10.513, word_reposition=1.281, ppl=12803.7, wps=18157.1, ups=0.89, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=1.327, clip=0, loss_scale=2048, train_wall=89, wall=2109
2022-07-11 18:13:02 | INFO | train_inner | epoch 003:    156 / 1122 loss=13.567, nll_loss=9.721, mask_ins=1.827, word_ins_ml=10.455, word_reposition=1.285, ppl=12133.3, wps=22884.7, ups=1.12, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=1.269, clip=0, loss_scale=2048, train_wall=89, wall=2199
2022-07-11 18:14:31 | INFO | train_inner | epoch 003:    256 / 1122 loss=13.561, nll_loss=9.669, mask_ins=1.822, word_ins_ml=10.41, word_reposition=1.328, ppl=12085.3, wps=22939.5, ups=1.11, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=1.285, clip=0, loss_scale=2048, train_wall=89, wall=2289
2022-07-11 18:16:01 | INFO | train_inner | epoch 003:    356 / 1122 loss=13.521, nll_loss=9.597, mask_ins=1.832, word_ins_ml=10.348, word_reposition=1.34, ppl=11753.6, wps=22827, ups=1.11, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=1.374, clip=0, loss_scale=2888, train_wall=89, wall=2379
2022-07-11 18:17:31 | INFO | train_inner | epoch 003:    456 / 1122 loss=13.46, nll_loss=9.536, mask_ins=1.81, word_ins_ml=10.296, word_reposition=1.354, ppl=11266.1, wps=22747, ups=1.12, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=1.289, clip=0, loss_scale=4096, train_wall=89, wall=2468
2022-07-11 18:19:00 | INFO | train_inner | epoch 003:    556 / 1122 loss=13.433, nll_loss=9.488, mask_ins=1.821, word_ins_ml=10.255, word_reposition=1.357, ppl=11060.5, wps=22931.1, ups=1.12, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=1.326, clip=0, loss_scale=4096, train_wall=89, wall=2558
2022-07-11 18:20:32 | INFO | train_inner | epoch 003:    656 / 1122 loss=13.415, nll_loss=9.433, mask_ins=1.828, word_ins_ml=10.209, word_reposition=1.379, ppl=10924.8, wps=22609.3, ups=1.1, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=1.422, clip=0, loss_scale=4096, train_wall=91, wall=2649
2022-07-11 18:22:01 | INFO | train_inner | epoch 003:    756 / 1122 loss=13.355, nll_loss=9.387, mask_ins=1.811, word_ins_ml=10.169, word_reposition=1.375, ppl=10474.3, wps=22999.2, ups=1.12, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=1.447, clip=0, loss_scale=4096, train_wall=89, wall=2738
2022-07-11 18:23:30 | INFO | train_inner | epoch 003:    856 / 1122 loss=13.324, nll_loss=9.341, mask_ins=1.82, word_ins_ml=10.131, word_reposition=1.373, ppl=10253.3, wps=23078.4, ups=1.12, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=1.62, clip=0, loss_scale=5284, train_wall=89, wall=2828
2022-07-11 18:25:00 | INFO | train_inner | epoch 003:    956 / 1122 loss=13.307, nll_loss=9.301, mask_ins=1.824, word_ins_ml=10.097, word_reposition=1.386, ppl=10132.6, wps=22924.8, ups=1.11, wpb=20572.9, bsz=256, num_updates=3200, lr=0.000320036, gnorm=1.53, clip=0, loss_scale=8192, train_wall=89, wall=2917
2022-07-11 18:26:30 | INFO | train_inner | epoch 003:   1056 / 1122 loss=13.264, nll_loss=9.262, mask_ins=1.817, word_ins_ml=10.064, word_reposition=1.383, ppl=9839.12, wps=22943.8, ups=1.12, wpb=20512.4, bsz=256, num_updates=3300, lr=0.000330034, gnorm=1.542, clip=0, loss_scale=8192, train_wall=89, wall=3007
2022-07-11 18:27:28 | INFO | train | epoch 003 | loss 13.422 | nll_loss 9.475 | mask_ins 1.823 | word_ins_ml 10.244 | word_reposition 1.354 | ppl 10972.3 | wps 22366.8 | ups 1.09 | wpb 20521.3 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 1.429 | clip 0 | loss_scale 4598 | train_wall 1000 | wall 3066
2022-07-11 18:27:46 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.659 | nll_loss 9.44 | mask_ins 2.026 | word_ins_ml 10.24 | word_reposition 1.394 | ppl 12938 | wps 55573.7 | wpb 2367.6 | bsz 32 | num_updates 3366 | best_loss 13.659
2022-07-11 18:27:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 3 @ 3366 updates, score 13.659) (writing took 5.155223625712097 seconds)
2022-07-11 18:28:22 | INFO | train_inner | epoch 004:     34 / 1122 loss=13.234, nll_loss=9.213, mask_ins=1.816, word_ins_ml=10.022, word_reposition=1.397, ppl=9633.51, wps=18096.3, ups=0.89, wpb=20354.1, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=1.635, clip=0, loss_scale=8192, train_wall=89, wall=3119
2022-07-11 18:29:52 | INFO | train_inner | epoch 004:    134 / 1122 loss=13.154, nll_loss=9.124, mask_ins=1.814, word_ins_ml=9.946, word_reposition=1.393, ppl=9111.79, wps=22838.7, ups=1.12, wpb=20472.6, bsz=256, num_updates=3500, lr=0.00035003, gnorm=1.612, clip=0, loss_scale=8192, train_wall=89, wall=3209
2022-07-11 18:31:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-11 18:31:22 | INFO | train_inner | epoch 004:    235 / 1122 loss=13.134, nll_loss=9.097, mask_ins=1.808, word_ins_ml=9.924, word_reposition=1.403, ppl=8988.41, wps=22772.6, ups=1.1, wpb=20615.2, bsz=256, num_updates=3600, lr=0.000360028, gnorm=1.516, clip=0, loss_scale=8516, train_wall=90, wall=3299
2022-07-11 18:32:52 | INFO | train_inner | epoch 004:    335 / 1122 loss=13.102, nll_loss=9.056, mask_ins=1.811, word_ins_ml=9.89, word_reposition=1.401, ppl=8792.38, wps=22890.3, ups=1.12, wpb=20432.2, bsz=256, num_updates=3700, lr=0.000370026, gnorm=1.495, clip=0, loss_scale=8192, train_wall=89, wall=3389
2022-07-11 18:34:21 | INFO | train_inner | epoch 004:    435 / 1122 loss=13.078, nll_loss=9.028, mask_ins=1.805, word_ins_ml=9.867, word_reposition=1.406, ppl=8648.29, wps=23056.7, ups=1.12, wpb=20656.5, bsz=256, num_updates=3800, lr=0.000380024, gnorm=1.448, clip=0, loss_scale=8192, train_wall=89, wall=3478
2022-07-11 18:35:50 | INFO | train_inner | epoch 004:    535 / 1122 loss=13.048, nll_loss=8.979, mask_ins=1.814, word_ins_ml=9.826, word_reposition=1.408, ppl=8467.94, wps=22911.4, ups=1.12, wpb=20481.9, bsz=256, num_updates=3900, lr=0.000390022, gnorm=1.577, clip=0, loss_scale=8192, train_wall=89, wall=3568
2022-07-11 18:37:20 | INFO | train_inner | epoch 004:    635 / 1122 loss=13.032, nll_loss=8.951, mask_ins=1.807, word_ins_ml=9.802, word_reposition=1.423, ppl=8374.85, wps=22934.1, ups=1.12, wpb=20519.3, bsz=256, num_updates=4000, lr=0.00040002, gnorm=1.492, clip=0, loss_scale=8192, train_wall=89, wall=3657
2022-07-11 18:38:49 | INFO | train_inner | epoch 004:    735 / 1122 loss=12.974, nll_loss=8.894, mask_ins=1.806, word_ins_ml=9.753, word_reposition=1.414, ppl=8043.64, wps=23024.9, ups=1.12, wpb=20609.6, bsz=256, num_updates=4100, lr=0.000410018, gnorm=1.647, clip=0, loss_scale=8356, train_wall=89, wall=3747
2022-07-11 18:39:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-11 18:39:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 18:40:21 | INFO | train_inner | epoch 004:    837 / 1122 loss=12.936, nll_loss=8.837, mask_ins=1.8, word_ins_ml=9.704, word_reposition=1.432, ppl=7837.84, wps=22474.1, ups=1.1, wpb=20464.2, bsz=256, num_updates=4200, lr=0.000420016, gnorm=1.659, clip=0, loss_scale=10280, train_wall=90, wall=3838
2022-07-11 18:41:50 | INFO | train_inner | epoch 004:    937 / 1122 loss=12.884, nll_loss=8.761, mask_ins=1.792, word_ins_ml=9.639, word_reposition=1.453, ppl=7559.4, wps=23025.9, ups=1.12, wpb=20636.4, bsz=256, num_updates=4300, lr=0.000430014, gnorm=1.711, clip=0, loss_scale=4096, train_wall=89, wall=3927
2022-07-11 18:43:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 18:43:21 | INFO | train_inner | epoch 004:   1038 / 1122 loss=12.833, nll_loss=8.701, mask_ins=1.79, word_ins_ml=9.587, word_reposition=1.456, ppl=7297.26, wps=22541.1, ups=1.1, wpb=20449.8, bsz=256, num_updates=4400, lr=0.000440012, gnorm=2.267, clip=0, loss_scale=3670, train_wall=90, wall=4018
2022-07-11 18:43:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 18:43:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 18:44:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-11 18:44:36 | INFO | train | epoch 004 | loss 13.011 | nll_loss 8.932 | mask_ins 1.804 | word_ins_ml 9.784 | word_reposition 1.423 | ppl 8255.79 | wps 22274.5 | ups 1.09 | wpb 20518.6 | bsz 255.8 | num_updates 4481 | lr 0.00044811 | gnorm 1.733 | clip 0 | loss_scale 7081 | train_wall 997 | wall 4093
2022-07-11 18:44:53 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 13.23 | nll_loss 8.781 | mask_ins 2.023 | word_ins_ml 9.692 | word_reposition 1.515 | ppl 9605.75 | wps 55321.6 | wpb 2367.6 | bsz 32 | num_updates 4481 | best_loss 13.23
2022-07-11 18:44:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 4 @ 4481 updates, score 13.23) (writing took 4.76167674548924 seconds)
2022-07-11 18:45:15 | INFO | train_inner | epoch 005:     19 / 1122 loss=12.835, nll_loss=8.665, mask_ins=1.797, word_ins_ml=9.557, word_reposition=1.48, ppl=7305.49, wps=17814.6, ups=0.87, wpb=20399.2, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.908, clip=0, loss_scale=490, train_wall=91, wall=4133
2022-07-11 18:46:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-11 18:46:46 | INFO | train_inner | epoch 005:    120 / 1122 loss=12.735, nll_loss=8.528, mask_ins=1.792, word_ins_ml=9.438, word_reposition=1.505, ppl=6815.36, wps=22805.8, ups=1.11, wpb=20592.3, bsz=256, num_updates=4600, lr=0.000460008, gnorm=4.704, clip=4, loss_scale=212, train_wall=90, wall=4223
2022-07-11 18:48:15 | INFO | train_inner | epoch 005:    220 / 1122 loss=12.649, nll_loss=8.428, mask_ins=1.778, word_ins_ml=9.353, word_reposition=1.518, ppl=6424.23, wps=22923.6, ups=1.12, wpb=20517.5, bsz=256, num_updates=4700, lr=0.000470006, gnorm=3.177, clip=2, loss_scale=128, train_wall=89, wall=4312
2022-07-11 18:49:45 | INFO | train_inner | epoch 005:    320 / 1122 loss=12.52, nll_loss=8.259, mask_ins=1.786, word_ins_ml=9.204, word_reposition=1.529, ppl=5871.53, wps=23025.4, ups=1.12, wpb=20598.7, bsz=256, num_updates=4800, lr=0.000480004, gnorm=1.846, clip=0, loss_scale=128, train_wall=89, wall=4402
2022-07-11 18:51:14 | INFO | train_inner | epoch 005:    420 / 1122 loss=12.404, nll_loss=8.154, mask_ins=1.756, word_ins_ml=9.113, word_reposition=1.535, ppl=5417.99, wps=22874.4, ups=1.12, wpb=20457, bsz=256, num_updates=4900, lr=0.000490002, gnorm=2.172, clip=0, loss_scale=128, train_wall=89, wall=4491
2022-07-11 18:52:43 | INFO | train_inner | epoch 005:    520 / 1122 loss=12.286, nll_loss=8.022, mask_ins=1.744, word_ins_ml=9, word_reposition=1.542, ppl=4994.7, wps=22783.8, ups=1.12, wpb=20366.6, bsz=256, num_updates=5000, lr=0.0005, gnorm=1.898, clip=0, loss_scale=128, train_wall=89, wall=4581
2022-07-11 18:54:13 | INFO | train_inner | epoch 005:    620 / 1122 loss=12.17, nll_loss=7.903, mask_ins=1.718, word_ins_ml=8.896, word_reposition=1.557, ppl=4608.71, wps=22929.6, ups=1.12, wpb=20518.5, bsz=256, num_updates=5100, lr=0.000495074, gnorm=1.986, clip=0, loss_scale=157, train_wall=89, wall=4670
2022-07-11 18:55:42 | INFO | train_inner | epoch 005:    720 / 1122 loss=12.099, nll_loss=7.836, mask_ins=1.718, word_ins_ml=8.839, word_reposition=1.542, ppl=4386.12, wps=23026.1, ups=1.12, wpb=20548.1, bsz=256, num_updates=5200, lr=0.00049029, gnorm=2.24, clip=0, loss_scale=256, train_wall=89, wall=4759
2022-07-11 18:57:12 | INFO | train_inner | epoch 005:    820 / 1122 loss=12.007, nll_loss=7.731, mask_ins=1.704, word_ins_ml=8.747, word_reposition=1.555, ppl=4115.23, wps=23102.3, ups=1.12, wpb=20632.4, bsz=256, num_updates=5300, lr=0.000485643, gnorm=2.331, clip=0, loss_scale=256, train_wall=89, wall=4849
2022-07-11 18:58:41 | INFO | train_inner | epoch 005:    920 / 1122 loss=11.9, nll_loss=7.627, mask_ins=1.696, word_ins_ml=8.658, word_reposition=1.547, ppl=3822.66, wps=23036.2, ups=1.12, wpb=20565.6, bsz=256, num_updates=5400, lr=0.000481125, gnorm=2.086, clip=0, loss_scale=256, train_wall=89, wall=4938
2022-07-11 19:00:10 | INFO | train_inner | epoch 005:   1020 / 1122 loss=11.862, nll_loss=7.6, mask_ins=1.683, word_ins_ml=8.636, word_reposition=1.543, ppl=3722.82, wps=22931, ups=1.12, wpb=20449.2, bsz=256, num_updates=5500, lr=0.000476731, gnorm=2.951, clip=0, loss_scale=256, train_wall=89, wall=5027
2022-07-11 19:01:40 | INFO | train_inner | epoch 005:   1120 / 1122 loss=12.006, nll_loss=7.728, mask_ins=1.689, word_ins_ml=8.75, word_reposition=1.567, ppl=4113.59, wps=22954.7, ups=1.12, wpb=20576.3, bsz=256, num_updates=5600, lr=0.000472456, gnorm=6.301, clip=3, loss_scale=284, train_wall=89, wall=5117
2022-07-11 19:01:41 | INFO | train | epoch 005 | loss 12.249 | nll_loss 7.993 | mask_ins 1.734 | word_ins_ml 8.975 | word_reposition 1.539 | ppl 4866.71 | wps 22428.1 | ups 1.09 | wpb 20519.2 | bsz 255.8 | num_updates 5602 | lr 0.000472371 | gnorm 2.891 | clip 0.8 | loss_scale 201 | train_wall 996 | wall 5118
2022-07-11 19:01:59 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 12.285 | nll_loss 7.854 | mask_ins 1.773 | word_ins_ml 8.917 | word_reposition 1.595 | ppl 4992.24 | wps 55668.7 | wpb 2367.6 | bsz 32 | num_updates 5602 | best_loss 12.285
2022-07-11 19:02:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 5 @ 5602 updates, score 12.285) (writing took 4.826501002535224 seconds)
2022-07-11 19:03:32 | INFO | train_inner | epoch 006:     98 / 1122 loss=11.684, nll_loss=7.421, mask_ins=1.664, word_ins_ml=8.48, word_reposition=1.541, ppl=3291.3, wps=18145.6, ups=0.89, wpb=20368.7, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=2.422, clip=0, loss_scale=512, train_wall=89, wall=5229
2022-07-11 19:05:01 | INFO | train_inner | epoch 006:    198 / 1122 loss=11.526, nll_loss=7.266, mask_ins=1.649, word_ins_ml=8.344, word_reposition=1.533, ppl=2949.57, wps=23088.1, ups=1.12, wpb=20577.1, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.69, clip=0, loss_scale=512, train_wall=89, wall=5318
2022-07-11 19:06:31 | INFO | train_inner | epoch 006:    298 / 1122 loss=11.421, nll_loss=7.139, mask_ins=1.652, word_ins_ml=8.235, word_reposition=1.535, ppl=2742.57, wps=23008.8, ups=1.12, wpb=20619.6, bsz=256, num_updates=5900, lr=0.000460287, gnorm=1.57, clip=0, loss_scale=512, train_wall=89, wall=5408
2022-07-11 19:08:01 | INFO | train_inner | epoch 006:    398 / 1122 loss=11.318, nll_loss=7.039, mask_ins=1.645, word_ins_ml=8.148, word_reposition=1.526, ppl=2553.61, wps=22774.1, ups=1.11, wpb=20571.1, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.496, clip=0, loss_scale=512, train_wall=90, wall=5498
2022-07-11 19:09:31 | INFO | train_inner | epoch 006:    498 / 1122 loss=11.22, nll_loss=6.924, mask_ins=1.658, word_ins_ml=8.049, word_reposition=1.512, ppl=2384.79, wps=22819, ups=1.11, wpb=20488.9, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.561, clip=0, loss_scale=512, train_wall=89, wall=5588
2022-07-11 19:11:00 | INFO | train_inner | epoch 006:    598 / 1122 loss=11.111, nll_loss=6.838, mask_ins=1.631, word_ins_ml=7.975, word_reposition=1.505, ppl=2212.23, wps=22799.7, ups=1.12, wpb=20356.2, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.593, clip=0, loss_scale=1019, train_wall=89, wall=5677
2022-07-11 19:12:30 | INFO | train_inner | epoch 006:    698 / 1122 loss=11.058, nll_loss=6.765, mask_ins=1.648, word_ins_ml=7.911, word_reposition=1.498, ppl=2131.6, wps=22917.8, ups=1.11, wpb=20667.1, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.648, clip=0, loss_scale=1024, train_wall=90, wall=5767
2022-07-11 19:13:59 | INFO | train_inner | epoch 006:    798 / 1122 loss=10.927, nll_loss=6.675, mask_ins=1.629, word_ins_ml=7.832, word_reposition=1.466, ppl=1947.26, wps=22962, ups=1.12, wpb=20483.8, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.971, clip=0, loss_scale=1024, train_wall=89, wall=5857
2022-07-11 19:15:29 | INFO | train_inner | epoch 006:    898 / 1122 loss=10.868, nll_loss=6.617, mask_ins=1.63, word_ins_ml=7.783, word_reposition=1.455, ppl=1868.7, wps=23078.8, ups=1.12, wpb=20613.2, bsz=256, num_updates=6500, lr=0.000438529, gnorm=2.627, clip=0, loss_scale=1024, train_wall=89, wall=5946
2022-07-11 19:15:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 19:15:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-11 19:17:00 | INFO | train_inner | epoch 006:   1000 / 1122 loss=10.823, nll_loss=6.586, mask_ins=1.625, word_ins_ml=7.756, word_reposition=1.442, ppl=1811.05, wps=22526.4, ups=1.1, wpb=20518, bsz=256, num_updates=6600, lr=0.000435194, gnorm=4.129, clip=1, loss_scale=289, train_wall=90, wall=6037
2022-07-11 19:18:29 | INFO | train_inner | epoch 006:   1100 / 1122 loss=10.743, nll_loss=6.474, mask_ins=1.625, word_ins_ml=7.658, word_reposition=1.459, ppl=1713.43, wps=23015.2, ups=1.12, wpb=20493.9, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.776, clip=0, loss_scale=256, train_wall=88, wall=6126
2022-07-11 19:18:48 | INFO | train | epoch 006 | loss 11.144 | nll_loss 6.877 | mask_ins 1.641 | word_ins_ml 8.007 | word_reposition 1.496 | ppl 2263.54 | wps 22380.8 | ups 1.09 | wpb 20520.8 | bsz 255.8 | num_updates 6722 | lr 0.000431227 | gnorm 2.029 | clip 0.1 | loss_scale 646 | train_wall 997 | wall 6145
2022-07-11 19:19:06 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.739 | nll_loss 7.33 | mask_ins 1.726 | word_ins_ml 8.522 | word_reposition 1.491 | ppl 3419.13 | wps 55411.7 | wpb 2367.6 | bsz 32 | num_updates 6722 | best_loss 11.739
2022-07-11 19:19:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 6 @ 6722 updates, score 11.739) (writing took 4.918613497167826 seconds)
2022-07-11 19:20:21 | INFO | train_inner | epoch 007:     78 / 1122 loss=10.642, nll_loss=6.394, mask_ins=1.611, word_ins_ml=7.589, word_reposition=1.442, ppl=1598.05, wps=18161.5, ups=0.89, wpb=20319.7, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=2.007, clip=0, loss_scale=256, train_wall=88, wall=6238
2022-07-11 19:21:52 | INFO | train_inner | epoch 007:    178 / 1122 loss=10.578, nll_loss=6.313, mask_ins=1.618, word_ins_ml=7.521, word_reposition=1.439, ppl=1528.21, wps=22666, ups=1.1, wpb=20608, bsz=256, num_updates=6900, lr=0.000425628, gnorm=2.377, clip=0, loss_scale=256, train_wall=90, wall=6329
2022-07-11 19:22:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-11 19:23:22 | INFO | train_inner | epoch 007:    279 / 1122 loss=10.643, nll_loss=6.44, mask_ins=1.611, word_ins_ml=7.633, word_reposition=1.399, ppl=1599.01, wps=22853.3, ups=1.11, wpb=20633.4, bsz=256, num_updates=7000, lr=0.000422577, gnorm=7.91, clip=4, loss_scale=160, train_wall=90, wall=6419
2022-07-11 19:24:51 | INFO | train_inner | epoch 007:    379 / 1122 loss=10.474, nll_loss=6.241, mask_ins=1.608, word_ins_ml=7.458, word_reposition=1.407, ppl=1422.41, wps=22961.2, ups=1.12, wpb=20525.2, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.682, clip=0, loss_scale=128, train_wall=89, wall=6508
2022-07-11 19:26:21 | INFO | train_inner | epoch 007:    479 / 1122 loss=10.387, nll_loss=6.174, mask_ins=1.591, word_ins_ml=7.4, word_reposition=1.396, ppl=1338.7, wps=22859.9, ups=1.12, wpb=20490.5, bsz=256, num_updates=7200, lr=0.000416667, gnorm=2.525, clip=0, loss_scale=128, train_wall=89, wall=6598
2022-07-11 19:27:52 | INFO | train_inner | epoch 007:    579 / 1122 loss=10.349, nll_loss=6.126, mask_ins=1.604, word_ins_ml=7.358, word_reposition=1.387, ppl=1304.26, wps=22523.3, ups=1.1, wpb=20480.8, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.796, clip=0, loss_scale=128, train_wall=90, wall=6689
2022-07-11 19:29:22 | INFO | train_inner | epoch 007:    679 / 1122 loss=10.317, nll_loss=6.09, mask_ins=1.61, word_ins_ml=7.327, word_reposition=1.379, ppl=1275.66, wps=22651.8, ups=1.1, wpb=20503, bsz=256, num_updates=7400, lr=0.000410997, gnorm=2.18, clip=0, loss_scale=128, train_wall=90, wall=6780
2022-07-11 19:30:54 | INFO | train_inner | epoch 007:    779 / 1122 loss=10.285, nll_loss=6.033, mask_ins=1.619, word_ins_ml=7.277, word_reposition=1.388, ppl=1247.48, wps=22536.2, ups=1.1, wpb=20572.4, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.952, clip=0, loss_scale=210, train_wall=91, wall=6871
2022-07-11 19:32:26 | INFO | train_inner | epoch 007:    879 / 1122 loss=10.231, nll_loss=6.015, mask_ins=1.592, word_ins_ml=7.262, word_reposition=1.377, ppl=1201.81, wps=22361.5, ups=1.09, wpb=20534.3, bsz=256, num_updates=7600, lr=0.000405554, gnorm=2.191, clip=0, loss_scale=256, train_wall=91, wall=6963
2022-07-11 19:33:55 | INFO | train_inner | epoch 007:    979 / 1122 loss=10.183, nll_loss=5.945, mask_ins=1.617, word_ins_ml=7.201, word_reposition=1.366, ppl=1162.67, wps=22942.7, ups=1.11, wpb=20579.1, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.944, clip=0, loss_scale=256, train_wall=89, wall=7052
2022-07-11 19:35:26 | INFO | train_inner | epoch 007:   1079 / 1122 loss=10.1, nll_loss=5.883, mask_ins=1.596, word_ins_ml=7.145, word_reposition=1.358, ppl=1097.29, wps=22710.2, ups=1.11, wpb=20520, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.589, clip=0, loss_scale=256, train_wall=90, wall=7143
2022-07-11 19:36:04 | INFO | train | epoch 007 | loss 10.363 | nll_loss 6.132 | mask_ins 1.607 | word_ins_ml 7.363 | word_reposition 1.392 | ppl 1316.74 | wps 22213.2 | ups 1.08 | wpb 20519.8 | bsz 255.8 | num_updates 7843 | lr 0.000399221 | gnorm 2.545 | clip 0.4 | loss_scale 198 | train_wall 1006 | wall 7181
2022-07-11 19:36:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.306 | nll_loss 7.001 | mask_ins 1.682 | word_ins_ml 8.25 | word_reposition 1.374 | ppl 2531.37 | wps 55551.5 | wpb 2367.6 | bsz 32 | num_updates 7843 | best_loss 11.306
2022-07-11 19:36:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 7 @ 7843 updates, score 11.306) (writing took 4.7558180103078485 seconds)
2022-07-11 19:37:18 | INFO | train_inner | epoch 008:     57 / 1122 loss=10.08, nll_loss=5.83, mask_ins=1.611, word_ins_ml=7.101, word_reposition=1.368, ppl=1082.64, wps=18121.4, ups=0.89, wpb=20401.9, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.772, clip=0, loss_scale=256, train_wall=89, wall=7255
2022-07-11 19:38:47 | INFO | train_inner | epoch 008:    157 / 1122 loss=9.902, nll_loss=5.685, mask_ins=1.586, word_ins_ml=6.973, word_reposition=1.344, ppl=956.93, wps=23001.2, ups=1.12, wpb=20507.8, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.619, clip=0, loss_scale=389, train_wall=89, wall=7344
2022-07-11 19:40:17 | INFO | train_inner | epoch 008:    257 / 1122 loss=9.911, nll_loss=5.689, mask_ins=1.591, word_ins_ml=6.977, word_reposition=1.343, ppl=963.03, wps=22764.9, ups=1.11, wpb=20494.3, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.883, clip=0, loss_scale=512, train_wall=89, wall=7434
2022-07-11 19:41:48 | INFO | train_inner | epoch 008:    357 / 1122 loss=9.854, nll_loss=5.639, mask_ins=1.578, word_ins_ml=6.934, word_reposition=1.343, ppl=925.6, wps=22731.1, ups=1.11, wpb=20562.2, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.709, clip=0, loss_scale=512, train_wall=90, wall=7525
2022-07-11 19:43:17 | INFO | train_inner | epoch 008:    457 / 1122 loss=9.811, nll_loss=5.6, mask_ins=1.577, word_ins_ml=6.899, word_reposition=1.335, ppl=898.28, wps=22996, ups=1.12, wpb=20542.7, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.601, clip=0, loss_scale=512, train_wall=89, wall=7614
2022-07-11 19:44:46 | INFO | train_inner | epoch 008:    557 / 1122 loss=9.788, nll_loss=5.564, mask_ins=1.585, word_ins_ml=6.867, word_reposition=1.336, ppl=884.33, wps=23048.4, ups=1.12, wpb=20578.9, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.655, clip=0, loss_scale=512, train_wall=89, wall=7704
2022-07-11 19:46:16 | INFO | train_inner | epoch 008:    657 / 1122 loss=9.718, nll_loss=5.526, mask_ins=1.568, word_ins_ml=6.834, word_reposition=1.316, ppl=841.98, wps=22880.9, ups=1.11, wpb=20590.5, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.57, clip=0, loss_scale=717, train_wall=89, wall=7794
2022-07-11 19:47:47 | INFO | train_inner | epoch 008:    757 / 1122 loss=9.662, nll_loss=5.459, mask_ins=1.569, word_ins_ml=6.775, word_reposition=1.317, ppl=809.94, wps=22617.4, ups=1.1, wpb=20482.1, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.62, clip=0, loss_scale=1024, train_wall=90, wall=7884
2022-07-11 19:49:18 | INFO | train_inner | epoch 008:    857 / 1122 loss=9.655, nll_loss=5.45, mask_ins=1.584, word_ins_ml=6.767, word_reposition=1.305, ppl=806.24, wps=22647.6, ups=1.1, wpb=20606.1, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.542, clip=0, loss_scale=1024, train_wall=90, wall=7975
2022-07-11 19:50:47 | INFO | train_inner | epoch 008:    957 / 1122 loss=9.584, nll_loss=5.359, mask_ins=1.584, word_ins_ml=6.687, word_reposition=1.312, ppl=767.23, wps=22977.7, ups=1.12, wpb=20544.7, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.64, clip=0, loss_scale=1024, train_wall=89, wall=8065
2022-07-11 19:52:17 | INFO | train_inner | epoch 008:   1057 / 1122 loss=9.595, nll_loss=5.381, mask_ins=1.577, word_ins_ml=6.706, word_reposition=1.312, ppl=773.36, wps=22694.4, ups=1.12, wpb=20353.3, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.757, clip=0, loss_scale=1024, train_wall=89, wall=8154
2022-07-11 19:53:15 | INFO | train | epoch 008 | loss 9.755 | nll_loss 5.542 | mask_ins 1.581 | word_ins_ml 6.848 | word_reposition 1.326 | ppl 864.02 | wps 22323.6 | ups 1.09 | wpb 20521.1 | bsz 255.8 | num_updates 8965 | lr 0.000373405 | gnorm 1.696 | clip 0 | loss_scale 718 | train_wall 1002 | wall 8212
2022-07-11 19:53:33 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.293 | nll_loss 6.927 | mask_ins 1.662 | word_ins_ml 8.208 | word_reposition 1.423 | ppl 2509.02 | wps 55483.9 | wpb 2367.6 | bsz 32 | num_updates 8965 | best_loss 11.293
2022-07-11 19:53:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 8 @ 8965 updates, score 11.293) (writing took 4.83160219527781 seconds)
2022-07-11 19:53:52 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 19:54:11 | INFO | train_inner | epoch 009:     36 / 1122 loss=9.564, nll_loss=5.391, mask_ins=1.561, word_ins_ml=6.714, word_reposition=1.289, ppl=757.12, wps=17735.7, ups=0.87, wpb=20292.8, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=2.729, clip=0, loss_scale=1095, train_wall=91, wall=8269
2022-07-11 19:55:42 | INFO | train_inner | epoch 009:    136 / 1122 loss=9.498, nll_loss=5.302, mask_ins=1.564, word_ins_ml=6.637, word_reposition=1.296, ppl=722.96, wps=22804.4, ups=1.1, wpb=20653.5, bsz=256, num_updates=9100, lr=0.000370625, gnorm=2.375, clip=0, loss_scale=1024, train_wall=90, wall=8359
2022-07-11 19:57:12 | INFO | train_inner | epoch 009:    236 / 1122 loss=9.453, nll_loss=5.243, mask_ins=1.565, word_ins_ml=6.585, word_reposition=1.303, ppl=700.74, wps=22700, ups=1.11, wpb=20473.5, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.77, clip=0, loss_scale=1024, train_wall=90, wall=8449
2022-07-11 19:58:42 | INFO | train_inner | epoch 009:    336 / 1122 loss=9.392, nll_loss=5.201, mask_ins=1.567, word_ins_ml=6.548, word_reposition=1.277, ppl=672.03, wps=22916.7, ups=1.12, wpb=20468.8, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.712, clip=0, loss_scale=1024, train_wall=89, wall=8539
2022-07-11 20:00:11 | INFO | train_inner | epoch 009:    436 / 1122 loss=9.336, nll_loss=5.142, mask_ins=1.562, word_ins_ml=6.496, word_reposition=1.278, ppl=646.18, wps=22747.6, ups=1.11, wpb=20449.6, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.732, clip=0, loss_scale=1024, train_wall=89, wall=8629
2022-07-11 20:01:42 | INFO | train_inner | epoch 009:    536 / 1122 loss=9.317, nll_loss=5.129, mask_ins=1.561, word_ins_ml=6.484, word_reposition=1.273, ppl=637.93, wps=22721.7, ups=1.1, wpb=20567.8, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.711, clip=0, loss_scale=1126, train_wall=90, wall=8719
2022-07-11 20:02:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 20:03:13 | INFO | train_inner | epoch 009:    637 / 1122 loss=9.315, nll_loss=5.126, mask_ins=1.558, word_ins_ml=6.481, word_reposition=1.276, ppl=636.9, wps=22593.8, ups=1.1, wpb=20565.5, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.942, clip=0, loss_scale=1399, train_wall=90, wall=8810
2022-07-11 20:04:43 | INFO | train_inner | epoch 009:    737 / 1122 loss=9.275, nll_loss=5.094, mask_ins=1.561, word_ins_ml=6.452, word_reposition=1.262, ppl=619.59, wps=22748.7, ups=1.11, wpb=20540.6, bsz=256, num_updates=9700, lr=0.000358979, gnorm=2.222, clip=0, loss_scale=1024, train_wall=90, wall=8900
2022-07-11 20:05:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 20:05:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-11 20:06:14 | INFO | train_inner | epoch 009:    839 / 1122 loss=9.248, nll_loss=5.081, mask_ins=1.544, word_ins_ml=6.442, word_reposition=1.262, ppl=607.94, wps=22542.7, ups=1.1, wpb=20485.7, bsz=256, num_updates=9800, lr=0.000357143, gnorm=2.651, clip=0, loss_scale=482, train_wall=90, wall=8991
2022-07-11 20:07:45 | INFO | train_inner | epoch 009:    939 / 1122 loss=9.208, nll_loss=5.031, mask_ins=1.555, word_ins_ml=6.397, word_reposition=1.257, ppl=591.59, wps=22738.1, ups=1.1, wpb=20589.2, bsz=256, num_updates=9900, lr=0.000355335, gnorm=2.112, clip=0, loss_scale=256, train_wall=90, wall=9082
2022-07-11 20:09:16 | INFO | train_inner | epoch 009:   1039 / 1122 loss=9.212, nll_loss=5.014, mask_ins=1.568, word_ins_ml=6.381, word_reposition=1.263, ppl=593.08, wps=22559.4, ups=1.1, wpb=20496.6, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.878, clip=0, loss_scale=256, train_wall=90, wall=9173
2022-07-11 20:10:30 | INFO | train | epoch 009 | loss 9.322 | nll_loss 5.134 | mask_ins 1.559 | word_ins_ml 6.488 | word_reposition 1.275 | ppl 639.94 | wps 22154.9 | ups 1.08 | wpb 20519 | bsz 255.8 | num_updates 10083 | lr 0.000352095 | gnorm 2.039 | clip 0 | loss_scale 830 | train_wall 1006 | wall 9248
2022-07-11 20:10:48 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.01 | nll_loss 6.724 | mask_ins 1.632 | word_ins_ml 8.038 | word_reposition 1.34 | ppl 2062.08 | wps 55074.8 | wpb 2367.6 | bsz 32 | num_updates 10083 | best_loss 11.01
2022-07-11 20:10:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 9 @ 10083 updates, score 11.01) (writing took 4.867234079167247 seconds)
2022-07-11 20:11:09 | INFO | train_inner | epoch 010:     17 / 1122 loss=9.166, nll_loss=4.984, mask_ins=1.545, word_ins_ml=6.354, word_reposition=1.267, ppl=574.46, wps=18078, ups=0.88, wpb=20487.8, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.767, clip=0, loss_scale=256, train_wall=90, wall=9286
2022-07-11 20:12:40 | INFO | train_inner | epoch 010:    117 / 1122 loss=9.069, nll_loss=4.908, mask_ins=1.534, word_ins_ml=6.289, word_reposition=1.246, ppl=537.08, wps=22706.3, ups=1.1, wpb=20584.3, bsz=256, num_updates=10200, lr=0.00035007, gnorm=2.19, clip=1, loss_scale=256, train_wall=90, wall=9377
2022-07-11 20:14:13 | INFO | train_inner | epoch 010:    217 / 1122 loss=9.053, nll_loss=4.89, mask_ins=1.54, word_ins_ml=6.272, word_reposition=1.241, ppl=531.31, wps=22069.3, ups=1.08, wpb=20523, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.636, clip=0, loss_scale=394, train_wall=92, wall=9470
2022-07-11 20:15:44 | INFO | train_inner | epoch 010:    317 / 1122 loss=9.025, nll_loss=4.853, mask_ins=1.545, word_ins_ml=6.24, word_reposition=1.24, ppl=520.92, wps=22466.7, ups=1.09, wpb=20597.7, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.625, clip=0, loss_scale=512, train_wall=91, wall=9561
2022-07-11 20:17:13 | INFO | train_inner | epoch 010:    417 / 1122 loss=9.011, nll_loss=4.855, mask_ins=1.534, word_ins_ml=6.241, word_reposition=1.236, ppl=515.75, wps=23115.3, ups=1.13, wpb=20542.5, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.659, clip=0, loss_scale=512, train_wall=88, wall=9650
2022-07-11 20:17:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-11 20:18:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-11 20:18:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-07-11 20:18:45 | INFO | train_inner | epoch 010:    520 / 1122 loss=9.044, nll_loss=4.892, mask_ins=1.537, word_ins_ml=6.274, word_reposition=1.233, ppl=527.77, wps=22314, ups=1.09, wpb=20557.8, bsz=256, num_updates=10600, lr=0.000343401, gnorm=6.956, clip=7, loss_scale=316, train_wall=92, wall=9742
2022-07-11 20:20:15 | INFO | train_inner | epoch 010:    620 / 1122 loss=9.02, nll_loss=4.86, mask_ins=1.548, word_ins_ml=6.245, word_reposition=1.226, ppl=519.16, wps=23143.6, ups=1.12, wpb=20678.1, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.836, clip=0, loss_scale=64, train_wall=89, wall=9832
2022-07-11 20:21:43 | INFO | train_inner | epoch 010:    720 / 1122 loss=8.951, nll_loss=4.81, mask_ins=1.53, word_ins_ml=6.2, word_reposition=1.221, ppl=494.89, wps=22957.9, ups=1.13, wpb=20397.3, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.722, clip=0, loss_scale=64, train_wall=88, wall=9921
2022-07-11 20:23:12 | INFO | train_inner | epoch 010:    820 / 1122 loss=8.884, nll_loss=4.752, mask_ins=1.515, word_ins_ml=6.149, word_reposition=1.22, ppl=472.4, wps=23122.4, ups=1.12, wpb=20586.2, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.877, clip=0, loss_scale=64, train_wall=88, wall=10010
2022-07-11 20:24:41 | INFO | train_inner | epoch 010:    920 / 1122 loss=8.955, nll_loss=4.801, mask_ins=1.54, word_ins_ml=6.192, word_reposition=1.223, ppl=496.3, wps=23088.3, ups=1.13, wpb=20518.5, bsz=256, num_updates=11000, lr=0.0003371, gnorm=4.308, clip=2, loss_scale=64, train_wall=88, wall=10098
2022-07-11 20:26:10 | INFO | train_inner | epoch 010:   1020 / 1122 loss=8.893, nll_loss=4.749, mask_ins=1.536, word_ins_ml=6.145, word_reposition=1.212, ppl=475.32, wps=23006.5, ups=1.13, wpb=20448.3, bsz=256, num_updates=11100, lr=0.000335578, gnorm=2.297, clip=1, loss_scale=75, train_wall=88, wall=10187
2022-07-11 20:27:39 | INFO | train_inner | epoch 010:   1120 / 1122 loss=8.8, nll_loss=4.678, mask_ins=1.515, word_ins_ml=6.083, word_reposition=1.202, ppl=445.64, wps=23035.9, ups=1.13, wpb=20470.7, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.659, clip=0, loss_scale=128, train_wall=88, wall=10276
2022-07-11 20:27:41 | INFO | train | epoch 010 | loss 8.975 | nll_loss 4.824 | mask_ins 1.534 | word_ins_ml 6.213 | word_reposition 1.227 | ppl 503.25 | wps 22294.3 | ups 1.09 | wpb 20523.6 | bsz 255.8 | num_updates 11202 | lr 0.000334047 | gnorm 2.518 | clip 1 | loss_scale 223 | train_wall 1000 | wall 10278
2022-07-11 20:27:58 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.023 | nll_loss 6.701 | mask_ins 1.628 | word_ins_ml 8.034 | word_reposition 1.362 | ppl 2081.63 | wps 55671.9 | wpb 2367.6 | bsz 32 | num_updates 11202 | best_loss 11.01
2022-07-11 20:28:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 10 @ 11202 updates, score 11.023) (writing took 2.6482072956860065 seconds)
2022-07-11 20:29:28 | INFO | train_inner | epoch 011:     98 / 1122 loss=8.755, nll_loss=4.631, mask_ins=1.512, word_ins_ml=6.042, word_reposition=1.201, ppl=432.04, wps=18702.4, ups=0.92, wpb=20417.5, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.773, clip=0, loss_scale=128, train_wall=88, wall=10385
2022-07-11 20:30:59 | INFO | train_inner | epoch 011:    198 / 1122 loss=8.698, nll_loss=4.576, mask_ins=1.513, word_ins_ml=5.994, word_reposition=1.191, ppl=415.44, wps=22648.9, ups=1.11, wpb=20494.3, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.686, clip=0, loss_scale=128, train_wall=90, wall=10476
2022-07-11 20:32:29 | INFO | train_inner | epoch 011:    298 / 1122 loss=8.722, nll_loss=4.592, mask_ins=1.516, word_ins_ml=6.007, word_reposition=1.199, ppl=422.3, wps=22565.9, ups=1.1, wpb=20476.2, bsz=256, num_updates=11500, lr=0.00032969, gnorm=2.486, clip=0, loss_scale=128, train_wall=90, wall=10567
2022-07-11 20:34:00 | INFO | train_inner | epoch 011:    398 / 1122 loss=8.697, nll_loss=4.575, mask_ins=1.512, word_ins_ml=5.991, word_reposition=1.193, ppl=415.06, wps=22765.7, ups=1.11, wpb=20590.3, bsz=256, num_updates=11600, lr=0.000328266, gnorm=2.967, clip=1, loss_scale=134, train_wall=90, wall=10657
2022-07-11 20:35:30 | INFO | train_inner | epoch 011:    498 / 1122 loss=8.661, nll_loss=4.558, mask_ins=1.494, word_ins_ml=5.977, word_reposition=1.19, ppl=404.7, wps=23001.5, ups=1.12, wpb=20620.6, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.719, clip=0, loss_scale=256, train_wall=89, wall=10747
2022-07-11 20:37:00 | INFO | train_inner | epoch 011:    598 / 1122 loss=8.609, nll_loss=4.504, mask_ins=1.5, word_ins_ml=5.928, word_reposition=1.181, ppl=390.52, wps=22785.2, ups=1.11, wpb=20575.7, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.684, clip=0, loss_scale=256, train_wall=90, wall=10837
2022-07-11 20:38:29 | INFO | train_inner | epoch 011:    698 / 1122 loss=8.582, nll_loss=4.48, mask_ins=1.503, word_ins_ml=5.907, word_reposition=1.172, ppl=383.31, wps=23028.9, ups=1.12, wpb=20510.4, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.711, clip=0, loss_scale=256, train_wall=88, wall=10926
2022-07-11 20:39:58 | INFO | train_inner | epoch 011:    798 / 1122 loss=8.583, nll_loss=4.496, mask_ins=1.493, word_ins_ml=5.921, word_reposition=1.17, ppl=383.6, wps=22855.8, ups=1.12, wpb=20341.8, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.701, clip=0, loss_scale=256, train_wall=88, wall=11015
2022-07-11 20:41:28 | INFO | train_inner | epoch 011:    898 / 1122 loss=8.473, nll_loss=4.391, mask_ins=1.485, word_ins_ml=5.828, word_reposition=1.16, ppl=355.43, wps=22940.3, ups=1.11, wpb=20607.2, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.709, clip=0, loss_scale=256, train_wall=89, wall=11105
2022-07-11 20:42:57 | INFO | train_inner | epoch 011:    998 / 1122 loss=8.517, nll_loss=4.415, mask_ins=1.504, word_ins_ml=5.849, word_reposition=1.164, ppl=366.26, wps=22959.4, ups=1.12, wpb=20502.1, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.709, clip=0, loss_scale=494, train_wall=89, wall=11194
2022-07-11 20:44:26 | INFO | train_inner | epoch 011:   1098 / 1122 loss=8.456, nll_loss=4.373, mask_ins=1.49, word_ins_ml=5.811, word_reposition=1.155, ppl=351.05, wps=23012, ups=1.12, wpb=20567.2, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.722, clip=0, loss_scale=512, train_wall=89, wall=11284
2022-07-11 20:44:48 | INFO | train | epoch 011 | loss 8.61 | nll_loss 4.505 | mask_ins 1.502 | word_ins_ml 5.929 | word_reposition 1.179 | ppl 390.78 | wps 22420.4 | ups 1.09 | wpb 20521.7 | bsz 255.8 | num_updates 12324 | lr 0.000318478 | gnorm 1.893 | clip 0.1 | loss_scale 261 | train_wall 1000 | wall 11305
2022-07-11 20:45:05 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.702 | nll_loss 6.427 | mask_ins 1.587 | word_ins_ml 7.778 | word_reposition 1.337 | ppl 1665.37 | wps 55629.6 | wpb 2367.6 | bsz 32 | num_updates 12324 | best_loss 10.702
2022-07-11 20:45:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 11 @ 12324 updates, score 10.702) (writing took 4.792888905853033 seconds)
2022-07-11 20:46:18 | INFO | train_inner | epoch 012:     76 / 1122 loss=8.415, nll_loss=4.346, mask_ins=1.478, word_ins_ml=5.788, word_reposition=1.149, ppl=341.37, wps=18262.4, ups=0.89, wpb=20450, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=1.888, clip=0, loss_scale=512, train_wall=89, wall=11396
2022-07-11 20:47:48 | INFO | train_inner | epoch 012:    176 / 1122 loss=8.365, nll_loss=4.301, mask_ins=1.468, word_ins_ml=5.748, word_reposition=1.149, ppl=329.64, wps=22928.8, ups=1.12, wpb=20499, bsz=256, num_updates=12500, lr=0.000316228, gnorm=1.719, clip=0, loss_scale=512, train_wall=89, wall=11485
2022-07-11 20:49:18 | INFO | train_inner | epoch 012:    276 / 1122 loss=8.353, nll_loss=4.286, mask_ins=1.471, word_ins_ml=5.735, word_reposition=1.148, ppl=327.04, wps=22868.1, ups=1.11, wpb=20515.5, bsz=256, num_updates=12600, lr=0.00031497, gnorm=1.738, clip=0, loss_scale=512, train_wall=89, wall=11575
2022-07-11 20:50:48 | INFO | train_inner | epoch 012:    376 / 1122 loss=8.365, nll_loss=4.282, mask_ins=1.484, word_ins_ml=5.73, word_reposition=1.151, ppl=329.8, wps=22874, ups=1.11, wpb=20609.7, bsz=256, num_updates=12700, lr=0.000313728, gnorm=1.727, clip=0, loss_scale=927, train_wall=89, wall=11665
2022-07-11 20:52:17 | INFO | train_inner | epoch 012:    476 / 1122 loss=8.286, nll_loss=4.238, mask_ins=1.461, word_ins_ml=5.691, word_reposition=1.133, ppl=312.17, wps=22854.1, ups=1.12, wpb=20489.6, bsz=256, num_updates=12800, lr=0.0003125, gnorm=1.737, clip=0, loss_scale=1024, train_wall=89, wall=11754
2022-07-11 20:53:48 | INFO | train_inner | epoch 012:    576 / 1122 loss=8.249, nll_loss=4.209, mask_ins=1.463, word_ins_ml=5.666, word_reposition=1.121, ppl=304.31, wps=22682.3, ups=1.1, wpb=20594.2, bsz=256, num_updates=12900, lr=0.000311286, gnorm=1.724, clip=0, loss_scale=1024, train_wall=90, wall=11845
2022-07-11 20:55:18 | INFO | train_inner | epoch 012:    676 / 1122 loss=8.201, nll_loss=4.164, mask_ins=1.458, word_ins_ml=5.625, word_reposition=1.118, ppl=294.35, wps=22759.3, ups=1.12, wpb=20403.4, bsz=256, num_updates=13000, lr=0.000310087, gnorm=1.796, clip=0, loss_scale=1024, train_wall=89, wall=11935
2022-07-11 20:56:48 | INFO | train_inner | epoch 012:    776 / 1122 loss=8.189, nll_loss=4.155, mask_ins=1.455, word_ins_ml=5.617, word_reposition=1.116, ppl=291.78, wps=22801.7, ups=1.11, wpb=20588.5, bsz=256, num_updates=13100, lr=0.000308901, gnorm=1.911, clip=0, loss_scale=1024, train_wall=90, wall=12025
2022-07-11 20:58:17 | INFO | train_inner | epoch 012:    876 / 1122 loss=8.178, nll_loss=4.153, mask_ins=1.448, word_ins_ml=5.615, word_reposition=1.115, ppl=289.6, wps=23106.8, ups=1.12, wpb=20566.9, bsz=256, num_updates=13200, lr=0.000307729, gnorm=1.765, clip=0, loss_scale=1731, train_wall=88, wall=12114
2022-07-11 20:59:46 | INFO | train_inner | epoch 012:    976 / 1122 loss=8.065, nll_loss=4.064, mask_ins=1.432, word_ins_ml=5.536, word_reposition=1.098, ppl=267.87, wps=22974.6, ups=1.12, wpb=20524.5, bsz=256, num_updates=13300, lr=0.00030657, gnorm=1.79, clip=0, loss_scale=2048, train_wall=89, wall=12204
2022-07-11 21:01:16 | INFO | train_inner | epoch 012:   1076 / 1122 loss=8.073, nll_loss=4.07, mask_ins=1.432, word_ins_ml=5.541, word_reposition=1.101, ppl=269.35, wps=22918.8, ups=1.12, wpb=20485.2, bsz=256, num_updates=13400, lr=0.000305424, gnorm=1.821, clip=0, loss_scale=2048, train_wall=89, wall=12293
2022-07-11 21:01:57 | INFO | train | epoch 012 | loss 8.238 | nll_loss 4.198 | mask_ins 1.458 | word_ins_ml 5.655 | word_reposition 1.125 | ppl 301.98 | wps 22372.2 | ups 1.09 | wpb 20520.5 | bsz 255.8 | num_updates 13446 | lr 0.000304901 | gnorm 1.78 | clip 0 | loss_scale 1177 | train_wall 1000 | wall 12334
2022-07-11 21:02:14 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.772 | nll_loss 6.473 | mask_ins 1.623 | word_ins_ml 7.829 | word_reposition 1.32 | ppl 1748.92 | wps 55699.8 | wpb 2367.6 | bsz 32 | num_updates 13446 | best_loss 10.702
2022-07-11 21:02:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 12 @ 13446 updates, score 10.772) (writing took 2.6342211719602346 seconds)
2022-07-11 21:03:05 | INFO | train_inner | epoch 013:     54 / 1122 loss=8.07, nll_loss=4.053, mask_ins=1.447, word_ins_ml=5.526, word_reposition=1.097, ppl=268.72, wps=18631.5, ups=0.91, wpb=20386.8, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=1.843, clip=0, loss_scale=2048, train_wall=88, wall=12402
2022-07-11 21:04:35 | INFO | train_inner | epoch 013:    154 / 1122 loss=8.018, nll_loss=4.018, mask_ins=1.427, word_ins_ml=5.495, word_reposition=1.095, ppl=259.15, wps=22846.6, ups=1.11, wpb=20593.3, bsz=256, num_updates=13600, lr=0.00030317, gnorm=1.778, clip=0, loss_scale=2048, train_wall=90, wall=12492
2022-07-11 21:06:04 | INFO | train_inner | epoch 013:    254 / 1122 loss=7.951, nll_loss=3.957, mask_ins=1.427, word_ins_ml=5.441, word_reposition=1.082, ppl=247.47, wps=22995.7, ups=1.12, wpb=20454.1, bsz=256, num_updates=13700, lr=0.000302061, gnorm=1.828, clip=0, loss_scale=3215, train_wall=88, wall=12581
2022-07-11 21:07:33 | INFO | train_inner | epoch 013:    354 / 1122 loss=7.907, nll_loss=3.958, mask_ins=1.399, word_ins_ml=5.442, word_reposition=1.066, ppl=239.99, wps=23017.9, ups=1.12, wpb=20530.2, bsz=256, num_updates=13800, lr=0.000300965, gnorm=1.809, clip=0, loss_scale=4096, train_wall=89, wall=12671
2022-07-11 21:09:02 | INFO | train_inner | epoch 013:    454 / 1122 loss=7.927, nll_loss=3.959, mask_ins=1.414, word_ins_ml=5.442, word_reposition=1.071, ppl=243.31, wps=23129.3, ups=1.12, wpb=20590.5, bsz=256, num_updates=13900, lr=0.00029988, gnorm=1.826, clip=0, loss_scale=4096, train_wall=88, wall=12760
2022-07-11 21:10:33 | INFO | train_inner | epoch 013:    554 / 1122 loss=7.87, nll_loss=3.905, mask_ins=1.405, word_ins_ml=5.394, word_reposition=1.071, ppl=233.97, wps=22630.4, ups=1.1, wpb=20525.6, bsz=256, num_updates=14000, lr=0.000298807, gnorm=1.814, clip=0, loss_scale=4096, train_wall=90, wall=12850
2022-07-11 21:12:03 | INFO | train_inner | epoch 013:    654 / 1122 loss=7.851, nll_loss=3.892, mask_ins=1.406, word_ins_ml=5.382, word_reposition=1.063, ppl=230.84, wps=22687.5, ups=1.11, wpb=20467.9, bsz=256, num_updates=14100, lr=0.000297746, gnorm=1.826, clip=0, loss_scale=4096, train_wall=90, wall=12941
2022-07-11 21:13:33 | INFO | train_inner | epoch 013:    754 / 1122 loss=7.785, nll_loss=3.863, mask_ins=1.381, word_ins_ml=5.356, word_reposition=1.048, ppl=220.51, wps=22997.8, ups=1.12, wpb=20534.2, bsz=256, num_updates=14200, lr=0.000296695, gnorm=1.839, clip=0, loss_scale=5939, train_wall=89, wall=13030
2022-07-11 21:15:02 | INFO | train_inner | epoch 013:    854 / 1122 loss=7.793, nll_loss=3.87, mask_ins=1.384, word_ins_ml=5.361, word_reposition=1.047, ppl=221.75, wps=23013.3, ups=1.12, wpb=20509.8, bsz=256, num_updates=14300, lr=0.000295656, gnorm=1.843, clip=0, loss_scale=8192, train_wall=89, wall=13119
2022-07-11 21:16:31 | INFO | train_inner | epoch 013:    954 / 1122 loss=7.694, nll_loss=3.781, mask_ins=1.377, word_ins_ml=5.283, word_reposition=1.034, ppl=207.03, wps=22975.5, ups=1.12, wpb=20475.1, bsz=256, num_updates=14400, lr=0.000294628, gnorm=1.849, clip=0, loss_scale=8192, train_wall=89, wall=13208
2022-07-11 21:17:13 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-11 21:18:02 | INFO | train_inner | epoch 013:   1055 / 1122 loss=7.683, nll_loss=3.775, mask_ins=1.372, word_ins_ml=5.277, word_reposition=1.034, ppl=205.51, wps=22727.9, ups=1.1, wpb=20658.2, bsz=256, num_updates=14500, lr=0.00029361, gnorm=1.85, clip=0, loss_scale=5962, train_wall=90, wall=13299
2022-07-11 21:18:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 21:19:02 | INFO | train | epoch 013 | loss 7.849 | nll_loss 3.898 | mask_ins 1.4 | word_ins_ml 5.388 | word_reposition 1.061 | ppl 230.51 | wps 22409 | ups 1.09 | wpb 20519.4 | bsz 255.8 | num_updates 14566 | lr 0.000292944 | gnorm 1.834 | clip 0 | loss_scale 4706 | train_wall 998 | wall 13359
2022-07-11 21:19:20 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.678 | nll_loss 6.356 | mask_ins 1.585 | word_ins_ml 7.726 | word_reposition 1.368 | ppl 1638.69 | wps 55496.9 | wpb 2367.6 | bsz 32 | num_updates 14566 | best_loss 10.678
2022-07-11 21:19:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 13 @ 14566 updates, score 10.678) (writing took 4.753561643883586 seconds)
2022-07-11 21:19:55 | INFO | train_inner | epoch 014:     34 / 1122 loss=7.66, nll_loss=3.771, mask_ins=1.358, word_ins_ml=5.274, word_reposition=1.028, ppl=202.23, wps=17968.4, ups=0.88, wpb=20389, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=1.9, clip=0, loss_scale=2372, train_wall=90, wall=13412
2022-07-11 21:21:25 | INFO | train_inner | epoch 014:    134 / 1122 loss=7.643, nll_loss=3.744, mask_ins=1.368, word_ins_ml=5.25, word_reposition=1.026, ppl=199.92, wps=22881.7, ups=1.12, wpb=20495.2, bsz=256, num_updates=14700, lr=0.000291606, gnorm=1.812, clip=0, loss_scale=2048, train_wall=89, wall=13502
2022-07-11 21:22:56 | INFO | train_inner | epoch 014:    234 / 1122 loss=7.586, nll_loss=3.698, mask_ins=1.356, word_ins_ml=5.208, word_reposition=1.022, ppl=192.16, wps=22460.2, ups=1.09, wpb=20552.3, bsz=256, num_updates=14800, lr=0.000290619, gnorm=1.843, clip=0, loss_scale=2048, train_wall=91, wall=13594
2022-07-11 21:24:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 21:24:27 | INFO | train_inner | epoch 014:    335 / 1122 loss=7.569, nll_loss=3.698, mask_ins=1.344, word_ins_ml=5.208, word_reposition=1.017, ppl=189.94, wps=22725.4, ups=1.11, wpb=20549.9, bsz=256, num_updates=14900, lr=0.000289642, gnorm=2.331, clip=0, loss_scale=1926, train_wall=90, wall=13684
2022-07-11 21:24:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 21:25:57 | INFO | train_inner | epoch 014:    436 / 1122 loss=7.571, nll_loss=3.703, mask_ins=1.34, word_ins_ml=5.212, word_reposition=1.018, ppl=190.11, wps=22724.2, ups=1.11, wpb=20498.1, bsz=256, num_updates=15000, lr=0.000288675, gnorm=4.712, clip=2, loss_scale=527, train_wall=90, wall=13774
2022-07-11 21:27:26 | INFO | train_inner | epoch 014:    536 / 1122 loss=7.468, nll_loss=3.606, mask_ins=1.338, word_ins_ml=5.126, word_reposition=1.004, ppl=177.06, wps=23008.4, ups=1.12, wpb=20496.6, bsz=256, num_updates=15100, lr=0.000287718, gnorm=1.928, clip=0, loss_scale=512, train_wall=88, wall=13863
2022-07-11 21:28:55 | INFO | train_inner | epoch 014:    636 / 1122 loss=7.443, nll_loss=3.594, mask_ins=1.331, word_ins_ml=5.115, word_reposition=0.997, ppl=174.06, wps=23065.3, ups=1.12, wpb=20538.6, bsz=256, num_updates=15200, lr=0.00028677, gnorm=1.89, clip=0, loss_scale=512, train_wall=88, wall=13952
2022-07-11 21:30:24 | INFO | train_inner | epoch 014:    736 / 1122 loss=7.407, nll_loss=3.575, mask_ins=1.313, word_ins_ml=5.098, word_reposition=0.996, ppl=169.76, wps=23171.1, ups=1.12, wpb=20617.9, bsz=256, num_updates=15300, lr=0.000285831, gnorm=1.997, clip=0, loss_scale=512, train_wall=88, wall=14041
2022-07-11 21:31:53 | INFO | train_inner | epoch 014:    836 / 1122 loss=7.417, nll_loss=3.608, mask_ins=1.311, word_ins_ml=5.127, word_reposition=0.979, ppl=170.84, wps=22924.8, ups=1.12, wpb=20408.2, bsz=256, num_updates=15400, lr=0.000284901, gnorm=1.871, clip=0, loss_scale=512, train_wall=88, wall=14130
2022-07-11 21:33:22 | INFO | train_inner | epoch 014:    936 / 1122 loss=7.352, nll_loss=3.538, mask_ins=1.306, word_ins_ml=5.065, word_reposition=0.981, ppl=163.34, wps=23031, ups=1.12, wpb=20511.4, bsz=256, num_updates=15500, lr=0.000283981, gnorm=1.88, clip=0, loss_scale=952, train_wall=88, wall=14219
2022-07-11 21:34:52 | INFO | train_inner | epoch 014:   1036 / 1122 loss=7.311, nll_loss=3.504, mask_ins=1.296, word_ins_ml=5.034, word_reposition=0.981, ppl=158.79, wps=22942.6, ups=1.12, wpb=20562.6, bsz=256, num_updates=15600, lr=0.000283069, gnorm=1.891, clip=0, loss_scale=1024, train_wall=89, wall=14309
2022-07-11 21:36:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 21:36:09 | INFO | train | epoch 014 | loss 7.467 | nll_loss 3.621 | mask_ins 1.328 | word_ins_ml 5.139 | word_reposition 1.001 | ppl 176.97 | wps 22364.9 | ups 1.09 | wpb 20518.3 | bsz 255.8 | num_updates 15685 | lr 0.000282301 | gnorm 2.226 | clip 0.3 | loss_scale 1081 | train_wall 997 | wall 14386
2022-07-11 21:36:27 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.622 | nll_loss 6.294 | mask_ins 1.591 | word_ins_ml 7.682 | word_reposition 1.349 | ppl 1576.25 | wps 55439.7 | wpb 2367.6 | bsz 32 | num_updates 15685 | best_loss 10.622
2022-07-11 21:36:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 14 @ 15685 updates, score 10.622) (writing took 4.9179100366309285 seconds)
2022-07-11 21:36:46 | INFO | train_inner | epoch 015:     15 / 1122 loss=7.301, nll_loss=3.505, mask_ins=1.29, word_ins_ml=5.034, word_reposition=0.977, ppl=157.72, wps=17933.3, ups=0.88, wpb=20452.7, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=2.428, clip=1, loss_scale=902, train_wall=91, wall=14423
2022-07-11 21:38:17 | INFO | train_inner | epoch 015:    115 / 1122 loss=7.274, nll_loss=3.484, mask_ins=1.291, word_ins_ml=5.016, word_reposition=0.967, ppl=154.74, wps=22718, ups=1.1, wpb=20631.5, bsz=256, num_updates=15800, lr=0.000281272, gnorm=2.012, clip=0, loss_scale=512, train_wall=90, wall=14514
2022-07-11 21:39:48 | INFO | train_inner | epoch 015:    215 / 1122 loss=7.198, nll_loss=3.432, mask_ins=1.265, word_ins_ml=4.969, word_reposition=0.963, ppl=146.8, wps=22516.4, ups=1.09, wpb=20577.3, bsz=256, num_updates=15900, lr=0.000280386, gnorm=2.144, clip=0, loss_scale=512, train_wall=91, wall=14605
2022-07-11 21:41:18 | INFO | train_inner | epoch 015:    315 / 1122 loss=7.151, nll_loss=3.389, mask_ins=1.261, word_ins_ml=4.931, word_reposition=0.958, ppl=142.14, wps=22897.2, ups=1.11, wpb=20550.2, bsz=256, num_updates=16000, lr=0.000279508, gnorm=2.747, clip=1, loss_scale=512, train_wall=89, wall=14695
2022-07-11 21:42:47 | INFO | train_inner | epoch 015:    415 / 1122 loss=7.2, nll_loss=3.43, mask_ins=1.271, word_ins_ml=4.966, word_reposition=0.963, ppl=147.02, wps=23025.3, ups=1.12, wpb=20540.6, bsz=256, num_updates=16100, lr=0.000278639, gnorm=2.268, clip=0, loss_scale=512, train_wall=89, wall=14784
2022-07-11 21:44:17 | INFO | train_inner | epoch 015:    515 / 1122 loss=7.144, nll_loss=3.409, mask_ins=1.253, word_ins_ml=4.947, word_reposition=0.944, ppl=141.45, wps=22788.8, ups=1.11, wpb=20479.4, bsz=256, num_updates=16200, lr=0.000277778, gnorm=3.096, clip=0, loss_scale=573, train_wall=89, wall=14874
2022-07-11 21:45:46 | INFO | train_inner | epoch 015:    615 / 1122 loss=7.124, nll_loss=3.384, mask_ins=1.246, word_ins_ml=4.925, word_reposition=0.953, ppl=139.44, wps=23012.4, ups=1.12, wpb=20501, bsz=256, num_updates=16300, lr=0.000276924, gnorm=2.225, clip=0, loss_scale=1024, train_wall=88, wall=14963
2022-07-11 21:46:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 21:47:16 | INFO | train_inner | epoch 015:    716 / 1122 loss=7.1, nll_loss=3.363, mask_ins=1.251, word_ins_ml=4.906, word_reposition=0.944, ppl=137.21, wps=22699.1, ups=1.11, wpb=20484.2, bsz=256, num_updates=16400, lr=0.000276079, gnorm=2.016, clip=0, loss_scale=654, train_wall=90, wall=15053
2022-07-11 21:48:46 | INFO | train_inner | epoch 015:    816 / 1122 loss=7.042, nll_loss=3.336, mask_ins=1.232, word_ins_ml=4.881, word_reposition=0.929, ppl=131.77, wps=22849.6, ups=1.11, wpb=20576.8, bsz=256, num_updates=16500, lr=0.000275241, gnorm=1.899, clip=0, loss_scale=512, train_wall=89, wall=15143
2022-07-11 21:50:15 | INFO | train_inner | epoch 015:    916 / 1122 loss=7.055, nll_loss=3.347, mask_ins=1.231, word_ins_ml=4.891, word_reposition=0.934, ppl=132.98, wps=22964.5, ups=1.12, wpb=20429.9, bsz=256, num_updates=16600, lr=0.000274411, gnorm=2.351, clip=1, loss_scale=512, train_wall=88, wall=15232
2022-07-11 21:51:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-11 21:51:46 | INFO | train_inner | epoch 015:   1017 / 1122 loss=7.08, nll_loss=3.369, mask_ins=1.232, word_ins_ml=4.91, word_reposition=0.939, ppl=135.34, wps=22537.8, ups=1.1, wpb=20548.4, bsz=256, num_updates=16700, lr=0.000273588, gnorm=2.362, clip=0, loss_scale=494, train_wall=91, wall=15324
2022-07-11 21:53:16 | INFO | train_inner | epoch 015:   1117 / 1122 loss=7.005, nll_loss=3.295, mask_ins=1.228, word_ins_ml=4.844, word_reposition=0.933, ppl=128.41, wps=22921.6, ups=1.11, wpb=20575.2, bsz=256, num_updates=16800, lr=0.000272772, gnorm=3.582, clip=3, loss_scale=256, train_wall=89, wall=15413
2022-07-11 21:53:20 | INFO | train | epoch 015 | loss 7.127 | nll_loss 3.387 | mask_ins 1.251 | word_ins_ml 4.928 | word_reposition 0.948 | ppl 139.81 | wps 22280 | ups 1.09 | wpb 20520.7 | bsz 255.8 | num_updates 16805 | lr 0.000272732 | gnorm 2.43 | clip 0.4 | loss_scale 550 | train_wall 1002 | wall 15418
2022-07-11 21:53:38 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.54 | nll_loss 6.219 | mask_ins 1.559 | word_ins_ml 7.62 | word_reposition 1.362 | ppl 1488.76 | wps 55661.3 | wpb 2367.6 | bsz 32 | num_updates 16805 | best_loss 10.54
2022-07-11 21:53:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 15 @ 16805 updates, score 10.54) (writing took 4.806210340932012 seconds)
2022-07-11 21:55:09 | INFO | train_inner | epoch 016:     95 / 1122 loss=6.952, nll_loss=3.271, mask_ins=1.205, word_ins_ml=4.823, word_reposition=0.923, ppl=123.77, wps=18065.7, ups=0.89, wpb=20289.1, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=3.105, clip=1, loss_scale=256, train_wall=89, wall=15526
2022-07-11 21:56:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 128.0
2022-07-11 21:56:39 | INFO | train_inner | epoch 016:    196 / 1122 loss=6.988, nll_loss=3.31, mask_ins=1.202, word_ins_ml=4.857, word_reposition=0.928, ppl=126.9, wps=22613.5, ups=1.1, wpb=20503.7, bsz=256, num_updates=17000, lr=0.000271163, gnorm=3.194, clip=0, loss_scale=247, train_wall=90, wall=15616
2022-07-11 21:57:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-07-11 21:58:09 | INFO | train_inner | epoch 016:    297 / 1122 loss=6.946, nll_loss=3.25, mask_ins=1.214, word_ins_ml=4.803, word_reposition=0.928, ppl=123.27, wps=22884.7, ups=1.11, wpb=20640.6, bsz=256, num_updates=17100, lr=0.000270369, gnorm=3.786, clip=1, loss_scale=110, train_wall=90, wall=15707
2022-07-11 21:59:39 | INFO | train_inner | epoch 016:    397 / 1122 loss=6.908, nll_loss=3.24, mask_ins=1.194, word_ins_ml=4.794, word_reposition=0.92, ppl=120.12, wps=22722.2, ups=1.11, wpb=20452.4, bsz=256, num_updates=17200, lr=0.000269582, gnorm=3.196, clip=2, loss_scale=64, train_wall=89, wall=15797
2022-07-11 22:01:10 | INFO | train_inner | epoch 016:    497 / 1122 loss=6.874, nll_loss=3.211, mask_ins=1.192, word_ins_ml=4.768, word_reposition=0.914, ppl=117.32, wps=22741.8, ups=1.1, wpb=20639.5, bsz=256, num_updates=17300, lr=0.000268802, gnorm=2.378, clip=0, loss_scale=64, train_wall=90, wall=15887
2022-07-11 22:02:40 | INFO | train_inner | epoch 016:    597 / 1122 loss=6.849, nll_loss=3.195, mask_ins=1.191, word_ins_ml=4.754, word_reposition=0.904, ppl=115.28, wps=22978.7, ups=1.12, wpb=20532.5, bsz=256, num_updates=17400, lr=0.000268028, gnorm=3.414, clip=2, loss_scale=64, train_wall=89, wall=15977
2022-07-11 22:04:09 | INFO | train_inner | epoch 016:    697 / 1122 loss=6.825, nll_loss=3.184, mask_ins=1.171, word_ins_ml=4.743, word_reposition=0.911, ppl=113.38, wps=23117.8, ups=1.12, wpb=20584.7, bsz=256, num_updates=17500, lr=0.000267261, gnorm=2.242, clip=0, loss_scale=64, train_wall=88, wall=16066
2022-07-11 22:05:39 | INFO | train_inner | epoch 016:    797 / 1122 loss=6.803, nll_loss=3.169, mask_ins=1.173, word_ins_ml=4.729, word_reposition=0.901, ppl=111.66, wps=22548.3, ups=1.1, wpb=20506.1, bsz=256, num_updates=17600, lr=0.000266501, gnorm=2.333, clip=1, loss_scale=74, train_wall=90, wall=16157
2022-07-11 22:07:10 | INFO | train_inner | epoch 016:    897 / 1122 loss=6.814, nll_loss=3.181, mask_ins=1.173, word_ins_ml=4.74, word_reposition=0.902, ppl=112.55, wps=22658.6, ups=1.1, wpb=20557.4, bsz=256, num_updates=17700, lr=0.000265747, gnorm=1.929, clip=0, loss_scale=128, train_wall=90, wall=16247
2022-07-11 22:08:41 | INFO | train_inner | epoch 016:    997 / 1122 loss=6.754, nll_loss=3.146, mask_ins=1.152, word_ins_ml=4.708, word_reposition=0.894, ppl=107.9, wps=22720.2, ups=1.11, wpb=20519.3, bsz=256, num_updates=17800, lr=0.000264999, gnorm=1.91, clip=0, loss_scale=128, train_wall=90, wall=16338
2022-07-11 22:10:11 | INFO | train_inner | epoch 016:   1097 / 1122 loss=6.759, nll_loss=3.14, mask_ins=1.161, word_ins_ml=4.703, word_reposition=0.895, ppl=108.29, wps=22625, ups=1.1, wpb=20494.8, bsz=256, num_updates=17900, lr=0.000264258, gnorm=1.899, clip=0, loss_scale=128, train_wall=90, wall=16428
2022-07-11 22:10:34 | INFO | train | epoch 016 | loss 6.86 | nll_loss 3.21 | mask_ins 1.184 | word_ins_ml 4.766 | word_reposition 0.911 | ppl 116.2 | wps 22244.6 | ups 1.08 | wpb 20518.7 | bsz 255.8 | num_updates 17925 | lr 0.000264074 | gnorm 2.652 | clip 0.6 | loss_scale 120 | train_wall 1004 | wall 16451
2022-07-11 22:10:51 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.339 | nll_loss 6.07 | mask_ins 1.553 | word_ins_ml 7.477 | word_reposition 1.31 | ppl 1295.39 | wps 55471 | wpb 2367.6 | bsz 32 | num_updates 17925 | best_loss 10.339
2022-07-11 22:10:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 16 @ 17925 updates, score 10.339) (writing took 4.934871296398342 seconds)
2022-07-11 22:12:05 | INFO | train_inner | epoch 017:     75 / 1122 loss=6.769, nll_loss=3.157, mask_ins=1.155, word_ins_ml=4.717, word_reposition=0.897, ppl=109.06, wps=18042.6, ups=0.88, wpb=20459.1, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=2.716, clip=0, loss_scale=128, train_wall=90, wall=16542
2022-07-11 22:13:35 | INFO | train_inner | epoch 017:    175 / 1122 loss=6.726, nll_loss=3.11, mask_ins=1.154, word_ins_ml=4.675, word_reposition=0.896, ppl=105.85, wps=22619.6, ups=1.1, wpb=20508, bsz=256, num_updates=18100, lr=0.000262794, gnorm=1.94, clip=0, loss_scale=133, train_wall=90, wall=16632
2022-07-11 22:15:06 | INFO | train_inner | epoch 017:    275 / 1122 loss=6.715, nll_loss=3.117, mask_ins=1.147, word_ins_ml=4.681, word_reposition=0.887, ppl=105.02, wps=22596.2, ups=1.1, wpb=20482.3, bsz=256, num_updates=18200, lr=0.000262071, gnorm=1.945, clip=0, loss_scale=256, train_wall=90, wall=16723
2022-07-11 22:16:36 | INFO | train_inner | epoch 017:    375 / 1122 loss=6.699, nll_loss=3.098, mask_ins=1.146, word_ins_ml=4.664, word_reposition=0.889, ppl=103.92, wps=22549.1, ups=1.11, wpb=20393.3, bsz=256, num_updates=18300, lr=0.000261354, gnorm=1.989, clip=0, loss_scale=256, train_wall=90, wall=16813
2022-07-11 22:18:07 | INFO | train_inner | epoch 017:    475 / 1122 loss=6.634, nll_loss=3.062, mask_ins=1.121, word_ins_ml=4.632, word_reposition=0.881, ppl=99.34, wps=22664.5, ups=1.11, wpb=20485.1, bsz=256, num_updates=18400, lr=0.000260643, gnorm=1.914, clip=0, loss_scale=256, train_wall=90, wall=16904
2022-07-11 22:19:37 | INFO | train_inner | epoch 017:    575 / 1122 loss=6.647, nll_loss=3.072, mask_ins=1.128, word_ins_ml=4.64, word_reposition=0.879, ppl=100.23, wps=22696.9, ups=1.11, wpb=20514.3, bsz=256, num_updates=18500, lr=0.000259938, gnorm=1.928, clip=0, loss_scale=256, train_wall=90, wall=16994
2022-07-11 22:21:08 | INFO | train_inner | epoch 017:    675 / 1122 loss=6.602, nll_loss=3.017, mask_ins=1.127, word_ins_ml=4.591, word_reposition=0.885, ppl=97.15, wps=22790.7, ups=1.11, wpb=20620.2, bsz=256, num_updates=18600, lr=0.000259238, gnorm=1.897, clip=0, loss_scale=256, train_wall=90, wall=17085
2022-07-11 22:22:37 | INFO | train_inner | epoch 017:    775 / 1122 loss=6.602, nll_loss=3.027, mask_ins=1.121, word_ins_ml=4.599, word_reposition=0.882, ppl=97.15, wps=23229.2, ups=1.12, wpb=20757.2, bsz=256, num_updates=18700, lr=0.000258544, gnorm=1.923, clip=0, loss_scale=492, train_wall=89, wall=17174
2022-07-11 22:24:06 | INFO | train_inner | epoch 017:    875 / 1122 loss=6.61, nll_loss=3.041, mask_ins=1.123, word_ins_ml=4.612, word_reposition=0.875, ppl=97.67, wps=23039.4, ups=1.13, wpb=20454.1, bsz=256, num_updates=18800, lr=0.000257855, gnorm=1.98, clip=0, loss_scale=512, train_wall=88, wall=17263
2022-07-11 22:25:35 | INFO | train_inner | epoch 017:    975 / 1122 loss=6.58, nll_loss=3.038, mask_ins=1.101, word_ins_ml=4.608, word_reposition=0.871, ppl=95.7, wps=23033.6, ups=1.12, wpb=20494.5, bsz=256, num_updates=18900, lr=0.000257172, gnorm=1.92, clip=0, loss_scale=512, train_wall=88, wall=17352
2022-07-11 22:27:04 | INFO | train_inner | epoch 017:   1075 / 1122 loss=6.502, nll_loss=2.964, mask_ins=1.094, word_ins_ml=4.541, word_reposition=0.866, ppl=90.63, wps=23106.4, ups=1.13, wpb=20538.5, bsz=256, num_updates=19000, lr=0.000256495, gnorm=2.621, clip=0, loss_scale=512, train_wall=88, wall=17441
2022-07-11 22:27:45 | INFO | train | epoch 017 | loss 6.633 | nll_loss 3.054 | mask_ins 1.127 | word_ins_ml 4.624 | word_reposition 0.882 | ppl 99.22 | wps 22315.8 | ups 1.09 | wpb 20519.4 | bsz 255.8 | num_updates 19047 | lr 0.000256178 | gnorm 2.079 | clip 0 | loss_scale 337 | train_wall 1002 | wall 17482
2022-07-11 22:28:03 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.605 | nll_loss 6.215 | mask_ins 1.608 | word_ins_ml 7.62 | word_reposition 1.377 | ppl 1556.97 | wps 54987 | wpb 2367.6 | bsz 32 | num_updates 19047 | best_loss 10.339
2022-07-11 22:28:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 17 @ 19047 updates, score 10.605) (writing took 2.527523155324161 seconds)
2022-07-11 22:28:53 | INFO | train_inner | epoch 018:     53 / 1122 loss=6.53, nll_loss=2.976, mask_ins=1.105, word_ins_ml=4.553, word_reposition=0.872, ppl=92.44, wps=18507.7, ups=0.91, wpb=20308.5, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=2.14, clip=0, loss_scale=512, train_wall=89, wall=17550
2022-07-11 22:30:23 | INFO | train_inner | epoch 018:    153 / 1122 loss=6.486, nll_loss=2.944, mask_ins=1.095, word_ins_ml=4.524, word_reposition=0.866, ppl=89.61, wps=22967.6, ups=1.12, wpb=20576.9, bsz=256, num_updates=19200, lr=0.000255155, gnorm=1.923, clip=0, loss_scale=922, train_wall=89, wall=17640
2022-07-11 22:31:53 | INFO | train_inner | epoch 018:    253 / 1122 loss=6.468, nll_loss=2.945, mask_ins=1.086, word_ins_ml=4.524, word_reposition=0.859, ppl=88.55, wps=22935.7, ups=1.11, wpb=20600.5, bsz=256, num_updates=19300, lr=0.000254493, gnorm=1.949, clip=0, loss_scale=1024, train_wall=89, wall=17730
2022-07-11 22:33:22 | INFO | train_inner | epoch 018:    353 / 1122 loss=6.492, nll_loss=2.971, mask_ins=1.083, word_ins_ml=4.547, word_reposition=0.863, ppl=90.02, wps=22902.8, ups=1.12, wpb=20465.5, bsz=256, num_updates=19400, lr=0.000253837, gnorm=1.911, clip=0, loss_scale=1024, train_wall=89, wall=17819
2022-07-11 22:34:52 | INFO | train_inner | epoch 018:    453 / 1122 loss=6.442, nll_loss=2.93, mask_ins=1.082, word_ins_ml=4.51, word_reposition=0.85, ppl=86.93, wps=22876.1, ups=1.11, wpb=20558.4, bsz=256, num_updates=19500, lr=0.000253185, gnorm=1.91, clip=0, loss_scale=1024, train_wall=89, wall=17909
2022-07-11 22:36:21 | INFO | train_inner | epoch 018:    553 / 1122 loss=6.433, nll_loss=2.936, mask_ins=1.065, word_ins_ml=4.515, word_reposition=0.854, ppl=86.4, wps=22981.5, ups=1.13, wpb=20426.6, bsz=256, num_updates=19600, lr=0.000252538, gnorm=1.911, clip=0, loss_scale=1024, train_wall=88, wall=17998
2022-07-11 22:37:50 | INFO | train_inner | epoch 018:    653 / 1122 loss=6.443, nll_loss=2.935, mask_ins=1.076, word_ins_ml=4.514, word_reposition=0.853, ppl=86.98, wps=23023.9, ups=1.13, wpb=20449.8, bsz=256, num_updates=19700, lr=0.000251896, gnorm=1.93, clip=0, loss_scale=1720, train_wall=88, wall=18087
2022-07-11 22:39:19 | INFO | train_inner | epoch 018:    753 / 1122 loss=6.421, nll_loss=2.91, mask_ins=1.073, word_ins_ml=4.492, word_reposition=0.856, ppl=85.69, wps=23167.6, ups=1.12, wpb=20640.4, bsz=256, num_updates=19800, lr=0.000251259, gnorm=1.9, clip=0, loss_scale=2048, train_wall=88, wall=18176
2022-07-11 22:40:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 22:40:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 22:40:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-11 22:40:52 | INFO | train_inner | epoch 018:    856 / 1122 loss=6.419, nll_loss=2.923, mask_ins=1.059, word_ins_ml=4.503, word_reposition=0.857, ppl=85.54, wps=22164.7, ups=1.07, wpb=20685, bsz=256, num_updates=19900, lr=0.000250627, gnorm=2.558, clip=0, loss_scale=1422, train_wall=93, wall=18269
2022-07-11 22:42:21 | INFO | train_inner | epoch 018:    956 / 1122 loss=6.394, nll_loss=2.895, mask_ins=1.063, word_ins_ml=4.477, word_reposition=0.853, ppl=84.07, wps=22933.9, ups=1.12, wpb=20467.2, bsz=256, num_updates=20000, lr=0.00025, gnorm=1.991, clip=0, loss_scale=256, train_wall=89, wall=18358
2022-07-11 22:43:50 | INFO | train_inner | epoch 018:   1056 / 1122 loss=6.374, nll_loss=2.881, mask_ins=1.057, word_ins_ml=4.464, word_reposition=0.852, ppl=82.94, wps=23022.9, ups=1.12, wpb=20511.2, bsz=256, num_updates=20100, lr=0.000249377, gnorm=1.958, clip=0, loss_scale=256, train_wall=88, wall=18447
2022-07-11 22:44:49 | INFO | train | epoch 018 | loss 6.441 | nll_loss 2.931 | mask_ins 1.074 | word_ins_ml 4.51 | word_reposition 0.857 | ppl 86.9 | wps 22429.7 | ups 1.09 | wpb 20521.3 | bsz 255.8 | num_updates 20166 | lr 0.000248969 | gnorm 1.998 | clip 0 | loss_scale 998 | train_wall 996 | wall 18506
2022-07-11 22:45:07 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.457 | nll_loss 6.139 | mask_ins 1.545 | word_ins_ml 7.548 | word_reposition 1.364 | ppl 1405.56 | wps 55560.4 | wpb 2367.6 | bsz 32 | num_updates 20166 | best_loss 10.339
2022-07-11 22:45:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 18 @ 20166 updates, score 10.457) (writing took 2.7239551274105906 seconds)
2022-07-11 22:45:40 | INFO | train_inner | epoch 019:     34 / 1122 loss=6.39, nll_loss=2.9, mask_ins=1.055, word_ins_ml=4.481, word_reposition=0.853, ppl=83.84, wps=18556.4, ups=0.91, wpb=20320.3, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=2.036, clip=0, loss_scale=256, train_wall=88, wall=18557
2022-07-11 22:47:09 | INFO | train_inner | epoch 019:    134 / 1122 loss=6.368, nll_loss=2.89, mask_ins=1.05, word_ins_ml=4.473, word_reposition=0.845, ppl=82.58, wps=22908, ups=1.13, wpb=20340, bsz=256, num_updates=20300, lr=0.000248146, gnorm=1.937, clip=0, loss_scale=256, train_wall=88, wall=18646
2022-07-11 22:48:38 | INFO | train_inner | epoch 019:    234 / 1122 loss=6.311, nll_loss=2.834, mask_ins=1.045, word_ins_ml=4.423, word_reposition=0.844, ppl=79.38, wps=23054.3, ups=1.12, wpb=20538.6, bsz=256, num_updates=20400, lr=0.000247537, gnorm=2.05, clip=0, loss_scale=282, train_wall=88, wall=18735
2022-07-11 22:50:07 | INFO | train_inner | epoch 019:    334 / 1122 loss=6.341, nll_loss=2.866, mask_ins=1.051, word_ins_ml=4.45, word_reposition=0.84, ppl=81.06, wps=23057.3, ups=1.12, wpb=20526.8, bsz=256, num_updates=20500, lr=0.000246932, gnorm=2.008, clip=0, loss_scale=512, train_wall=88, wall=18824
2022-07-11 22:51:36 | INFO | train_inner | epoch 019:    434 / 1122 loss=6.321, nll_loss=2.844, mask_ins=1.05, word_ins_ml=4.431, word_reposition=0.84, ppl=79.94, wps=22952.6, ups=1.12, wpb=20402.4, bsz=256, num_updates=20600, lr=0.000246332, gnorm=1.915, clip=0, loss_scale=512, train_wall=88, wall=18913
2022-07-11 22:53:05 | INFO | train_inner | epoch 019:    534 / 1122 loss=6.303, nll_loss=2.841, mask_ins=1.041, word_ins_ml=4.427, word_reposition=0.835, ppl=78.97, wps=23234.2, ups=1.12, wpb=20659.2, bsz=256, num_updates=20700, lr=0.000245737, gnorm=1.879, clip=0, loss_scale=512, train_wall=88, wall=19002
2022-07-11 22:54:34 | INFO | train_inner | epoch 019:    634 / 1122 loss=6.257, nll_loss=2.814, mask_ins=1.021, word_ins_ml=4.404, word_reposition=0.832, ppl=76.49, wps=23031.7, ups=1.12, wpb=20619.6, bsz=256, num_updates=20800, lr=0.000245145, gnorm=1.885, clip=0, loss_scale=512, train_wall=89, wall=19091
2022-07-11 22:56:03 | INFO | train_inner | epoch 019:    734 / 1122 loss=6.282, nll_loss=2.823, mask_ins=1.025, word_ins_ml=4.412, word_reposition=0.845, ppl=77.8, wps=23225, ups=1.12, wpb=20709.2, bsz=256, num_updates=20900, lr=0.000244558, gnorm=1.933, clip=0, loss_scale=512, train_wall=89, wall=19180
2022-07-11 22:57:33 | INFO | train_inner | epoch 019:    834 / 1122 loss=6.245, nll_loss=2.798, mask_ins=1.026, word_ins_ml=4.389, word_reposition=0.83, ppl=75.83, wps=23031.9, ups=1.12, wpb=20558.9, bsz=256, num_updates=21000, lr=0.000243975, gnorm=1.945, clip=0, loss_scale=1014, train_wall=89, wall=19270
2022-07-11 22:59:01 | INFO | train_inner | epoch 019:    934 / 1122 loss=6.232, nll_loss=2.798, mask_ins=1.025, word_ins_ml=4.388, word_reposition=0.82, ppl=75.17, wps=22984.7, ups=1.12, wpb=20432.8, bsz=256, num_updates=21100, lr=0.000243396, gnorm=1.882, clip=0, loss_scale=1024, train_wall=88, wall=19359
2022-07-11 23:00:31 | INFO | train_inner | epoch 019:   1034 / 1122 loss=6.229, nll_loss=2.786, mask_ins=1.02, word_ins_ml=4.378, word_reposition=0.832, ppl=75, wps=23033.2, ups=1.12, wpb=20620.4, bsz=256, num_updates=21200, lr=0.000242821, gnorm=1.899, clip=0, loss_scale=1024, train_wall=89, wall=19448
2022-07-11 23:01:49 | INFO | train | epoch 019 | loss 6.287 | nll_loss 2.828 | mask_ins 1.035 | word_ins_ml 4.416 | word_reposition 0.836 | ppl 78.1 | wps 22568.9 | ups 1.1 | wpb 20519.9 | bsz 255.8 | num_updates 21288 | lr 0.000242319 | gnorm 1.936 | clip 0 | loss_scale 637 | train_wall 993 | wall 19526
2022-07-11 23:02:07 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.469 | nll_loss 6.117 | mask_ins 1.569 | word_ins_ml 7.525 | word_reposition 1.375 | ppl 1417.44 | wps 55768.3 | wpb 2367.6 | bsz 32 | num_updates 21288 | best_loss 10.339
2022-07-11 23:02:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 19 @ 21288 updates, score 10.469) (writing took 2.803321515209973 seconds)
2022-07-11 23:02:20 | INFO | train_inner | epoch 020:     12 / 1122 loss=6.242, nll_loss=2.796, mask_ins=1.024, word_ins_ml=4.386, word_reposition=0.831, ppl=75.67, wps=18649.8, ups=0.91, wpb=20413.8, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=1.947, clip=0, loss_scale=1024, train_wall=88, wall=19558
2022-07-11 23:03:50 | INFO | train_inner | epoch 020:    112 / 1122 loss=6.201, nll_loss=2.765, mask_ins=1.011, word_ins_ml=4.359, word_reposition=0.831, ppl=73.56, wps=22887.3, ups=1.11, wpb=20531.2, bsz=256, num_updates=21400, lr=0.000241684, gnorm=1.93, clip=0, loss_scale=1024, train_wall=89, wall=19647
2022-07-11 23:05:20 | INFO | train_inner | epoch 020:    212 / 1122 loss=6.164, nll_loss=2.736, mask_ins=1.01, word_ins_ml=4.333, word_reposition=0.821, ppl=71.7, wps=22706.2, ups=1.11, wpb=20515.2, bsz=256, num_updates=21500, lr=0.000241121, gnorm=1.925, clip=0, loss_scale=1905, train_wall=90, wall=19738
2022-07-11 23:06:50 | INFO | train_inner | epoch 020:    312 / 1122 loss=6.142, nll_loss=2.711, mask_ins=1.012, word_ins_ml=4.31, word_reposition=0.82, ppl=70.6, wps=23019.8, ups=1.12, wpb=20617.9, bsz=256, num_updates=21600, lr=0.000240563, gnorm=1.862, clip=0, loss_scale=2048, train_wall=89, wall=19827
2022-07-11 23:08:19 | INFO | train_inner | epoch 020:    412 / 1122 loss=6.152, nll_loss=2.739, mask_ins=1.001, word_ins_ml=4.335, word_reposition=0.817, ppl=71.13, wps=23090.5, ups=1.12, wpb=20584.1, bsz=256, num_updates=21700, lr=0.000240008, gnorm=1.858, clip=0, loss_scale=2048, train_wall=89, wall=19916
2022-07-11 23:09:48 | INFO | train_inner | epoch 020:    512 / 1122 loss=6.057, nll_loss=2.662, mask_ins=0.985, word_ins_ml=4.266, word_reposition=0.806, ppl=66.57, wps=23056.6, ups=1.13, wpb=20491.9, bsz=256, num_updates=21800, lr=0.000239457, gnorm=1.898, clip=0, loss_scale=2048, train_wall=88, wall=20005
2022-07-11 23:11:17 | INFO | train_inner | epoch 020:    612 / 1122 loss=6.154, nll_loss=2.737, mask_ins=1.002, word_ins_ml=4.332, word_reposition=0.819, ppl=71.2, wps=23105.4, ups=1.12, wpb=20615.6, bsz=256, num_updates=21900, lr=0.000238909, gnorm=1.891, clip=0, loss_scale=2048, train_wall=89, wall=20094
2022-07-11 23:12:46 | INFO | train_inner | epoch 020:    712 / 1122 loss=6.124, nll_loss=2.722, mask_ins=0.993, word_ins_ml=4.319, word_reposition=0.813, ppl=69.75, wps=22935, ups=1.12, wpb=20391.8, bsz=256, num_updates=22000, lr=0.000238366, gnorm=1.911, clip=0, loss_scale=3564, train_wall=88, wall=20183
2022-07-11 23:14:15 | INFO | train_inner | epoch 020:    812 / 1122 loss=6.097, nll_loss=2.698, mask_ins=0.992, word_ins_ml=4.297, word_reposition=0.808, ppl=68.45, wps=23059.3, ups=1.13, wpb=20495.4, bsz=256, num_updates=22100, lr=0.000237826, gnorm=1.89, clip=0, loss_scale=4096, train_wall=88, wall=20272
2022-07-11 23:14:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 23:14:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-11 23:15:47 | INFO | train_inner | epoch 020:    914 / 1122 loss=6.098, nll_loss=2.688, mask_ins=0.996, word_ins_ml=4.288, word_reposition=0.814, ppl=68.5, wps=22466.1, ups=1.09, wpb=20602.3, bsz=256, num_updates=22200, lr=0.000237289, gnorm=1.905, clip=0, loss_scale=1787, train_wall=91, wall=20364
2022-07-11 23:17:16 | INFO | train_inner | epoch 020:   1014 / 1122 loss=6.089, nll_loss=2.691, mask_ins=0.985, word_ins_ml=4.29, word_reposition=0.813, ppl=68.06, wps=23054.6, ups=1.12, wpb=20558.2, bsz=256, num_updates=22300, lr=0.000236757, gnorm=1.944, clip=0, loss_scale=1024, train_wall=89, wall=20453
2022-07-11 23:18:45 | INFO | train_inner | epoch 020:   1114 / 1122 loss=6.12, nll_loss=2.721, mask_ins=0.996, word_ins_ml=4.317, word_reposition=0.807, ppl=69.54, wps=22874.3, ups=1.12, wpb=20480, bsz=256, num_updates=22400, lr=0.000236228, gnorm=1.914, clip=0, loss_scale=1024, train_wall=89, wall=20543
2022-07-11 23:18:52 | INFO | train | epoch 020 | loss 6.129 | nll_loss 2.716 | mask_ins 0.999 | word_ins_ml 4.314 | word_reposition 0.816 | ppl 69.98 | wps 22460.1 | ups 1.09 | wpb 20521 | bsz 255.8 | num_updates 22408 | lr 0.000236186 | gnorm 1.912 | clip 0 | loss_scale 2037 | train_wall 996 | wall 20550
2022-07-11 23:19:10 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 10.164 | nll_loss 5.954 | mask_ins 1.507 | word_ins_ml 7.375 | word_reposition 1.282 | ppl 1147.6 | wps 55802.9 | wpb 2367.6 | bsz 32 | num_updates 22408 | best_loss 10.164
2022-07-11 23:19:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 20 @ 22408 updates, score 10.164) (writing took 5.048542372882366 seconds)
2022-07-11 23:19:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-11 23:20:39 | INFO | train_inner | epoch 021:     93 / 1122 loss=6.183, nll_loss=2.773, mask_ins=1, word_ins_ml=4.363, word_reposition=0.819, ppl=72.65, wps=18032.1, ups=0.88, wpb=20452.9, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=5.159, clip=2, loss_scale=669, train_wall=90, wall=20656
2022-07-11 23:22:08 | INFO | train_inner | epoch 021:    193 / 1122 loss=6.08, nll_loss=2.671, mask_ins=0.992, word_ins_ml=4.273, word_reposition=0.816, ppl=67.67, wps=22974.6, ups=1.12, wpb=20450.2, bsz=256, num_updates=22600, lr=0.00023518, gnorm=1.98, clip=0, loss_scale=512, train_wall=88, wall=20745
2022-07-11 23:22:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-11 23:23:38 | INFO | train_inner | epoch 021:    294 / 1122 loss=6.065, nll_loss=2.68, mask_ins=0.977, word_ins_ml=4.28, word_reposition=0.809, ppl=66.96, wps=22924.6, ups=1.11, wpb=20655.3, bsz=256, num_updates=22700, lr=0.000234662, gnorm=1.889, clip=0, loss_scale=291, train_wall=89, wall=20835
2022-07-11 23:25:07 | INFO | train_inner | epoch 021:    394 / 1122 loss=6.019, nll_loss=2.631, mask_ins=0.977, word_ins_ml=4.237, word_reposition=0.805, ppl=64.84, wps=23044.2, ups=1.12, wpb=20516.2, bsz=256, num_updates=22800, lr=0.000234146, gnorm=2.092, clip=0, loss_scale=256, train_wall=88, wall=20924
2022-07-11 23:26:36 | INFO | train_inner | epoch 021:    494 / 1122 loss=6.032, nll_loss=2.66, mask_ins=0.969, word_ins_ml=4.262, word_reposition=0.801, ppl=65.45, wps=23128, ups=1.12, wpb=20629.1, bsz=256, num_updates=22900, lr=0.000233635, gnorm=1.889, clip=0, loss_scale=256, train_wall=89, wall=21013
2022-07-11 23:28:05 | INFO | train_inner | epoch 021:    594 / 1122 loss=6.026, nll_loss=2.647, mask_ins=0.971, word_ins_ml=4.25, word_reposition=0.805, ppl=65.18, wps=23057.2, ups=1.12, wpb=20512.8, bsz=256, num_updates=23000, lr=0.000233126, gnorm=1.893, clip=0, loss_scale=256, train_wall=88, wall=21102
2022-07-11 23:29:34 | INFO | train_inner | epoch 021:    694 / 1122 loss=6.006, nll_loss=2.649, mask_ins=0.957, word_ins_ml=4.252, word_reposition=0.798, ppl=64.29, wps=22949.8, ups=1.12, wpb=20418.3, bsz=256, num_updates=23100, lr=0.000232621, gnorm=1.895, clip=0, loss_scale=256, train_wall=88, wall=21191
2022-07-11 23:31:03 | INFO | train_inner | epoch 021:    794 / 1122 loss=6.013, nll_loss=2.639, mask_ins=0.966, word_ins_ml=4.243, word_reposition=0.804, ppl=64.58, wps=23072.2, ups=1.12, wpb=20539.7, bsz=256, num_updates=23200, lr=0.000232119, gnorm=1.906, clip=0, loss_scale=448, train_wall=88, wall=21280
2022-07-11 23:32:33 | INFO | train_inner | epoch 021:    894 / 1122 loss=5.973, nll_loss=2.596, mask_ins=0.973, word_ins_ml=4.205, word_reposition=0.796, ppl=62.82, wps=22882.7, ups=1.12, wpb=20451.4, bsz=256, num_updates=23300, lr=0.000231621, gnorm=1.879, clip=0, loss_scale=512, train_wall=89, wall=21370
2022-07-11 23:34:02 | INFO | train_inner | epoch 021:    994 / 1122 loss=6.003, nll_loss=2.632, mask_ins=0.965, word_ins_ml=4.236, word_reposition=0.802, ppl=64.15, wps=23024.7, ups=1.12, wpb=20569, bsz=256, num_updates=23400, lr=0.000231125, gnorm=1.87, clip=0, loss_scale=512, train_wall=89, wall=21459
2022-07-11 23:35:31 | INFO | train_inner | epoch 021:   1094 / 1122 loss=6.024, nll_loss=2.655, mask_ins=0.963, word_ins_ml=4.256, word_reposition=0.805, ppl=65.08, wps=23011, ups=1.12, wpb=20480.3, bsz=256, num_updates=23500, lr=0.000230633, gnorm=1.892, clip=0, loss_scale=512, train_wall=88, wall=21548
2022-07-11 23:35:56 | INFO | train | epoch 021 | loss 6.036 | nll_loss 2.656 | mask_ins 0.973 | word_ins_ml 4.258 | word_reposition 0.805 | ppl 65.63 | wps 22457.3 | ups 1.09 | wpb 20520.7 | bsz 255.8 | num_updates 23528 | lr 0.000230496 | gnorm 2.206 | clip 0.2 | loss_scale 406 | train_wall 994 | wall 21573
2022-07-11 23:36:14 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 10.34 | nll_loss 6.026 | mask_ins 1.543 | word_ins_ml 7.444 | word_reposition 1.353 | ppl 1296.31 | wps 55613.7 | wpb 2367.6 | bsz 32 | num_updates 23528 | best_loss 10.164
2022-07-11 23:36:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 21 @ 23528 updates, score 10.34) (writing took 2.7672435799613595 seconds)
2022-07-11 23:37:22 | INFO | train_inner | epoch 022:     72 / 1122 loss=5.983, nll_loss=2.634, mask_ins=0.947, word_ins_ml=4.238, word_reposition=0.797, ppl=63.23, wps=18324.8, ups=0.9, wpb=20347.7, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=2.006, clip=0, loss_scale=512, train_wall=90, wall=21659
2022-07-11 23:38:51 | INFO | train_inner | epoch 022:    172 / 1122 loss=5.938, nll_loss=2.59, mask_ins=0.95, word_ins_ml=4.199, word_reposition=0.79, ppl=61.32, wps=23068.9, ups=1.12, wpb=20560.6, bsz=256, num_updates=23700, lr=0.000229658, gnorm=1.871, clip=0, loss_scale=835, train_wall=88, wall=21748
2022-07-11 23:40:20 | INFO | train_inner | epoch 022:    272 / 1122 loss=5.91, nll_loss=2.552, mask_ins=0.953, word_ins_ml=4.165, word_reposition=0.792, ppl=60.13, wps=23153.9, ups=1.12, wpb=20618.2, bsz=256, num_updates=23800, lr=0.000229175, gnorm=1.835, clip=0, loss_scale=1024, train_wall=88, wall=21837
2022-07-11 23:41:50 | INFO | train_inner | epoch 022:    372 / 1122 loss=5.905, nll_loss=2.563, mask_ins=0.938, word_ins_ml=4.174, word_reposition=0.793, ppl=59.91, wps=23014.4, ups=1.12, wpb=20632.8, bsz=256, num_updates=23900, lr=0.000228695, gnorm=1.843, clip=0, loss_scale=1024, train_wall=89, wall=21927
2022-07-11 23:43:19 | INFO | train_inner | epoch 022:    472 / 1122 loss=5.957, nll_loss=2.608, mask_ins=0.945, word_ins_ml=4.214, word_reposition=0.798, ppl=62.13, wps=23083.7, ups=1.12, wpb=20631.7, bsz=256, num_updates=24000, lr=0.000228218, gnorm=1.859, clip=0, loss_scale=1024, train_wall=89, wall=22016
2022-07-11 23:44:50 | INFO | train_inner | epoch 022:    572 / 1122 loss=5.94, nll_loss=2.587, mask_ins=0.956, word_ins_ml=4.196, word_reposition=0.788, ppl=61.38, wps=22621.1, ups=1.1, wpb=20507.6, bsz=256, num_updates=24100, lr=0.000227744, gnorm=1.827, clip=0, loss_scale=1024, train_wall=90, wall=22107
2022-07-11 23:46:20 | INFO | train_inner | epoch 022:    672 / 1122 loss=5.95, nll_loss=2.598, mask_ins=0.948, word_ins_ml=4.205, word_reposition=0.796, ppl=61.81, wps=22872.9, ups=1.11, wpb=20578, bsz=256, num_updates=24200, lr=0.000227273, gnorm=2.082, clip=0, loss_scale=1546, train_wall=89, wall=22197
2022-07-11 23:47:49 | INFO | train_inner | epoch 022:    772 / 1122 loss=5.897, nll_loss=2.553, mask_ins=0.94, word_ins_ml=4.164, word_reposition=0.792, ppl=59.58, wps=23040.6, ups=1.12, wpb=20533.8, bsz=256, num_updates=24300, lr=0.000226805, gnorm=1.868, clip=0, loss_scale=2048, train_wall=89, wall=22286
2022-07-11 23:49:18 | INFO | train_inner | epoch 022:    872 / 1122 loss=5.899, nll_loss=2.565, mask_ins=0.932, word_ins_ml=4.175, word_reposition=0.792, ppl=59.68, wps=22904, ups=1.12, wpb=20399, bsz=256, num_updates=24400, lr=0.000226339, gnorm=1.881, clip=0, loss_scale=2048, train_wall=88, wall=22375
2022-07-11 23:50:47 | INFO | train_inner | epoch 022:    972 / 1122 loss=5.886, nll_loss=2.549, mask_ins=0.935, word_ins_ml=4.16, word_reposition=0.791, ppl=59.14, wps=23130.7, ups=1.12, wpb=20630, bsz=256, num_updates=24500, lr=0.000225877, gnorm=1.87, clip=0, loss_scale=2048, train_wall=89, wall=22464
2022-07-11 23:52:16 | INFO | train_inner | epoch 022:   1072 / 1122 loss=5.918, nll_loss=2.572, mask_ins=0.942, word_ins_ml=4.181, word_reposition=0.795, ppl=60.45, wps=22901.4, ups=1.12, wpb=20420.8, bsz=256, num_updates=24600, lr=0.000225417, gnorm=1.933, clip=0, loss_scale=2048, train_wall=89, wall=22553
2022-07-11 23:53:00 | INFO | train | epoch 022 | loss 5.922 | nll_loss 2.577 | mask_ins 0.943 | word_ins_ml 4.187 | word_reposition 0.792 | ppl 60.65 | wps 22471.6 | ups 1.1 | wpb 20521 | bsz 255.8 | num_updates 24650 | lr 0.000225189 | gnorm 1.896 | clip 0 | loss_scale 1432 | train_wall 997 | wall 22598
2022-07-11 23:53:18 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 10.339 | nll_loss 5.967 | mask_ins 1.516 | word_ins_ml 7.397 | word_reposition 1.425 | ppl 1294.8 | wps 55719.6 | wpb 2367.6 | bsz 32 | num_updates 24650 | best_loss 10.164
2022-07-11 23:53:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 22 @ 24650 updates, score 10.339) (writing took 3.0496528623625636 seconds)
2022-07-11 23:54:06 | INFO | train_inner | epoch 023:     50 / 1122 loss=5.906, nll_loss=2.577, mask_ins=0.934, word_ins_ml=4.186, word_reposition=0.786, ppl=59.95, wps=18481.8, ups=0.91, wpb=20241.8, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=1.97, clip=0, loss_scale=2847, train_wall=88, wall=22663
2022-07-11 23:55:35 | INFO | train_inner | epoch 023:    150 / 1122 loss=5.837, nll_loss=2.513, mask_ins=0.924, word_ins_ml=4.128, word_reposition=0.786, ppl=57.17, wps=23217.8, ups=1.12, wpb=20689.2, bsz=256, num_updates=24800, lr=0.000224507, gnorm=1.914, clip=0, loss_scale=4096, train_wall=88, wall=22752
2022-07-11 23:56:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-11 23:57:06 | INFO | train_inner | epoch 023:    251 / 1122 loss=5.862, nll_loss=2.53, mask_ins=0.928, word_ins_ml=4.144, word_reposition=0.79, ppl=58.16, wps=22671.2, ups=1.1, wpb=20525.7, bsz=256, num_updates=24900, lr=0.000224055, gnorm=2.201, clip=0, loss_scale=3407, train_wall=90, wall=22843
2022-07-11 23:58:35 | INFO | train_inner | epoch 023:    351 / 1122 loss=5.868, nll_loss=2.525, mask_ins=0.936, word_ins_ml=4.139, word_reposition=0.793, ppl=58.42, wps=23014.7, ups=1.12, wpb=20560, bsz=256, num_updates=25000, lr=0.000223607, gnorm=1.842, clip=0, loss_scale=2048, train_wall=89, wall=22932
2022-07-12 00:00:04 | INFO | train_inner | epoch 023:    451 / 1122 loss=5.841, nll_loss=2.518, mask_ins=0.926, word_ins_ml=4.133, word_reposition=0.783, ppl=57.31, wps=22986.8, ups=1.12, wpb=20498.9, bsz=256, num_updates=25100, lr=0.000223161, gnorm=1.868, clip=0, loss_scale=2048, train_wall=89, wall=23021
2022-07-12 00:01:33 | INFO | train_inner | epoch 023:    551 / 1122 loss=5.808, nll_loss=2.488, mask_ins=0.925, word_ins_ml=4.106, word_reposition=0.777, ppl=56.02, wps=23077.6, ups=1.12, wpb=20535.5, bsz=256, num_updates=25200, lr=0.000222718, gnorm=1.843, clip=0, loss_scale=2048, train_wall=88, wall=23110
2022-07-12 00:03:02 | INFO | train_inner | epoch 023:    651 / 1122 loss=5.893, nll_loss=2.555, mask_ins=0.933, word_ins_ml=4.165, word_reposition=0.795, ppl=59.43, wps=23114.7, ups=1.12, wpb=20566.7, bsz=256, num_updates=25300, lr=0.000222277, gnorm=1.853, clip=0, loss_scale=2048, train_wall=88, wall=23199
2022-07-12 00:04:31 | INFO | train_inner | epoch 023:    751 / 1122 loss=5.83, nll_loss=2.511, mask_ins=0.925, word_ins_ml=4.125, word_reposition=0.78, ppl=56.87, wps=23082.8, ups=1.12, wpb=20547, bsz=256, num_updates=25400, lr=0.000221839, gnorm=1.849, clip=0, loss_scale=2499, train_wall=88, wall=23288
2022-07-12 00:06:00 | INFO | train_inner | epoch 023:    851 / 1122 loss=5.849, nll_loss=2.53, mask_ins=0.926, word_ins_ml=4.143, word_reposition=0.781, ppl=57.64, wps=23000.6, ups=1.12, wpb=20525.8, bsz=256, num_updates=25500, lr=0.000221404, gnorm=1.837, clip=0, loss_scale=4096, train_wall=89, wall=23377
2022-07-12 00:07:30 | INFO | train_inner | epoch 023:    951 / 1122 loss=5.82, nll_loss=2.503, mask_ins=0.924, word_ins_ml=4.118, word_reposition=0.778, ppl=56.49, wps=23008.9, ups=1.12, wpb=20609.6, bsz=256, num_updates=25600, lr=0.000220971, gnorm=1.829, clip=0, loss_scale=4096, train_wall=89, wall=23467
2022-07-12 00:08:59 | INFO | train_inner | epoch 023:   1051 / 1122 loss=5.782, nll_loss=2.484, mask_ins=0.914, word_ins_ml=4.101, word_reposition=0.767, ppl=55.04, wps=22972.8, ups=1.12, wpb=20427.3, bsz=256, num_updates=25700, lr=0.000220541, gnorm=1.801, clip=0, loss_scale=4096, train_wall=88, wall=23556
2022-07-12 00:10:02 | INFO | train | epoch 023 | loss 5.84 | nll_loss 2.517 | mask_ins 0.926 | word_ins_ml 4.131 | word_reposition 0.782 | ppl 57.29 | wps 22522.4 | ups 1.1 | wpb 20521.8 | bsz 255.8 | num_updates 25771 | lr 0.000220237 | gnorm 1.888 | clip 0 | loss_scale 3141 | train_wall 994 | wall 23619
2022-07-12 00:10:20 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 10.145 | nll_loss 5.88 | mask_ins 1.484 | word_ins_ml 7.307 | word_reposition 1.354 | ppl 1131.88 | wps 55511.2 | wpb 2367.6 | bsz 32 | num_updates 25771 | best_loss 10.145
2022-07-12 00:10:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 23 @ 25771 updates, score 10.145) (writing took 5.097913219593465 seconds)
2022-07-12 00:10:51 | INFO | train_inner | epoch 024:     29 / 1122 loss=5.791, nll_loss=2.474, mask_ins=0.924, word_ins_ml=4.093, word_reposition=0.774, ppl=55.36, wps=18063.5, ups=0.89, wpb=20232.6, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=1.916, clip=0, loss_scale=4096, train_wall=88, wall=23668
2022-07-12 00:12:21 | INFO | train_inner | epoch 024:    129 / 1122 loss=5.813, nll_loss=2.5, mask_ins=0.918, word_ins_ml=4.115, word_reposition=0.779, ppl=56.21, wps=22832.5, ups=1.11, wpb=20501.2, bsz=256, num_updates=25900, lr=0.000219687, gnorm=1.81, clip=0, loss_scale=4506, train_wall=89, wall=23758
2022-07-12 00:13:50 | INFO | train_inner | epoch 024:    229 / 1122 loss=5.8, nll_loss=2.485, mask_ins=0.911, word_ins_ml=4.102, word_reposition=0.787, ppl=55.72, wps=22851.5, ups=1.12, wpb=20491.8, bsz=256, num_updates=26000, lr=0.000219265, gnorm=1.832, clip=0, loss_scale=8192, train_wall=89, wall=23847
2022-07-12 00:15:19 | INFO | train_inner | epoch 024:    329 / 1122 loss=5.758, nll_loss=2.456, mask_ins=0.91, word_ins_ml=4.076, word_reposition=0.772, ppl=54.12, wps=22972.4, ups=1.12, wpb=20492.8, bsz=256, num_updates=26100, lr=0.000218844, gnorm=1.825, clip=0, loss_scale=8192, train_wall=89, wall=23937
2022-07-12 00:16:49 | INFO | train_inner | epoch 024:    429 / 1122 loss=5.759, nll_loss=2.446, mask_ins=0.915, word_ins_ml=4.067, word_reposition=0.777, ppl=54.15, wps=23038.2, ups=1.12, wpb=20593.1, bsz=256, num_updates=26200, lr=0.000218426, gnorm=1.872, clip=0, loss_scale=8192, train_wall=89, wall=24026
2022-07-12 00:18:18 | INFO | train_inner | epoch 024:    529 / 1122 loss=5.738, nll_loss=2.441, mask_ins=0.907, word_ins_ml=4.062, word_reposition=0.768, ppl=53.37, wps=23007.4, ups=1.12, wpb=20554.2, bsz=256, num_updates=26300, lr=0.00021801, gnorm=1.813, clip=0, loss_scale=8192, train_wall=89, wall=24115
2022-07-12 00:19:48 | INFO | train_inner | epoch 024:    629 / 1122 loss=5.729, nll_loss=2.426, mask_ins=0.911, word_ins_ml=4.049, word_reposition=0.769, ppl=53.04, wps=22905.9, ups=1.12, wpb=20472, bsz=256, num_updates=26400, lr=0.000217597, gnorm=1.829, clip=0, loss_scale=8192, train_wall=89, wall=24205
2022-07-12 00:20:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 00:21:18 | INFO | train_inner | epoch 024:    730 / 1122 loss=5.786, nll_loss=2.495, mask_ins=0.903, word_ins_ml=4.109, word_reposition=0.774, ppl=55.18, wps=22663.4, ups=1.1, wpb=20596.9, bsz=256, num_updates=26500, lr=0.000217186, gnorm=1.831, clip=0, loss_scale=10382, train_wall=90, wall=24296
2022-07-12 00:22:48 | INFO | train_inner | epoch 024:    830 / 1122 loss=5.706, nll_loss=2.41, mask_ins=0.904, word_ins_ml=4.034, word_reposition=0.768, ppl=52.21, wps=22984.9, ups=1.12, wpb=20572, bsz=256, num_updates=26600, lr=0.000216777, gnorm=1.813, clip=0, loss_scale=8192, train_wall=89, wall=24385
2022-07-12 00:24:18 | INFO | train_inner | epoch 024:    930 / 1122 loss=5.77, nll_loss=2.464, mask_ins=0.917, word_ins_ml=4.082, word_reposition=0.771, ppl=54.57, wps=22908.1, ups=1.11, wpb=20566.2, bsz=256, num_updates=26700, lr=0.000216371, gnorm=1.822, clip=0, loss_scale=8192, train_wall=89, wall=24475
2022-07-12 00:25:47 | INFO | train_inner | epoch 024:   1030 / 1122 loss=5.764, nll_loss=2.457, mask_ins=0.913, word_ins_ml=4.075, word_reposition=0.776, ppl=54.34, wps=22896, ups=1.12, wpb=20526, bsz=256, num_updates=26800, lr=0.000215967, gnorm=1.807, clip=0, loss_scale=8192, train_wall=89, wall=24564
2022-07-12 00:27:10 | INFO | train | epoch 024 | loss 5.762 | nll_loss 2.458 | mask_ins 0.911 | word_ins_ml 4.077 | word_reposition 0.774 | ppl 54.25 | wps 22374.5 | ups 1.09 | wpb 20521.1 | bsz 255.8 | num_updates 26892 | lr 0.000215597 | gnorm 1.833 | clip 0 | loss_scale 7955 | train_wall 998 | wall 24647
2022-07-12 00:27:28 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 10.172 | nll_loss 5.875 | mask_ins 1.53 | word_ins_ml 7.306 | word_reposition 1.335 | ppl 1153.5 | wps 55602.4 | wpb 2367.6 | bsz 32 | num_updates 26892 | best_loss 10.145
2022-07-12 00:27:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 24 @ 26892 updates, score 10.172) (writing took 2.683139287866652 seconds)
2022-07-12 00:27:38 | INFO | train_inner | epoch 025:      8 / 1122 loss=5.752, nll_loss=2.458, mask_ins=0.903, word_ins_ml=4.076, word_reposition=0.774, ppl=53.91, wps=18491.3, ups=0.9, wpb=20453.9, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=1.895, clip=0, loss_scale=8192, train_wall=89, wall=24675
2022-07-12 00:29:09 | INFO | train_inner | epoch 025:    108 / 1122 loss=5.69, nll_loss=2.407, mask_ins=0.891, word_ins_ml=4.031, word_reposition=0.768, ppl=51.63, wps=22763.9, ups=1.1, wpb=20614.8, bsz=256, num_updates=27000, lr=0.000215166, gnorm=1.811, clip=0, loss_scale=13107, train_wall=90, wall=24766
2022-07-12 00:30:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 00:30:40 | INFO | train_inner | epoch 025:    209 / 1122 loss=5.725, nll_loss=2.434, mask_ins=0.896, word_ins_ml=4.055, word_reposition=0.773, ppl=52.88, wps=22419.3, ups=1.09, wpb=20510.5, bsz=256, num_updates=27100, lr=0.000214768, gnorm=1.813, clip=0, loss_scale=14032, train_wall=91, wall=24857
2022-07-12 00:32:10 | INFO | train_inner | epoch 025:    309 / 1122 loss=5.684, nll_loss=2.395, mask_ins=0.898, word_ins_ml=4.02, word_reposition=0.766, ppl=51.41, wps=22988.7, ups=1.12, wpb=20607.6, bsz=256, num_updates=27200, lr=0.000214373, gnorm=1.815, clip=0, loss_scale=8192, train_wall=89, wall=24947
2022-07-12 00:33:39 | INFO | train_inner | epoch 025:    409 / 1122 loss=5.696, nll_loss=2.425, mask_ins=0.884, word_ins_ml=4.047, word_reposition=0.765, ppl=51.84, wps=23068, ups=1.12, wpb=20628.3, bsz=256, num_updates=27300, lr=0.00021398, gnorm=1.821, clip=0, loss_scale=8192, train_wall=89, wall=25036
2022-07-12 00:35:08 | INFO | train_inner | epoch 025:    509 / 1122 loss=5.701, nll_loss=2.418, mask_ins=0.9, word_ins_ml=4.041, word_reposition=0.76, ppl=52.04, wps=22956.9, ups=1.12, wpb=20458.7, bsz=256, num_updates=27400, lr=0.000213589, gnorm=1.832, clip=0, loss_scale=8192, train_wall=89, wall=25125
2022-07-12 00:36:37 | INFO | train_inner | epoch 025:    609 / 1122 loss=5.672, nll_loss=2.383, mask_ins=0.898, word_ins_ml=4.01, word_reposition=0.765, ppl=50.98, wps=22885.8, ups=1.12, wpb=20426.7, bsz=256, num_updates=27500, lr=0.000213201, gnorm=1.767, clip=0, loss_scale=8192, train_wall=89, wall=25215
2022-07-12 00:38:07 | INFO | train_inner | epoch 025:    709 / 1122 loss=5.698, nll_loss=2.418, mask_ins=0.892, word_ins_ml=4.04, word_reposition=0.765, ppl=51.9, wps=22971.6, ups=1.12, wpb=20520.1, bsz=256, num_updates=27600, lr=0.000212814, gnorm=1.77, clip=0, loss_scale=9585, train_wall=89, wall=25304
2022-07-12 00:39:36 | INFO | train_inner | epoch 025:    809 / 1122 loss=5.661, nll_loss=2.385, mask_ins=0.889, word_ins_ml=4.01, word_reposition=0.762, ppl=50.58, wps=23165.3, ups=1.12, wpb=20736.3, bsz=256, num_updates=27700, lr=0.00021243, gnorm=1.822, clip=0, loss_scale=16384, train_wall=89, wall=25393
2022-07-12 00:39:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 00:41:07 | INFO | train_inner | epoch 025:    910 / 1122 loss=5.659, nll_loss=2.391, mask_ins=0.89, word_ins_ml=4.016, word_reposition=0.754, ppl=50.53, wps=22533.6, ups=1.1, wpb=20424.1, bsz=256, num_updates=27800, lr=0.000212047, gnorm=1.784, clip=0, loss_scale=8435, train_wall=90, wall=25484
2022-07-12 00:42:39 | INFO | train_inner | epoch 025:   1010 / 1122 loss=5.658, nll_loss=2.399, mask_ins=0.881, word_ins_ml=4.023, word_reposition=0.755, ppl=50.5, wps=22254.9, ups=1.09, wpb=20434, bsz=256, num_updates=27900, lr=0.000211667, gnorm=1.835, clip=0, loss_scale=8192, train_wall=91, wall=25576
2022-07-12 00:44:09 | INFO | train_inner | epoch 025:   1110 / 1122 loss=5.696, nll_loss=2.407, mask_ins=0.901, word_ins_ml=4.03, word_reposition=0.765, ppl=51.84, wps=22655.8, ups=1.11, wpb=20450.4, bsz=256, num_updates=28000, lr=0.000211289, gnorm=1.799, clip=0, loss_scale=8192, train_wall=90, wall=25666
2022-07-12 00:44:20 | INFO | train | epoch 025 | loss 5.686 | nll_loss 2.406 | mask_ins 0.892 | word_ins_ml 4.029 | word_reposition 0.764 | ppl 51.47 | wps 22319.9 | ups 1.09 | wpb 20521.8 | bsz 255.8 | num_updates 28012 | lr 0.000211243 | gnorm 1.812 | clip 0 | loss_scale 10032 | train_wall 1002 | wall 25677
2022-07-12 00:44:38 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 10.33 | nll_loss 6.01 | mask_ins 1.519 | word_ins_ml 7.434 | word_reposition 1.377 | ppl 1287.02 | wps 55233.7 | wpb 2367.6 | bsz 32 | num_updates 28012 | best_loss 10.145
2022-07-12 00:44:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 25 @ 28012 updates, score 10.33) (writing took 2.810303504578769 seconds)
2022-07-12 00:46:01 | INFO | train_inner | epoch 026:     88 / 1122 loss=5.647, nll_loss=2.378, mask_ins=0.884, word_ins_ml=4.005, word_reposition=0.759, ppl=50.11, wps=18274.5, ups=0.89, wpb=20445.2, bsz=253.8, num_updates=28100, lr=0.000210912, gnorm=1.868, clip=0, loss_scale=8192, train_wall=91, wall=25778
2022-07-12 00:47:30 | INFO | train_inner | epoch 026:    188 / 1122 loss=5.632, nll_loss=2.368, mask_ins=0.877, word_ins_ml=3.995, word_reposition=0.76, ppl=49.6, wps=22947.8, ups=1.12, wpb=20500.7, bsz=256, num_updates=28200, lr=0.000210538, gnorm=1.793, clip=0, loss_scale=8192, train_wall=89, wall=25867
2022-07-12 00:49:00 | INFO | train_inner | epoch 026:    288 / 1122 loss=5.595, nll_loss=2.334, mask_ins=0.875, word_ins_ml=3.965, word_reposition=0.754, ppl=48.34, wps=22788.7, ups=1.12, wpb=20419.3, bsz=256, num_updates=28300, lr=0.000210166, gnorm=1.776, clip=0, loss_scale=15237, train_wall=89, wall=25957
2022-07-12 00:49:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 00:50:30 | INFO | train_inner | epoch 026:    389 / 1122 loss=5.661, nll_loss=2.391, mask_ins=0.885, word_ins_ml=4.015, word_reposition=0.76, ppl=50.6, wps=22788.1, ups=1.11, wpb=20595.5, bsz=256, num_updates=28400, lr=0.000209795, gnorm=1.785, clip=0, loss_scale=11436, train_wall=90, wall=26047
2022-07-12 00:52:00 | INFO | train_inner | epoch 026:    489 / 1122 loss=5.631, nll_loss=2.367, mask_ins=0.882, word_ins_ml=3.994, word_reposition=0.756, ppl=49.57, wps=22959.8, ups=1.12, wpb=20553.1, bsz=256, num_updates=28500, lr=0.000209427, gnorm=1.78, clip=0, loss_scale=8192, train_wall=89, wall=26137
2022-07-12 00:53:30 | INFO | train_inner | epoch 026:    589 / 1122 loss=5.665, nll_loss=2.408, mask_ins=0.874, word_ins_ml=4.031, word_reposition=0.76, ppl=50.72, wps=22559.8, ups=1.1, wpb=20424.2, bsz=256, num_updates=28600, lr=0.000209061, gnorm=1.827, clip=0, loss_scale=8192, train_wall=90, wall=26227
2022-07-12 00:55:00 | INFO | train_inner | epoch 026:    689 / 1122 loss=5.651, nll_loss=2.382, mask_ins=0.888, word_ins_ml=4.007, word_reposition=0.756, ppl=50.25, wps=22798, ups=1.11, wpb=20563.5, bsz=256, num_updates=28700, lr=0.000208696, gnorm=1.813, clip=0, loss_scale=8192, train_wall=90, wall=26318
2022-07-12 00:56:30 | INFO | train_inner | epoch 026:    789 / 1122 loss=5.61, nll_loss=2.357, mask_ins=0.874, word_ins_ml=3.985, word_reposition=0.752, ppl=48.86, wps=23040.3, ups=1.12, wpb=20584.8, bsz=256, num_updates=28800, lr=0.000208333, gnorm=1.814, clip=0, loss_scale=8192, train_wall=89, wall=26407
2022-07-12 00:57:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 00:58:00 | INFO | train_inner | epoch 026:    890 / 1122 loss=5.619, nll_loss=2.351, mask_ins=0.882, word_ins_ml=3.979, word_reposition=0.758, ppl=49.14, wps=22814, ups=1.11, wpb=20626.7, bsz=256, num_updates=28900, lr=0.000207973, gnorm=1.752, clip=0, loss_scale=11761, train_wall=90, wall=26497
2022-07-12 00:59:29 | INFO | train_inner | epoch 026:    990 / 1122 loss=5.604, nll_loss=2.347, mask_ins=0.879, word_ins_ml=3.976, word_reposition=0.75, ppl=48.65, wps=23065, ups=1.12, wpb=20566.7, bsz=256, num_updates=29000, lr=0.000207614, gnorm=1.827, clip=0, loss_scale=8192, train_wall=89, wall=26587
2022-07-12 01:00:59 | INFO | train_inner | epoch 026:   1090 / 1122 loss=5.606, nll_loss=2.339, mask_ins=0.874, word_ins_ml=3.968, word_reposition=0.763, ppl=48.7, wps=23033.7, ups=1.12, wpb=20534, bsz=256, num_updates=29100, lr=0.000207257, gnorm=1.788, clip=0, loss_scale=8192, train_wall=89, wall=26676
2022-07-12 01:01:27 | INFO | train | epoch 026 | loss 5.629 | nll_loss 2.365 | mask_ins 0.879 | word_ins_ml 3.992 | word_reposition 0.757 | ppl 49.48 | wps 22373.2 | ups 1.09 | wpb 20519.9 | bsz 255.8 | num_updates 29132 | lr 0.000207143 | gnorm 1.8 | clip 0 | loss_scale 9433 | train_wall 1000 | wall 26704
2022-07-12 01:01:45 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.277 | nll_loss 5.989 | mask_ins 1.53 | word_ins_ml 7.417 | word_reposition 1.33 | ppl 1240.96 | wps 55573.4 | wpb 2367.6 | bsz 32 | num_updates 29132 | best_loss 10.145
2022-07-12 01:01:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 26 @ 29132 updates, score 10.277) (writing took 2.7105076359584928 seconds)
2022-07-12 01:01:50 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 01:02:50 | INFO | train_inner | epoch 027:     69 / 1122 loss=5.624, nll_loss=2.37, mask_ins=0.87, word_ins_ml=3.996, word_reposition=0.758, ppl=49.32, wps=18339.9, ups=0.9, wpb=20354.6, bsz=253.8, num_updates=29200, lr=0.000206901, gnorm=1.883, clip=0, loss_scale=5475, train_wall=90, wall=26787
2022-07-12 01:04:20 | INFO | train_inner | epoch 027:    169 / 1122 loss=5.596, nll_loss=2.335, mask_ins=0.879, word_ins_ml=3.964, word_reposition=0.753, ppl=48.37, wps=22845.1, ups=1.11, wpb=20672.9, bsz=256, num_updates=29300, lr=0.000206548, gnorm=1.78, clip=0, loss_scale=4096, train_wall=90, wall=26877
2022-07-12 01:05:49 | INFO | train_inner | epoch 027:    269 / 1122 loss=5.581, nll_loss=2.326, mask_ins=0.872, word_ins_ml=3.957, word_reposition=0.753, ppl=47.88, wps=23172.3, ups=1.12, wpb=20656.8, bsz=256, num_updates=29400, lr=0.000206197, gnorm=1.814, clip=0, loss_scale=4096, train_wall=89, wall=26966
2022-07-12 01:07:18 | INFO | train_inner | epoch 027:    369 / 1122 loss=5.579, nll_loss=2.329, mask_ins=0.872, word_ins_ml=3.959, word_reposition=0.748, ppl=47.79, wps=22764.1, ups=1.12, wpb=20329, bsz=256, num_updates=29500, lr=0.000205847, gnorm=1.791, clip=0, loss_scale=4096, train_wall=89, wall=27056
2022-07-12 01:08:48 | INFO | train_inner | epoch 027:    469 / 1122 loss=5.583, nll_loss=2.337, mask_ins=0.872, word_ins_ml=3.966, word_reposition=0.745, ppl=47.94, wps=22842.2, ups=1.12, wpb=20426, bsz=256, num_updates=29600, lr=0.000205499, gnorm=1.767, clip=0, loss_scale=4096, train_wall=89, wall=27145
2022-07-12 01:10:17 | INFO | train_inner | epoch 027:    569 / 1122 loss=5.559, nll_loss=2.324, mask_ins=0.857, word_ins_ml=3.954, word_reposition=0.747, ppl=47.13, wps=22999.7, ups=1.12, wpb=20549.6, bsz=256, num_updates=29700, lr=0.000205152, gnorm=1.804, clip=0, loss_scale=6349, train_wall=89, wall=27234
2022-07-12 01:11:47 | INFO | train_inner | epoch 027:    669 / 1122 loss=5.617, nll_loss=2.366, mask_ins=0.866, word_ins_ml=3.992, word_reposition=0.76, ppl=49.08, wps=22953.6, ups=1.12, wpb=20566.3, bsz=256, num_updates=29800, lr=0.000204808, gnorm=1.796, clip=0, loss_scale=8192, train_wall=89, wall=27324
2022-07-12 01:13:18 | INFO | train_inner | epoch 027:    769 / 1122 loss=5.556, nll_loss=2.317, mask_ins=0.863, word_ins_ml=3.948, word_reposition=0.745, ppl=47.03, wps=22644.8, ups=1.1, wpb=20544, bsz=256, num_updates=29900, lr=0.000204465, gnorm=1.75, clip=0, loss_scale=8192, train_wall=90, wall=27415
2022-07-12 01:14:48 | INFO | train_inner | epoch 027:    869 / 1122 loss=5.542, nll_loss=2.304, mask_ins=0.862, word_ins_ml=3.936, word_reposition=0.744, ppl=46.58, wps=22627, ups=1.1, wpb=20557.3, bsz=256, num_updates=30000, lr=0.000204124, gnorm=1.792, clip=0, loss_scale=8192, train_wall=90, wall=27506
2022-07-12 01:16:19 | INFO | train_inner | epoch 027:    969 / 1122 loss=5.553, nll_loss=2.321, mask_ins=0.857, word_ins_ml=3.952, word_reposition=0.744, ppl=46.95, wps=22778.9, ups=1.11, wpb=20589.5, bsz=256, num_updates=30100, lr=0.000203785, gnorm=1.777, clip=0, loss_scale=8192, train_wall=90, wall=27596
2022-07-12 01:16:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 01:16:39 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 01:16:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-12 01:17:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-12 01:17:51 | INFO | train_inner | epoch 027:   1073 / 1122 loss=5.542, nll_loss=2.299, mask_ins=0.869, word_ins_ml=3.932, word_reposition=0.741, ppl=46.58, wps=22140.4, ups=1.08, wpb=20502.6, bsz=256, num_updates=30200, lr=0.000203447, gnorm=2.017, clip=0, loss_scale=2270, train_wall=92, wall=27689
2022-07-12 01:18:35 | INFO | train | epoch 027 | loss 5.575 | nll_loss 2.33 | mask_ins 0.867 | word_ins_ml 3.959 | word_reposition 0.748 | ppl 47.68 | wps 22302.3 | ups 1.09 | wpb 20521.2 | bsz 255.8 | num_updates 30249 | lr 0.000203282 | gnorm 1.816 | clip 0 | loss_scale 5439 | train_wall 1000 | wall 27732
2022-07-12 01:18:53 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.158 | nll_loss 5.909 | mask_ins 1.492 | word_ins_ml 7.343 | word_reposition 1.323 | ppl 1142.86 | wps 55669.4 | wpb 2367.6 | bsz 32 | num_updates 30249 | best_loss 10.145
2022-07-12 01:18:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 27 @ 30249 updates, score 10.158) (writing took 2.673229810781777 seconds)
2022-07-12 01:19:41 | INFO | train_inner | epoch 028:     51 / 1122 loss=5.544, nll_loss=2.304, mask_ins=0.862, word_ins_ml=3.936, word_reposition=0.746, ppl=46.64, wps=18583.9, ups=0.91, wpb=20362.4, bsz=253.8, num_updates=30300, lr=0.000203111, gnorm=1.831, clip=0, loss_scale=512, train_wall=88, wall=27798
2022-07-12 01:21:11 | INFO | train_inner | epoch 028:    151 / 1122 loss=5.565, nll_loss=2.32, mask_ins=0.867, word_ins_ml=3.951, word_reposition=0.747, ppl=47.34, wps=22637.4, ups=1.11, wpb=20481.2, bsz=256, num_updates=30400, lr=0.000202777, gnorm=1.799, clip=0, loss_scale=512, train_wall=90, wall=27889
2022-07-12 01:22:42 | INFO | train_inner | epoch 028:    251 / 1122 loss=5.522, nll_loss=2.282, mask_ins=0.863, word_ins_ml=3.916, word_reposition=0.743, ppl=45.96, wps=22557.8, ups=1.1, wpb=20532, bsz=256, num_updates=30500, lr=0.000202444, gnorm=1.789, clip=0, loss_scale=512, train_wall=90, wall=27980
2022-07-12 01:24:12 | INFO | train_inner | epoch 028:    351 / 1122 loss=5.531, nll_loss=2.289, mask_ins=0.86, word_ins_ml=3.922, word_reposition=0.749, ppl=46.24, wps=22948.6, ups=1.12, wpb=20506.3, bsz=256, num_updates=30600, lr=0.000202113, gnorm=1.755, clip=0, loss_scale=512, train_wall=89, wall=28069
2022-07-12 01:25:45 | INFO | train_inner | epoch 028:    451 / 1122 loss=5.524, nll_loss=2.29, mask_ins=0.858, word_ins_ml=3.923, word_reposition=0.743, ppl=46.02, wps=22314.6, ups=1.08, wpb=20697.9, bsz=256, num_updates=30700, lr=0.000201784, gnorm=1.759, clip=0, loss_scale=712, train_wall=92, wall=28162
2022-07-12 01:27:14 | INFO | train_inner | epoch 028:    551 / 1122 loss=5.502, nll_loss=2.27, mask_ins=0.853, word_ins_ml=3.905, word_reposition=0.743, ppl=45.31, wps=22953.4, ups=1.12, wpb=20482.7, bsz=256, num_updates=30800, lr=0.000201456, gnorm=1.758, clip=0, loss_scale=1024, train_wall=89, wall=28251
2022-07-12 01:28:43 | INFO | train_inner | epoch 028:    651 / 1122 loss=5.537, nll_loss=2.292, mask_ins=0.864, word_ins_ml=3.925, word_reposition=0.748, ppl=46.42, wps=22871.3, ups=1.12, wpb=20467.5, bsz=256, num_updates=30900, lr=0.000201129, gnorm=1.765, clip=0, loss_scale=1024, train_wall=89, wall=28340
2022-07-12 01:30:13 | INFO | train_inner | epoch 028:    751 / 1122 loss=5.522, nll_loss=2.285, mask_ins=0.86, word_ins_ml=3.919, word_reposition=0.743, ppl=45.94, wps=22908.2, ups=1.12, wpb=20466.7, bsz=256, num_updates=31000, lr=0.000200805, gnorm=1.801, clip=0, loss_scale=1024, train_wall=89, wall=28430
2022-07-12 01:31:42 | INFO | train_inner | epoch 028:    851 / 1122 loss=5.493, nll_loss=2.266, mask_ins=0.85, word_ins_ml=3.901, word_reposition=0.742, ppl=45.04, wps=22959.3, ups=1.12, wpb=20491.7, bsz=256, num_updates=31100, lr=0.000200482, gnorm=1.746, clip=0, loss_scale=1024, train_wall=89, wall=28519
2022-07-12 01:33:14 | INFO | train_inner | epoch 028:    951 / 1122 loss=5.521, nll_loss=2.299, mask_ins=0.848, word_ins_ml=3.93, word_reposition=0.743, ppl=45.92, wps=22166.4, ups=1.08, wpb=20488.3, bsz=256, num_updates=31200, lr=0.00020016, gnorm=1.754, clip=0, loss_scale=1300, train_wall=92, wall=28611
2022-07-12 01:34:46 | INFO | train_inner | epoch 028:   1051 / 1122 loss=5.516, nll_loss=2.285, mask_ins=0.855, word_ins_ml=3.919, word_reposition=0.742, ppl=45.74, wps=22556.8, ups=1.09, wpb=20604.5, bsz=256, num_updates=31300, lr=0.00019984, gnorm=1.785, clip=0, loss_scale=2048, train_wall=91, wall=28703
2022-07-12 01:35:50 | INFO | train | epoch 028 | loss 5.518 | nll_loss 2.284 | mask_ins 0.856 | word_ins_ml 3.918 | word_reposition 0.744 | ppl 45.81 | wps 22248.1 | ups 1.08 | wpb 20520.7 | bsz 255.8 | num_updates 31371 | lr 0.000199614 | gnorm 1.777 | clip 0 | loss_scale 1017 | train_wall 1008 | wall 28767
2022-07-12 01:36:08 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.159 | nll_loss 5.943 | mask_ins 1.529 | word_ins_ml 7.373 | word_reposition 1.257 | ppl 1143.59 | wps 55060.2 | wpb 2367.6 | bsz 32 | num_updates 31371 | best_loss 10.145
2022-07-12 01:36:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 28 @ 31371 updates, score 10.159) (writing took 2.7231221832334995 seconds)
2022-07-12 01:36:37 | INFO | train_inner | epoch 029:     29 / 1122 loss=5.458, nll_loss=2.245, mask_ins=0.837, word_ins_ml=3.883, word_reposition=0.738, ppl=43.95, wps=18403.2, ups=0.9, wpb=20473.3, bsz=253.8, num_updates=31400, lr=0.000199522, gnorm=1.838, clip=0, loss_scale=2048, train_wall=90, wall=28814
2022-07-12 01:38:08 | INFO | train_inner | epoch 029:    129 / 1122 loss=5.491, nll_loss=2.273, mask_ins=0.849, word_ins_ml=3.908, word_reposition=0.735, ppl=44.99, wps=22616.2, ups=1.1, wpb=20577.9, bsz=256, num_updates=31500, lr=0.000199205, gnorm=1.752, clip=0, loss_scale=2048, train_wall=90, wall=28905
2022-07-12 01:39:39 | INFO | train_inner | epoch 029:    229 / 1122 loss=5.46, nll_loss=2.257, mask_ins=0.84, word_ins_ml=3.893, word_reposition=0.727, ppl=44.03, wps=22615.9, ups=1.1, wpb=20501.8, bsz=256, num_updates=31600, lr=0.000198889, gnorm=1.781, clip=0, loss_scale=2048, train_wall=90, wall=28996
2022-07-12 01:41:09 | INFO | train_inner | epoch 029:    329 / 1122 loss=5.475, nll_loss=2.246, mask_ins=0.85, word_ins_ml=3.883, word_reposition=0.742, ppl=44.49, wps=22771.4, ups=1.11, wpb=20537.6, bsz=256, num_updates=31700, lr=0.000198575, gnorm=1.725, clip=0, loss_scale=2355, train_wall=90, wall=29086
2022-07-12 01:42:40 | INFO | train_inner | epoch 029:    429 / 1122 loss=5.488, nll_loss=2.271, mask_ins=0.85, word_ins_ml=3.906, word_reposition=0.732, ppl=44.88, wps=22545.2, ups=1.1, wpb=20501.5, bsz=256, num_updates=31800, lr=0.000198263, gnorm=1.794, clip=0, loss_scale=4096, train_wall=90, wall=29177
2022-07-12 01:44:11 | INFO | train_inner | epoch 029:    529 / 1122 loss=5.454, nll_loss=2.245, mask_ins=0.841, word_ins_ml=3.882, word_reposition=0.731, ppl=43.83, wps=22530.3, ups=1.1, wpb=20500.6, bsz=256, num_updates=31900, lr=0.000197952, gnorm=1.731, clip=0, loss_scale=4096, train_wall=90, wall=29268
2022-07-12 01:45:41 | INFO | train_inner | epoch 029:    629 / 1122 loss=5.467, nll_loss=2.255, mask_ins=0.843, word_ins_ml=3.891, word_reposition=0.733, ppl=44.22, wps=22662.7, ups=1.1, wpb=20576.6, bsz=256, num_updates=32000, lr=0.000197642, gnorm=1.796, clip=0, loss_scale=4096, train_wall=90, wall=29359
2022-07-12 01:47:12 | INFO | train_inner | epoch 029:    729 / 1122 loss=5.486, nll_loss=2.268, mask_ins=0.843, word_ins_ml=3.902, word_reposition=0.741, ppl=44.81, wps=22711.2, ups=1.11, wpb=20525.3, bsz=256, num_updates=32100, lr=0.000197334, gnorm=1.768, clip=0, loss_scale=4096, train_wall=90, wall=29449
2022-07-12 01:47:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 01:48:44 | INFO | train_inner | epoch 029:    830 / 1122 loss=5.459, nll_loss=2.245, mask_ins=0.845, word_ins_ml=3.882, word_reposition=0.733, ppl=44, wps=22334.8, ups=1.09, wpb=20538, bsz=256, num_updates=32200, lr=0.000197028, gnorm=1.757, clip=0, loss_scale=2372, train_wall=91, wall=29541
2022-07-12 01:50:14 | INFO | train_inner | epoch 029:    930 / 1122 loss=5.468, nll_loss=2.255, mask_ins=0.837, word_ins_ml=3.891, word_reposition=0.74, ppl=44.25, wps=22933.1, ups=1.11, wpb=20613.2, bsz=256, num_updates=32300, lr=0.000196722, gnorm=1.793, clip=0, loss_scale=2048, train_wall=89, wall=29631
2022-07-12 01:51:43 | INFO | train_inner | epoch 029:   1030 / 1122 loss=5.486, nll_loss=2.265, mask_ins=0.849, word_ins_ml=3.9, word_reposition=0.737, ppl=44.81, wps=22834.7, ups=1.12, wpb=20475.5, bsz=256, num_updates=32400, lr=0.000196419, gnorm=1.766, clip=0, loss_scale=2048, train_wall=89, wall=29721
2022-07-12 01:53:06 | INFO | train | epoch 029 | loss 5.471 | nll_loss 2.256 | mask_ins 0.844 | word_ins_ml 3.892 | word_reposition 0.735 | ppl 44.35 | wps 22200.3 | ups 1.08 | wpb 20520.4 | bsz 255.8 | num_updates 32492 | lr 0.00019614 | gnorm 1.773 | clip 0 | loss_scale 2835 | train_wall 1009 | wall 29803
2022-07-12 01:53:24 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.16 | nll_loss 5.822 | mask_ins 1.52 | word_ins_ml 7.264 | word_reposition 1.377 | ppl 1144.4 | wps 55293.3 | wpb 2367.6 | bsz 32 | num_updates 32492 | best_loss 10.145
2022-07-12 01:53:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 29 @ 32492 updates, score 10.16) (writing took 2.7376070404425263 seconds)
2022-07-12 01:53:34 | INFO | train_inner | epoch 030:      8 / 1122 loss=5.439, nll_loss=2.229, mask_ins=0.841, word_ins_ml=3.867, word_reposition=0.732, ppl=43.39, wps=18427.1, ups=0.91, wpb=20343.4, bsz=253.8, num_updates=32500, lr=0.000196116, gnorm=1.836, clip=0, loss_scale=2048, train_wall=89, wall=29831
2022-07-12 01:55:05 | INFO | train_inner | epoch 030:    108 / 1122 loss=5.406, nll_loss=2.203, mask_ins=0.832, word_ins_ml=3.844, word_reposition=0.73, ppl=42.4, wps=22573.4, ups=1.1, wpb=20509.6, bsz=256, num_updates=32600, lr=0.000195815, gnorm=1.755, clip=0, loss_scale=2048, train_wall=90, wall=29922
2022-07-12 01:56:35 | INFO | train_inner | epoch 030:    208 / 1122 loss=5.455, nll_loss=2.243, mask_ins=0.837, word_ins_ml=3.88, word_reposition=0.738, ppl=43.87, wps=22700.8, ups=1.11, wpb=20515.8, bsz=256, num_updates=32700, lr=0.000195515, gnorm=1.787, clip=0, loss_scale=3543, train_wall=90, wall=30012
2022-07-12 01:58:06 | INFO | train_inner | epoch 030:    308 / 1122 loss=5.42, nll_loss=2.219, mask_ins=0.833, word_ins_ml=3.859, word_reposition=0.728, ppl=42.81, wps=22614.2, ups=1.1, wpb=20513.5, bsz=256, num_updates=32800, lr=0.000195217, gnorm=1.766, clip=0, loss_scale=4096, train_wall=90, wall=30103
2022-07-12 01:59:36 | INFO | train_inner | epoch 030:    408 / 1122 loss=5.39, nll_loss=2.201, mask_ins=0.824, word_ins_ml=3.842, word_reposition=0.723, ppl=41.92, wps=22585.9, ups=1.11, wpb=20412.4, bsz=256, num_updates=32900, lr=0.00019492, gnorm=1.752, clip=0, loss_scale=4096, train_wall=90, wall=30193
2022-07-12 02:01:08 | INFO | train_inner | epoch 030:    508 / 1122 loss=5.39, nll_loss=2.182, mask_ins=0.827, word_ins_ml=3.826, word_reposition=0.738, ppl=41.94, wps=22633.1, ups=1.09, wpb=20695.8, bsz=256, num_updates=33000, lr=0.000194625, gnorm=1.754, clip=0, loss_scale=4096, train_wall=91, wall=30285
2022-07-12 02:02:39 | INFO | train_inner | epoch 030:    608 / 1122 loss=5.466, nll_loss=2.264, mask_ins=0.835, word_ins_ml=3.898, word_reposition=0.734, ppl=44.2, wps=22460.6, ups=1.1, wpb=20481.9, bsz=256, num_updates=33100, lr=0.000194331, gnorm=1.799, clip=0, loss_scale=4096, train_wall=91, wall=30376
2022-07-12 02:04:10 | INFO | train_inner | epoch 030:    708 / 1122 loss=5.409, nll_loss=2.215, mask_ins=0.83, word_ins_ml=3.855, word_reposition=0.724, ppl=42.49, wps=22399.5, ups=1.1, wpb=20393.5, bsz=256, num_updates=33200, lr=0.000194038, gnorm=1.737, clip=0, loss_scale=6595, train_wall=90, wall=30467
2022-07-12 02:04:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 02:05:43 | INFO | train_inner | epoch 030:    809 / 1122 loss=5.384, nll_loss=2.194, mask_ins=0.822, word_ins_ml=3.835, word_reposition=0.727, ppl=41.77, wps=22177, ups=1.08, wpb=20606.5, bsz=256, num_updates=33300, lr=0.000193746, gnorm=1.708, clip=0, loss_scale=4623, train_wall=92, wall=30560
2022-07-12 02:07:12 | INFO | train_inner | epoch 030:    909 / 1122 loss=5.397, nll_loss=2.201, mask_ins=0.828, word_ins_ml=3.841, word_reposition=0.727, ppl=42.12, wps=22979.7, ups=1.12, wpb=20584.7, bsz=256, num_updates=33400, lr=0.000193456, gnorm=1.751, clip=0, loss_scale=4096, train_wall=89, wall=30649
2022-07-12 02:08:41 | INFO | train_inner | epoch 030:   1009 / 1122 loss=5.399, nll_loss=2.209, mask_ins=0.824, word_ins_ml=3.849, word_reposition=0.726, ppl=42.19, wps=23162.7, ups=1.12, wpb=20636.1, bsz=256, num_updates=33500, lr=0.000193167, gnorm=1.747, clip=0, loss_scale=4096, train_wall=88, wall=30739
2022-07-12 02:10:11 | INFO | train_inner | epoch 030:   1109 / 1122 loss=5.397, nll_loss=2.206, mask_ins=0.825, word_ins_ml=3.846, word_reposition=0.726, ppl=42.15, wps=22969.5, ups=1.12, wpb=20547.9, bsz=256, num_updates=33600, lr=0.000192879, gnorm=1.752, clip=0, loss_scale=4096, train_wall=89, wall=30828
2022-07-12 02:10:22 | INFO | train | epoch 030 | loss 5.41 | nll_loss 2.212 | mask_ins 0.829 | word_ins_ml 3.852 | word_reposition 0.729 | ppl 42.52 | wps 22200.9 | ups 1.08 | wpb 20521 | bsz 255.8 | num_updates 33613 | lr 0.000192842 | gnorm 1.762 | clip 0 | loss_scale 4120 | train_wall 1008 | wall 30839
2022-07-12 02:10:40 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.03 | nll_loss 5.797 | mask_ins 1.512 | word_ins_ml 7.236 | word_reposition 1.281 | ppl 1045.28 | wps 55695.9 | wpb 2367.6 | bsz 32 | num_updates 33613 | best_loss 10.03
2022-07-12 02:10:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 30 @ 33613 updates, score 10.03) (writing took 4.94768139347434 seconds)
2022-07-12 02:12:03 | INFO | train_inner | epoch 031:     87 / 1122 loss=5.375, nll_loss=2.184, mask_ins=0.817, word_ins_ml=3.827, word_reposition=0.732, ppl=41.5, wps=18278.7, ups=0.89, wpb=20505, bsz=253.8, num_updates=33700, lr=0.000192593, gnorm=1.803, clip=0, loss_scale=4096, train_wall=89, wall=30940
2022-07-12 02:13:33 | INFO | train_inner | epoch 031:    187 / 1122 loss=5.378, nll_loss=2.186, mask_ins=0.825, word_ins_ml=3.828, word_reposition=0.725, ppl=41.59, wps=22859.9, ups=1.11, wpb=20621.5, bsz=256, num_updates=33800, lr=0.000192308, gnorm=1.753, clip=0, loss_scale=7209, train_wall=90, wall=31030
2022-07-12 02:15:03 | INFO | train_inner | epoch 031:    287 / 1122 loss=5.389, nll_loss=2.19, mask_ins=0.831, word_ins_ml=3.832, word_reposition=0.726, ppl=41.89, wps=22825.9, ups=1.12, wpb=20406.4, bsz=256, num_updates=33900, lr=0.000192024, gnorm=1.754, clip=0, loss_scale=8192, train_wall=89, wall=31120
2022-07-12 02:16:32 | INFO | train_inner | epoch 031:    387 / 1122 loss=5.413, nll_loss=2.214, mask_ins=0.83, word_ins_ml=3.853, word_reposition=0.729, ppl=42.6, wps=23076.5, ups=1.12, wpb=20597.6, bsz=256, num_updates=34000, lr=0.000191741, gnorm=1.789, clip=0, loss_scale=8192, train_wall=89, wall=31209
2022-07-12 02:18:02 | INFO | train_inner | epoch 031:    487 / 1122 loss=5.397, nll_loss=2.202, mask_ins=0.83, word_ins_ml=3.842, word_reposition=0.725, ppl=42.14, wps=22772, ups=1.11, wpb=20434.2, bsz=256, num_updates=34100, lr=0.00019146, gnorm=1.77, clip=0, loss_scale=8192, train_wall=89, wall=31299
2022-07-12 02:19:31 | INFO | train_inner | epoch 031:    587 / 1122 loss=5.386, nll_loss=2.199, mask_ins=0.82, word_ins_ml=3.84, word_reposition=0.727, ppl=41.83, wps=23160.6, ups=1.12, wpb=20634.6, bsz=256, num_updates=34200, lr=0.00019118, gnorm=1.756, clip=0, loss_scale=8192, train_wall=88, wall=31388
2022-07-12 02:21:00 | INFO | train_inner | epoch 031:    687 / 1122 loss=5.389, nll_loss=2.199, mask_ins=0.819, word_ins_ml=3.84, word_reposition=0.73, ppl=41.91, wps=23048.4, ups=1.12, wpb=20553.4, bsz=256, num_updates=34300, lr=0.000190901, gnorm=1.743, clip=0, loss_scale=13435, train_wall=89, wall=31477
2022-07-12 02:22:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 02:22:30 | INFO | train_inner | epoch 031:    788 / 1122 loss=5.367, nll_loss=2.181, mask_ins=0.817, word_ins_ml=3.823, word_reposition=0.726, ppl=41.26, wps=22781.6, ups=1.11, wpb=20559.8, bsz=256, num_updates=34400, lr=0.000190623, gnorm=1.757, clip=0, loss_scale=13789, train_wall=90, wall=31567
2022-07-12 02:24:00 | INFO | train_inner | epoch 031:    888 / 1122 loss=5.321, nll_loss=2.145, mask_ins=0.809, word_ins_ml=3.791, word_reposition=0.72, ppl=39.97, wps=22687.4, ups=1.11, wpb=20486.5, bsz=256, num_updates=34500, lr=0.000190347, gnorm=1.755, clip=0, loss_scale=8192, train_wall=90, wall=31658
2022-07-12 02:25:31 | INFO | train_inner | epoch 031:    988 / 1122 loss=5.313, nll_loss=2.127, mask_ins=0.819, word_ins_ml=3.775, word_reposition=0.719, ppl=39.75, wps=22421.1, ups=1.1, wpb=20413.2, bsz=256, num_updates=34600, lr=0.000190071, gnorm=1.716, clip=0, loss_scale=8192, train_wall=90, wall=31749
2022-07-12 02:27:02 | INFO | train_inner | epoch 031:   1088 / 1122 loss=5.342, nll_loss=2.156, mask_ins=0.822, word_ins_ml=3.801, word_reposition=0.719, ppl=40.55, wps=22685.1, ups=1.1, wpb=20541.3, bsz=256, num_updates=34700, lr=0.000189797, gnorm=1.714, clip=0, loss_scale=8192, train_wall=90, wall=31839
2022-07-12 02:27:32 | INFO | train | epoch 031 | loss 5.369 | nll_loss 2.179 | mask_ins 0.822 | word_ins_ml 3.822 | word_reposition 0.725 | ppl 41.32 | wps 22330.3 | ups 1.09 | wpb 20522.1 | bsz 255.8 | num_updates 34734 | lr 0.000189704 | gnorm 1.755 | clip 0 | loss_scale 8758 | train_wall 1000 | wall 31869
2022-07-12 02:27:50 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.052 | nll_loss 5.802 | mask_ins 1.476 | word_ins_ml 7.241 | word_reposition 1.334 | ppl 1061.23 | wps 55572.8 | wpb 2367.6 | bsz 32 | num_updates 34734 | best_loss 10.03
2022-07-12 02:27:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 31 @ 34734 updates, score 10.052) (writing took 2.7983591947704554 seconds)
2022-07-12 02:28:54 | INFO | train_inner | epoch 032:     66 / 1122 loss=5.381, nll_loss=2.195, mask_ins=0.823, word_ins_ml=3.835, word_reposition=0.722, ppl=41.66, wps=18096.5, ups=0.89, wpb=20331.6, bsz=253.8, num_updates=34800, lr=0.000189525, gnorm=1.864, clip=0, loss_scale=8192, train_wall=91, wall=31952
2022-07-12 02:30:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 02:30:26 | INFO | train_inner | epoch 032:    167 / 1122 loss=5.345, nll_loss=2.164, mask_ins=0.818, word_ins_ml=3.809, word_reposition=0.718, ppl=40.65, wps=22388.7, ups=1.09, wpb=20449.4, bsz=256, num_updates=34900, lr=0.000189253, gnorm=1.769, clip=0, loss_scale=8841, train_wall=91, wall=32043
2022-07-12 02:31:57 | INFO | train_inner | epoch 032:    267 / 1122 loss=5.34, nll_loss=2.157, mask_ins=0.813, word_ins_ml=3.802, word_reposition=0.725, ppl=40.49, wps=22504.4, ups=1.1, wpb=20518.8, bsz=256, num_updates=35000, lr=0.000188982, gnorm=1.772, clip=0, loss_scale=8192, train_wall=91, wall=32134
2022-07-12 02:32:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 02:33:28 | INFO | train_inner | epoch 032:    368 / 1122 loss=5.345, nll_loss=2.156, mask_ins=0.82, word_ins_ml=3.801, word_reposition=0.724, ppl=40.64, wps=22587.9, ups=1.09, wpb=20692.7, bsz=256, num_updates=35100, lr=0.000188713, gnorm=1.747, clip=0, loss_scale=4948, train_wall=91, wall=32226
2022-07-12 02:34:58 | INFO | train_inner | epoch 032:    468 / 1122 loss=5.334, nll_loss=2.149, mask_ins=0.816, word_ins_ml=3.794, word_reposition=0.725, ppl=40.35, wps=23137.6, ups=1.12, wpb=20742.9, bsz=256, num_updates=35200, lr=0.000188445, gnorm=1.735, clip=0, loss_scale=4096, train_wall=89, wall=32315
2022-07-12 02:36:28 | INFO | train_inner | epoch 032:    568 / 1122 loss=5.326, nll_loss=2.141, mask_ins=0.819, word_ins_ml=3.787, word_reposition=0.719, ppl=40.1, wps=22891.2, ups=1.11, wpb=20548.1, bsz=256, num_updates=35300, lr=0.000188177, gnorm=1.723, clip=0, loss_scale=4096, train_wall=89, wall=32405
2022-07-12 02:37:57 | INFO | train_inner | epoch 032:    668 / 1122 loss=5.37, nll_loss=2.192, mask_ins=0.818, word_ins_ml=3.832, word_reposition=0.72, ppl=41.36, wps=22858.6, ups=1.12, wpb=20442, bsz=256, num_updates=35400, lr=0.000187912, gnorm=1.763, clip=0, loss_scale=4096, train_wall=89, wall=32494
2022-07-12 02:39:27 | INFO | train_inner | epoch 032:    768 / 1122 loss=5.347, nll_loss=2.153, mask_ins=0.825, word_ins_ml=3.798, word_reposition=0.724, ppl=40.7, wps=22784.9, ups=1.12, wpb=20350.5, bsz=256, num_updates=35500, lr=0.000187647, gnorm=1.732, clip=0, loss_scale=4096, train_wall=89, wall=32584
2022-07-12 02:40:57 | INFO | train_inner | epoch 032:    868 / 1122 loss=5.329, nll_loss=2.152, mask_ins=0.813, word_ins_ml=3.797, word_reposition=0.719, ppl=40.2, wps=22525.6, ups=1.1, wpb=20411.2, bsz=256, num_updates=35600, lr=0.000187383, gnorm=1.743, clip=0, loss_scale=6881, train_wall=90, wall=32674
2022-07-12 02:42:27 | INFO | train_inner | epoch 032:    968 / 1122 loss=5.365, nll_loss=2.187, mask_ins=0.81, word_ins_ml=3.828, word_reposition=0.727, ppl=41.21, wps=23051.8, ups=1.12, wpb=20638.5, bsz=256, num_updates=35700, lr=0.00018712, gnorm=1.71, clip=0, loss_scale=8192, train_wall=89, wall=32764
2022-07-12 02:43:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 02:43:57 | INFO | train_inner | epoch 032:   1069 / 1122 loss=5.284, nll_loss=2.106, mask_ins=0.811, word_ins_ml=3.756, word_reposition=0.717, ppl=38.95, wps=22845.4, ups=1.11, wpb=20578.8, bsz=256, num_updates=35800, lr=0.000186859, gnorm=1.724, clip=0, loss_scale=7300, train_wall=89, wall=32854
2022-07-12 02:44:44 | INFO | train | epoch 032 | loss 5.342 | nll_loss 2.16 | mask_ins 0.816 | word_ins_ml 3.804 | word_reposition 0.721 | ppl 40.56 | wps 22255.7 | ups 1.08 | wpb 20521.2 | bsz 255.8 | num_updates 35853 | lr 0.000186721 | gnorm 1.75 | clip 0 | loss_scale 6107 | train_wall 1004 | wall 32901
2022-07-12 02:45:02 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.176 | nll_loss 5.829 | mask_ins 1.517 | word_ins_ml 7.269 | word_reposition 1.389 | ppl 1156.48 | wps 55432.8 | wpb 2367.6 | bsz 32 | num_updates 35853 | best_loss 10.03
2022-07-12 02:45:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 32 @ 35853 updates, score 10.176) (writing took 2.901007995940745 seconds)
2022-07-12 02:45:47 | INFO | train_inner | epoch 033:     47 / 1122 loss=5.364, nll_loss=2.187, mask_ins=0.81, word_ins_ml=3.828, word_reposition=0.726, ppl=41.19, wps=18529.1, ups=0.91, wpb=20408.3, bsz=253.8, num_updates=35900, lr=0.000186598, gnorm=1.76, clip=0, loss_scale=4096, train_wall=89, wall=32964
2022-07-12 02:47:17 | INFO | train_inner | epoch 033:    147 / 1122 loss=5.339, nll_loss=2.156, mask_ins=0.817, word_ins_ml=3.8, word_reposition=0.722, ppl=40.47, wps=23066.9, ups=1.12, wpb=20682.7, bsz=256, num_updates=36000, lr=0.000186339, gnorm=1.732, clip=0, loss_scale=4096, train_wall=89, wall=33054
2022-07-12 02:48:46 | INFO | train_inner | epoch 033:    247 / 1122 loss=5.27, nll_loss=2.096, mask_ins=0.809, word_ins_ml=3.747, word_reposition=0.715, ppl=38.59, wps=22955.1, ups=1.12, wpb=20534.4, bsz=256, num_updates=36100, lr=0.000186081, gnorm=1.691, clip=0, loss_scale=4096, train_wall=89, wall=33143
2022-07-12 02:50:17 | INFO | train_inner | epoch 033:    347 / 1122 loss=5.29, nll_loss=2.113, mask_ins=0.809, word_ins_ml=3.762, word_reposition=0.719, ppl=39.12, wps=22753.9, ups=1.11, wpb=20586.8, bsz=256, num_updates=36200, lr=0.000185824, gnorm=1.75, clip=0, loss_scale=4096, train_wall=90, wall=33234
2022-07-12 02:51:46 | INFO | train_inner | epoch 033:    447 / 1122 loss=5.328, nll_loss=2.154, mask_ins=0.809, word_ins_ml=3.798, word_reposition=0.72, ppl=40.17, wps=22883.9, ups=1.12, wpb=20441.4, bsz=256, num_updates=36300, lr=0.000185567, gnorm=1.739, clip=0, loss_scale=4506, train_wall=89, wall=33323
2022-07-12 02:53:15 | INFO | train_inner | epoch 033:    547 / 1122 loss=5.332, nll_loss=2.164, mask_ins=0.803, word_ins_ml=3.807, word_reposition=0.722, ppl=40.29, wps=22973, ups=1.12, wpb=20521.4, bsz=256, num_updates=36400, lr=0.000185312, gnorm=1.704, clip=0, loss_scale=8192, train_wall=89, wall=33412
2022-07-12 02:54:45 | INFO | train_inner | epoch 033:    647 / 1122 loss=5.317, nll_loss=2.144, mask_ins=0.815, word_ins_ml=3.789, word_reposition=0.713, ppl=39.86, wps=22879.6, ups=1.12, wpb=20485.9, bsz=256, num_updates=36500, lr=0.000185058, gnorm=1.754, clip=0, loss_scale=8192, train_wall=89, wall=33502
2022-07-12 02:56:14 | INFO | train_inner | epoch 033:    747 / 1122 loss=5.33, nll_loss=2.162, mask_ins=0.805, word_ins_ml=3.805, word_reposition=0.721, ppl=40.24, wps=22940.1, ups=1.12, wpb=20494.4, bsz=256, num_updates=36600, lr=0.000184805, gnorm=1.729, clip=0, loss_scale=8192, train_wall=89, wall=33591
2022-07-12 02:57:44 | INFO | train_inner | epoch 033:    847 / 1122 loss=5.296, nll_loss=2.136, mask_ins=0.801, word_ins_ml=3.782, word_reposition=0.713, ppl=39.28, wps=22859.7, ups=1.12, wpb=20487.5, bsz=256, num_updates=36700, lr=0.000184553, gnorm=1.736, clip=0, loss_scale=8192, train_wall=89, wall=33681
2022-07-12 02:59:13 | INFO | train_inner | epoch 033:    947 / 1122 loss=5.307, nll_loss=2.126, mask_ins=0.81, word_ins_ml=3.773, word_reposition=0.725, ppl=39.6, wps=22994, ups=1.12, wpb=20521.2, bsz=256, num_updates=36800, lr=0.000184302, gnorm=1.71, clip=0, loss_scale=8192, train_wall=89, wall=33770
2022-07-12 02:59:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 03:00:43 | INFO | train_inner | epoch 033:   1048 / 1122 loss=5.286, nll_loss=2.115, mask_ins=0.804, word_ins_ml=3.763, word_reposition=0.718, ppl=39.01, wps=22905.6, ups=1.11, wpb=20617, bsz=256, num_updates=36900, lr=0.000184053, gnorm=1.676, clip=0, loss_scale=9895, train_wall=89, wall=33860
2022-07-12 03:01:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 03:01:49 | INFO | train | epoch 033 | loss 5.313 | nll_loss 2.14 | mask_ins 0.809 | word_ins_ml 3.786 | word_reposition 0.719 | ppl 39.77 | wps 22423.1 | ups 1.09 | wpb 20520.8 | bsz 255.8 | num_updates 36973 | lr 0.000183871 | gnorm 1.726 | clip 0 | loss_scale 6699 | train_wall 997 | wall 33926
2022-07-12 03:02:07 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.111 | nll_loss 5.817 | mask_ins 1.51 | word_ins_ml 7.257 | word_reposition 1.344 | ppl 1105.98 | wps 55279.2 | wpb 2367.6 | bsz 32 | num_updates 36973 | best_loss 10.03
2022-07-12 03:02:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 33 @ 36973 updates, score 10.111) (writing took 3.1357700768858194 seconds)
2022-07-12 03:02:35 | INFO | train_inner | epoch 034:     27 / 1122 loss=5.315, nll_loss=2.147, mask_ins=0.81, word_ins_ml=3.791, word_reposition=0.714, ppl=39.82, wps=18156.6, ups=0.89, wpb=20327, bsz=253.8, num_updates=37000, lr=0.000183804, gnorm=1.778, clip=0, loss_scale=6529, train_wall=90, wall=33972
2022-07-12 03:04:05 | INFO | train_inner | epoch 034:    127 / 1122 loss=5.31, nll_loss=2.13, mask_ins=0.807, word_ins_ml=3.776, word_reposition=0.727, ppl=39.67, wps=22628.5, ups=1.11, wpb=20470.5, bsz=256, num_updates=37100, lr=0.000183556, gnorm=1.733, clip=0, loss_scale=4096, train_wall=90, wall=34063
2022-07-12 03:05:35 | INFO | train_inner | epoch 034:    227 / 1122 loss=5.312, nll_loss=2.158, mask_ins=0.796, word_ins_ml=3.802, word_reposition=0.715, ppl=39.73, wps=22737.4, ups=1.11, wpb=20452.5, bsz=256, num_updates=37200, lr=0.000183309, gnorm=1.703, clip=0, loss_scale=4096, train_wall=89, wall=34153
2022-07-12 03:07:05 | INFO | train_inner | epoch 034:    327 / 1122 loss=5.255, nll_loss=2.105, mask_ins=0.79, word_ins_ml=3.754, word_reposition=0.711, ppl=38.18, wps=22966.1, ups=1.12, wpb=20514.9, bsz=256, num_updates=37300, lr=0.000183063, gnorm=1.724, clip=0, loss_scale=4096, train_wall=89, wall=34242
2022-07-12 03:08:34 | INFO | train_inner | epoch 034:    427 / 1122 loss=5.286, nll_loss=2.121, mask_ins=0.798, word_ins_ml=3.768, word_reposition=0.72, ppl=39.03, wps=22990.8, ups=1.12, wpb=20518.7, bsz=256, num_updates=37400, lr=0.000182818, gnorm=1.74, clip=0, loss_scale=4096, train_wall=89, wall=34331
2022-07-12 03:10:04 | INFO | train_inner | epoch 034:    527 / 1122 loss=5.328, nll_loss=2.166, mask_ins=0.804, word_ins_ml=3.808, word_reposition=0.716, ppl=40.17, wps=22688.7, ups=1.11, wpb=20510.7, bsz=256, num_updates=37500, lr=0.000182574, gnorm=1.698, clip=0, loss_scale=5284, train_wall=90, wall=34422
2022-07-12 03:11:34 | INFO | train_inner | epoch 034:    627 / 1122 loss=5.266, nll_loss=2.113, mask_ins=0.79, word_ins_ml=3.761, word_reposition=0.715, ppl=38.49, wps=22886.2, ups=1.11, wpb=20603.5, bsz=256, num_updates=37600, lr=0.000182331, gnorm=1.717, clip=0, loss_scale=8192, train_wall=89, wall=34512
2022-07-12 03:13:04 | INFO | train_inner | epoch 034:    727 / 1122 loss=5.302, nll_loss=2.128, mask_ins=0.806, word_ins_ml=3.774, word_reposition=0.722, ppl=39.46, wps=23131.2, ups=1.12, wpb=20668.8, bsz=256, num_updates=37700, lr=0.000182089, gnorm=1.72, clip=0, loss_scale=8192, train_wall=89, wall=34601
2022-07-12 03:14:33 | INFO | train_inner | epoch 034:    827 / 1122 loss=5.248, nll_loss=2.094, mask_ins=0.79, word_ins_ml=3.744, word_reposition=0.714, ppl=38, wps=23092.9, ups=1.12, wpb=20579.5, bsz=256, num_updates=37800, lr=0.000181848, gnorm=1.675, clip=0, loss_scale=8192, train_wall=89, wall=34690
2022-07-12 03:16:03 | INFO | train_inner | epoch 034:    927 / 1122 loss=5.248, nll_loss=2.094, mask_ins=0.792, word_ins_ml=3.744, word_reposition=0.713, ppl=38.01, wps=22771.7, ups=1.11, wpb=20569.1, bsz=256, num_updates=37900, lr=0.000181608, gnorm=1.678, clip=0, loss_scale=8192, train_wall=90, wall=34780
2022-07-12 03:17:34 | INFO | train_inner | epoch 034:   1027 / 1122 loss=5.312, nll_loss=2.148, mask_ins=0.803, word_ins_ml=3.792, word_reposition=0.717, ppl=39.73, wps=22661.5, ups=1.11, wpb=20483.8, bsz=256, num_updates=38000, lr=0.000181369, gnorm=1.68, clip=0, loss_scale=9585, train_wall=90, wall=34871
2022-07-12 03:18:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 03:18:59 | INFO | train | epoch 034 | loss 5.284 | nll_loss 2.124 | mask_ins 0.797 | word_ins_ml 3.771 | word_reposition 0.717 | ppl 38.97 | wps 22337.1 | ups 1.09 | wpb 20522.8 | bsz 255.8 | num_updates 38094 | lr 0.000181145 | gnorm 1.714 | clip 0 | loss_scale 7090 | train_wall 1002 | wall 34956
2022-07-12 03:19:17 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 9.911 | nll_loss 5.71 | mask_ins 1.497 | word_ins_ml 7.149 | word_reposition 1.265 | ppl 962.48 | wps 55600.9 | wpb 2367.6 | bsz 32 | num_updates 38094 | best_loss 9.911
2022-07-12 03:19:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 34 @ 38094 updates, score 9.911) (writing took 5.069271688349545 seconds)
2022-07-12 03:19:28 | INFO | train_inner | epoch 035:      6 / 1122 loss=5.25, nll_loss=2.097, mask_ins=0.79, word_ins_ml=3.746, word_reposition=0.714, ppl=38.06, wps=17891.7, ups=0.88, wpb=20381.7, bsz=253.8, num_updates=38100, lr=0.000181131, gnorm=1.779, clip=0, loss_scale=14762, train_wall=90, wall=34985
2022-07-12 03:20:58 | INFO | train_inner | epoch 035:    106 / 1122 loss=5.281, nll_loss=2.122, mask_ins=0.795, word_ins_ml=3.769, word_reposition=0.717, ppl=38.89, wps=22628.4, ups=1.1, wpb=20495.3, bsz=256, num_updates=38200, lr=0.000180894, gnorm=1.72, clip=0, loss_scale=8192, train_wall=90, wall=35075
2022-07-12 03:22:28 | INFO | train_inner | epoch 035:    206 / 1122 loss=5.235, nll_loss=2.09, mask_ins=0.786, word_ins_ml=3.74, word_reposition=0.709, ppl=37.66, wps=22732.5, ups=1.11, wpb=20525.2, bsz=256, num_updates=38300, lr=0.000180657, gnorm=1.652, clip=0, loss_scale=8192, train_wall=90, wall=35166
2022-07-12 03:23:59 | INFO | train_inner | epoch 035:    306 / 1122 loss=5.31, nll_loss=2.137, mask_ins=0.808, word_ins_ml=3.781, word_reposition=0.72, ppl=39.66, wps=22724, ups=1.1, wpb=20582.3, bsz=256, num_updates=38400, lr=0.000180422, gnorm=1.695, clip=0, loss_scale=8192, train_wall=90, wall=35256
2022-07-12 03:25:30 | INFO | train_inner | epoch 035:    406 / 1122 loss=5.272, nll_loss=2.119, mask_ins=0.797, word_ins_ml=3.766, word_reposition=0.709, ppl=38.64, wps=22628.2, ups=1.1, wpb=20554.6, bsz=256, num_updates=38500, lr=0.000180187, gnorm=1.704, clip=0, loss_scale=8192, train_wall=90, wall=35347
2022-07-12 03:27:01 | INFO | train_inner | epoch 035:    506 / 1122 loss=5.236, nll_loss=2.069, mask_ins=0.795, word_ins_ml=3.722, word_reposition=0.719, ppl=37.68, wps=22633.1, ups=1.1, wpb=20610.7, bsz=256, num_updates=38600, lr=0.000179954, gnorm=1.703, clip=0, loss_scale=8847, train_wall=90, wall=35438
2022-07-12 03:27:14 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 03:28:32 | INFO | train_inner | epoch 035:    607 / 1122 loss=5.22, nll_loss=2.078, mask_ins=0.786, word_ins_ml=3.729, word_reposition=0.706, ppl=37.28, wps=22343.9, ups=1.09, wpb=20432.1, bsz=256, num_updates=38700, lr=0.000179721, gnorm=1.687, clip=0, loss_scale=9328, train_wall=91, wall=35529
2022-07-12 03:30:03 | INFO | train_inner | epoch 035:    707 / 1122 loss=5.273, nll_loss=2.122, mask_ins=0.794, word_ins_ml=3.768, word_reposition=0.712, ppl=38.68, wps=22751, ups=1.11, wpb=20587.5, bsz=256, num_updates=38800, lr=0.00017949, gnorm=1.697, clip=0, loss_scale=8192, train_wall=90, wall=35620
2022-07-12 03:31:33 | INFO | train_inner | epoch 035:    807 / 1122 loss=5.215, nll_loss=2.056, mask_ins=0.794, word_ins_ml=3.709, word_reposition=0.712, ppl=37.14, wps=22630.7, ups=1.11, wpb=20468.3, bsz=256, num_updates=38900, lr=0.000179259, gnorm=1.677, clip=0, loss_scale=8192, train_wall=90, wall=35710
2022-07-12 03:33:04 | INFO | train_inner | epoch 035:    907 / 1122 loss=5.272, nll_loss=2.125, mask_ins=0.786, word_ins_ml=3.77, word_reposition=0.717, ppl=38.65, wps=22600.7, ups=1.1, wpb=20495.2, bsz=256, num_updates=39000, lr=0.000179029, gnorm=1.699, clip=0, loss_scale=8192, train_wall=90, wall=35801
2022-07-12 03:34:34 | INFO | train_inner | epoch 035:   1007 / 1122 loss=5.215, nll_loss=2.067, mask_ins=0.789, word_ins_ml=3.719, word_reposition=0.708, ppl=37.15, wps=22691.9, ups=1.1, wpb=20541.9, bsz=256, num_updates=39100, lr=0.0001788, gnorm=1.655, clip=0, loss_scale=8192, train_wall=90, wall=35892
2022-07-12 03:34:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 03:36:06 | INFO | train_inner | epoch 035:   1108 / 1122 loss=5.224, nll_loss=2.085, mask_ins=0.781, word_ins_ml=3.735, word_reposition=0.708, ppl=37.38, wps=22453.9, ups=1.09, wpb=20592.4, bsz=256, num_updates=39200, lr=0.000178571, gnorm=1.696, clip=0, loss_scale=8273, train_wall=91, wall=35983
2022-07-12 03:36:19 | INFO | train | epoch 035 | loss 5.251 | nll_loss 2.098 | mask_ins 0.792 | word_ins_ml 3.747 | word_reposition 0.713 | ppl 38.09 | wps 22111.1 | ups 1.08 | wpb 20521.8 | bsz 255.8 | num_updates 39214 | lr 0.00017854 | gnorm 1.695 | clip 0 | loss_scale 8360 | train_wall 1009 | wall 35996
2022-07-12 03:36:36 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 10.139 | nll_loss 5.849 | mask_ins 1.507 | word_ins_ml 7.289 | word_reposition 1.343 | ppl 1127.64 | wps 55636.8 | wpb 2367.6 | bsz 32 | num_updates 39214 | best_loss 9.911
2022-07-12 03:36:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 35 @ 39214 updates, score 10.139) (writing took 2.911442852579057 seconds)
2022-07-12 03:37:56 | INFO | train_inner | epoch 036:     86 / 1122 loss=5.246, nll_loss=2.087, mask_ins=0.797, word_ins_ml=3.737, word_reposition=0.712, ppl=37.94, wps=18494.2, ups=0.91, wpb=20388.2, bsz=253.8, num_updates=39300, lr=0.000178344, gnorm=1.742, clip=0, loss_scale=8192, train_wall=89, wall=36094
2022-07-12 03:39:25 | INFO | train_inner | epoch 036:    186 / 1122 loss=5.241, nll_loss=2.095, mask_ins=0.784, word_ins_ml=3.744, word_reposition=0.713, ppl=37.83, wps=23119.6, ups=1.12, wpb=20587.2, bsz=256, num_updates=39400, lr=0.000178118, gnorm=1.691, clip=0, loss_scale=8192, train_wall=88, wall=36183
2022-07-12 03:40:55 | INFO | train_inner | epoch 036:    286 / 1122 loss=5.249, nll_loss=2.102, mask_ins=0.786, word_ins_ml=3.75, word_reposition=0.712, ppl=38.02, wps=22918.9, ups=1.11, wpb=20559.1, bsz=256, num_updates=39500, lr=0.000177892, gnorm=1.716, clip=0, loss_scale=8192, train_wall=89, wall=36272
2022-07-12 03:42:25 | INFO | train_inner | epoch 036:    386 / 1122 loss=5.22, nll_loss=2.067, mask_ins=0.796, word_ins_ml=3.718, word_reposition=0.705, ppl=37.26, wps=22882.8, ups=1.11, wpb=20576, bsz=256, num_updates=39600, lr=0.000177667, gnorm=1.773, clip=0, loss_scale=8192, train_wall=89, wall=36362
2022-07-12 03:42:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 03:43:56 | INFO | train_inner | epoch 036:    487 / 1122 loss=5.267, nll_loss=2.125, mask_ins=0.784, word_ins_ml=3.77, word_reposition=0.713, ppl=38.51, wps=22474.2, ups=1.1, wpb=20444.3, bsz=256, num_updates=39700, lr=0.000177443, gnorm=1.663, clip=0, loss_scale=5353, train_wall=90, wall=36453
2022-07-12 03:45:26 | INFO | train_inner | epoch 036:    587 / 1122 loss=5.216, nll_loss=2.081, mask_ins=0.783, word_ins_ml=3.732, word_reposition=0.701, ppl=37.18, wps=22673.3, ups=1.11, wpb=20484.8, bsz=256, num_updates=39800, lr=0.00017722, gnorm=1.694, clip=0, loss_scale=4096, train_wall=90, wall=36544
2022-07-12 03:46:56 | INFO | train_inner | epoch 036:    687 / 1122 loss=5.217, nll_loss=2.062, mask_ins=0.787, word_ins_ml=3.714, word_reposition=0.715, ppl=37.19, wps=22981.2, ups=1.12, wpb=20576.3, bsz=256, num_updates=39900, lr=0.000176998, gnorm=1.666, clip=0, loss_scale=4096, train_wall=89, wall=36633
2022-07-12 03:48:26 | INFO | train_inner | epoch 036:    787 / 1122 loss=5.249, nll_loss=2.101, mask_ins=0.787, word_ins_ml=3.749, word_reposition=0.713, ppl=38.04, wps=22780.7, ups=1.11, wpb=20438.7, bsz=256, num_updates=40000, lr=0.000176777, gnorm=1.707, clip=0, loss_scale=4096, train_wall=89, wall=36723
2022-07-12 03:49:56 | INFO | train_inner | epoch 036:    887 / 1122 loss=5.244, nll_loss=2.1, mask_ins=0.789, word_ins_ml=3.748, word_reposition=0.706, ppl=37.89, wps=22634, ups=1.11, wpb=20467.9, bsz=256, num_updates=40100, lr=0.000176556, gnorm=1.671, clip=0, loss_scale=4096, train_wall=90, wall=36813
2022-07-12 03:51:27 | INFO | train_inner | epoch 036:    987 / 1122 loss=5.201, nll_loss=2.054, mask_ins=0.781, word_ins_ml=3.707, word_reposition=0.713, ppl=36.78, wps=22734.8, ups=1.1, wpb=20662.3, bsz=256, num_updates=40200, lr=0.000176336, gnorm=1.674, clip=0, loss_scale=6472, train_wall=90, wall=36904
2022-07-12 03:52:56 | INFO | train_inner | epoch 036:   1087 / 1122 loss=5.217, nll_loss=2.082, mask_ins=0.779, word_ins_ml=3.732, word_reposition=0.707, ppl=37.2, wps=23018.4, ups=1.12, wpb=20559.7, bsz=256, num_updates=40300, lr=0.000176117, gnorm=1.724, clip=0, loss_scale=8192, train_wall=89, wall=36993
2022-07-12 03:53:27 | INFO | train | epoch 036 | loss 5.233 | nll_loss 2.086 | mask_ins 0.787 | word_ins_ml 3.736 | word_reposition 0.71 | ppl 37.6 | wps 22360.5 | ups 1.09 | wpb 20520.4 | bsz 255.8 | num_updates 40335 | lr 0.000176041 | gnorm 1.703 | clip 0 | loss_scale 6323 | train_wall 1001 | wall 37024
2022-07-12 03:53:45 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.994 | nll_loss 5.741 | mask_ins 1.474 | word_ins_ml 7.19 | word_reposition 1.33 | ppl 1019.81 | wps 55628.7 | wpb 2367.6 | bsz 32 | num_updates 40335 | best_loss 9.911
2022-07-12 03:53:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 36 @ 40335 updates, score 9.994) (writing took 2.938792636618018 seconds)
2022-07-12 03:54:46 | INFO | train_inner | epoch 037:     65 / 1122 loss=5.263, nll_loss=2.093, mask_ins=0.804, word_ins_ml=3.742, word_reposition=0.717, ppl=38.41, wps=18517.6, ups=0.91, wpb=20283.7, bsz=253.8, num_updates=40400, lr=0.000175899, gnorm=1.773, clip=0, loss_scale=8192, train_wall=88, wall=37103
2022-07-12 03:56:17 | INFO | train_inner | epoch 037:    165 / 1122 loss=5.204, nll_loss=2.058, mask_ins=0.788, word_ins_ml=3.711, word_reposition=0.706, ppl=36.86, wps=22507, ups=1.1, wpb=20536.7, bsz=256, num_updates=40500, lr=0.000175682, gnorm=1.667, clip=0, loss_scale=8192, train_wall=91, wall=37194
2022-07-12 03:57:46 | INFO | train_inner | epoch 037:    265 / 1122 loss=5.211, nll_loss=2.067, mask_ins=0.781, word_ins_ml=3.718, word_reposition=0.712, ppl=37.03, wps=23009.9, ups=1.12, wpb=20565.7, bsz=256, num_updates=40600, lr=0.000175466, gnorm=1.687, clip=0, loss_scale=8192, train_wall=89, wall=37284
2022-07-12 03:59:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 03:59:19 | INFO | train_inner | epoch 037:    366 / 1122 loss=5.195, nll_loss=2.051, mask_ins=0.784, word_ins_ml=3.705, word_reposition=0.706, ppl=36.64, wps=22206.2, ups=1.08, wpb=20598.7, bsz=256, num_updates=40700, lr=0.00017525, gnorm=1.694, clip=0, loss_scale=11761, train_wall=92, wall=37376
2022-07-12 04:00:50 | INFO | train_inner | epoch 037:    466 / 1122 loss=5.136, nll_loss=2, mask_ins=0.775, word_ins_ml=3.659, word_reposition=0.702, ppl=35.17, wps=22487.5, ups=1.1, wpb=20502.8, bsz=256, num_updates=40800, lr=0.000175035, gnorm=1.613, clip=0, loss_scale=8192, train_wall=91, wall=37468
2022-07-12 04:02:20 | INFO | train_inner | epoch 037:    566 / 1122 loss=5.224, nll_loss=2.082, mask_ins=0.785, word_ins_ml=3.732, word_reposition=0.707, ppl=37.37, wps=22911.2, ups=1.12, wpb=20500.7, bsz=256, num_updates=40900, lr=0.000174821, gnorm=1.663, clip=0, loss_scale=8192, train_wall=89, wall=37557
2022-07-12 04:03:50 | INFO | train_inner | epoch 037:    666 / 1122 loss=5.214, nll_loss=2.085, mask_ins=0.776, word_ins_ml=3.734, word_reposition=0.703, ppl=37.11, wps=22968.8, ups=1.11, wpb=20625.3, bsz=256, num_updates=41000, lr=0.000174608, gnorm=1.682, clip=0, loss_scale=8192, train_wall=89, wall=37647
2022-07-12 04:05:21 | INFO | train_inner | epoch 037:    766 / 1122 loss=5.231, nll_loss=2.083, mask_ins=0.782, word_ins_ml=3.733, word_reposition=0.716, ppl=37.55, wps=22639, ups=1.1, wpb=20583.2, bsz=256, num_updates=41100, lr=0.000174395, gnorm=1.701, clip=0, loss_scale=8192, train_wall=90, wall=37738
2022-07-12 04:06:54 | INFO | train_inner | epoch 037:    866 / 1122 loss=5.185, nll_loss=2.054, mask_ins=0.771, word_ins_ml=3.707, word_reposition=0.706, ppl=36.38, wps=21922.2, ups=1.07, wpb=20527.5, bsz=256, num_updates=41200, lr=0.000174183, gnorm=1.659, clip=0, loss_scale=8192, train_wall=93, wall=37831
2022-07-12 04:08:25 | INFO | train_inner | epoch 037:    966 / 1122 loss=5.222, nll_loss=2.091, mask_ins=0.774, word_ins_ml=3.739, word_reposition=0.708, ppl=37.31, wps=22642.5, ups=1.1, wpb=20548, bsz=256, num_updates=41300, lr=0.000173972, gnorm=1.662, clip=0, loss_scale=15647, train_wall=90, wall=37922
2022-07-12 04:08:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 04:09:55 | INFO | train_inner | epoch 037:   1067 / 1122 loss=5.202, nll_loss=2.075, mask_ins=0.776, word_ins_ml=3.725, word_reposition=0.701, ppl=36.81, wps=22649, ups=1.1, wpb=20499.6, bsz=256, num_updates=41400, lr=0.000173762, gnorm=1.701, clip=0, loss_scale=10220, train_wall=90, wall=38013
2022-07-12 04:10:44 | INFO | train | epoch 037 | loss 5.205 | nll_loss 2.066 | mask_ins 0.78 | word_ins_ml 3.717 | word_reposition 0.707 | ppl 36.88 | wps 22163.2 | ups 1.08 | wpb 20521.2 | bsz 255.8 | num_updates 41455 | lr 0.000173647 | gnorm 1.681 | clip 0 | loss_scale 9360 | train_wall 1009 | wall 38061
2022-07-12 04:11:02 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 10.061 | nll_loss 5.799 | mask_ins 1.523 | word_ins_ml 7.24 | word_reposition 1.299 | ppl 1068.47 | wps 55462.5 | wpb 2367.6 | bsz 32 | num_updates 41455 | best_loss 9.911
2022-07-12 04:11:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 37 @ 41455 updates, score 10.061) (writing took 2.852269775234163 seconds)
2022-07-12 04:11:46 | INFO | train_inner | epoch 038:     45 / 1122 loss=5.147, nll_loss=2.019, mask_ins=0.773, word_ins_ml=3.676, word_reposition=0.698, ppl=35.44, wps=18440.7, ups=0.91, wpb=20335.1, bsz=253.8, num_updates=41500, lr=0.000173553, gnorm=1.754, clip=0, loss_scale=8192, train_wall=89, wall=38123
2022-07-12 04:13:15 | INFO | train_inner | epoch 038:    145 / 1122 loss=5.202, nll_loss=2.066, mask_ins=0.779, word_ins_ml=3.717, word_reposition=0.705, ppl=36.8, wps=22964.3, ups=1.11, wpb=20601, bsz=256, num_updates=41600, lr=0.000173344, gnorm=1.682, clip=0, loss_scale=8192, train_wall=89, wall=38213
2022-07-12 04:14:48 | INFO | train_inner | epoch 038:    245 / 1122 loss=5.206, nll_loss=2.083, mask_ins=0.773, word_ins_ml=3.732, word_reposition=0.701, ppl=36.91, wps=22374, ups=1.08, wpb=20625, bsz=256, num_updates=41700, lr=0.000173136, gnorm=1.638, clip=0, loss_scale=8192, train_wall=92, wall=38305
2022-07-12 04:16:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 04:16:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 04:16:20 | INFO | train_inner | epoch 038:    347 / 1122 loss=5.147, nll_loss=2.007, mask_ins=0.777, word_ins_ml=3.665, word_reposition=0.705, ppl=35.42, wps=22016.2, ups=1.08, wpb=20397, bsz=256, num_updates=41800, lr=0.000172929, gnorm=1.671, clip=0, loss_scale=7228, train_wall=92, wall=38397
2022-07-12 04:17:51 | INFO | train_inner | epoch 038:    447 / 1122 loss=5.207, nll_loss=2.07, mask_ins=0.776, word_ins_ml=3.721, word_reposition=0.71, ppl=36.94, wps=22592, ups=1.11, wpb=20438.7, bsz=256, num_updates=41900, lr=0.000172722, gnorm=1.651, clip=0, loss_scale=2048, train_wall=90, wall=38488
2022-07-12 04:19:22 | INFO | train_inner | epoch 038:    547 / 1122 loss=5.141, nll_loss=2.013, mask_ins=0.772, word_ins_ml=3.67, word_reposition=0.699, ppl=35.29, wps=22522.9, ups=1.1, wpb=20457.8, bsz=256, num_updates=42000, lr=0.000172516, gnorm=1.651, clip=0, loss_scale=2048, train_wall=90, wall=38579
2022-07-12 04:20:51 | INFO | train_inner | epoch 038:    647 / 1122 loss=5.201, nll_loss=2.069, mask_ins=0.778, word_ins_ml=3.72, word_reposition=0.703, ppl=36.78, wps=22953.6, ups=1.12, wpb=20543.8, bsz=256, num_updates=42100, lr=0.000172311, gnorm=1.701, clip=0, loss_scale=2048, train_wall=89, wall=38668
2022-07-12 04:22:21 | INFO | train_inner | epoch 038:    747 / 1122 loss=5.183, nll_loss=2.023, mask_ins=0.785, word_ins_ml=3.679, word_reposition=0.719, ppl=36.34, wps=23021.6, ups=1.12, wpb=20606.2, bsz=256, num_updates=42200, lr=0.000172107, gnorm=1.64, clip=0, loss_scale=2048, train_wall=89, wall=38758
2022-07-12 04:23:50 | INFO | train_inner | epoch 038:    847 / 1122 loss=5.187, nll_loss=2.056, mask_ins=0.777, word_ins_ml=3.708, word_reposition=0.703, ppl=36.44, wps=22938.6, ups=1.12, wpb=20512.3, bsz=256, num_updates=42300, lr=0.000171904, gnorm=1.678, clip=0, loss_scale=2048, train_wall=89, wall=38847
2022-07-12 04:25:20 | INFO | train_inner | epoch 038:    947 / 1122 loss=5.268, nll_loss=2.117, mask_ins=0.793, word_ins_ml=3.761, word_reposition=0.714, ppl=38.54, wps=22853.9, ups=1.11, wpb=20634.9, bsz=256, num_updates=42400, lr=0.000171701, gnorm=1.707, clip=0, loss_scale=4096, train_wall=90, wall=38937
2022-07-12 04:26:51 | INFO | train_inner | epoch 038:   1047 / 1122 loss=5.174, nll_loss=2.038, mask_ins=0.778, word_ins_ml=3.693, word_reposition=0.704, ppl=36.11, wps=22644.7, ups=1.11, wpb=20445.9, bsz=256, num_updates=42500, lr=0.000171499, gnorm=1.68, clip=0, loss_scale=4096, train_wall=90, wall=39028
2022-07-12 04:27:57 | INFO | train | epoch 038 | loss 5.188 | nll_loss 2.052 | mask_ins 0.778 | word_ins_ml 3.705 | word_reposition 0.706 | ppl 36.46 | wps 22247.1 | ups 1.08 | wpb 20520.7 | bsz 255.8 | num_updates 42575 | lr 0.000171347 | gnorm 1.675 | clip 0 | loss_scale 4362 | train_wall 1005 | wall 39095
2022-07-12 04:28:15 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.946 | nll_loss 5.719 | mask_ins 1.485 | word_ins_ml 7.166 | word_reposition 1.295 | ppl 986.48 | wps 55542.7 | wpb 2367.6 | bsz 32 | num_updates 42575 | best_loss 9.911
2022-07-12 04:28:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 38 @ 42575 updates, score 9.946) (writing took 2.7243612660095096 seconds)
2022-07-12 04:28:41 | INFO | train_inner | epoch 039:     25 / 1122 loss=5.166, nll_loss=2.043, mask_ins=0.77, word_ins_ml=3.696, word_reposition=0.7, ppl=35.9, wps=18607.8, ups=0.91, wpb=20454.1, bsz=253.8, num_updates=42600, lr=0.000171297, gnorm=1.693, clip=0, loss_scale=4096, train_wall=89, wall=39138
2022-07-12 04:30:12 | INFO | train_inner | epoch 039:    125 / 1122 loss=5.162, nll_loss=2.032, mask_ins=0.772, word_ins_ml=3.687, word_reposition=0.704, ppl=35.81, wps=22257.2, ups=1.09, wpb=20463, bsz=256, num_updates=42700, lr=0.000171096, gnorm=1.66, clip=0, loss_scale=4096, train_wall=91, wall=39230
2022-07-12 04:31:44 | INFO | train_inner | epoch 039:    225 / 1122 loss=5.163, nll_loss=2.024, mask_ins=0.776, word_ins_ml=3.68, word_reposition=0.707, ppl=35.82, wps=22403, ups=1.09, wpb=20496.7, bsz=256, num_updates=42800, lr=0.000170896, gnorm=1.664, clip=0, loss_scale=4096, train_wall=91, wall=39321
2022-07-12 04:33:16 | INFO | train_inner | epoch 039:    325 / 1122 loss=5.189, nll_loss=2.056, mask_ins=0.776, word_ins_ml=3.708, word_reposition=0.705, ppl=36.48, wps=22412.5, ups=1.09, wpb=20591.1, bsz=256, num_updates=42900, lr=0.000170697, gnorm=1.67, clip=0, loss_scale=7700, train_wall=91, wall=39413
2022-07-12 04:34:46 | INFO | train_inner | epoch 039:    425 / 1122 loss=5.147, nll_loss=2.015, mask_ins=0.773, word_ins_ml=3.672, word_reposition=0.702, ppl=35.43, wps=22851, ups=1.11, wpb=20500, bsz=256, num_updates=43000, lr=0.000170499, gnorm=1.703, clip=0, loss_scale=8192, train_wall=89, wall=39503
2022-07-12 04:36:17 | INFO | train_inner | epoch 039:    525 / 1122 loss=5.216, nll_loss=2.084, mask_ins=0.773, word_ins_ml=3.733, word_reposition=0.71, ppl=37.17, wps=22549.6, ups=1.1, wpb=20582.3, bsz=256, num_updates=43100, lr=0.000170301, gnorm=1.676, clip=0, loss_scale=8192, train_wall=91, wall=39594
2022-07-12 04:37:46 | INFO | train_inner | epoch 039:    625 / 1122 loss=5.167, nll_loss=2.04, mask_ins=0.776, word_ins_ml=3.693, word_reposition=0.697, ppl=35.92, wps=22699.1, ups=1.12, wpb=20352.3, bsz=256, num_updates=43200, lr=0.000170103, gnorm=1.694, clip=0, loss_scale=8192, train_wall=89, wall=39684
2022-07-12 04:39:16 | INFO | train_inner | epoch 039:    725 / 1122 loss=5.17, nll_loss=2.034, mask_ins=0.778, word_ins_ml=3.689, word_reposition=0.704, ppl=36.01, wps=23035.2, ups=1.12, wpb=20611.4, bsz=256, num_updates=43300, lr=0.000169907, gnorm=1.653, clip=0, loss_scale=8192, train_wall=89, wall=39773
2022-07-12 04:40:45 | INFO | train_inner | epoch 039:    825 / 1122 loss=5.175, nll_loss=2.054, mask_ins=0.766, word_ins_ml=3.706, word_reposition=0.704, ppl=36.13, wps=23018.1, ups=1.12, wpb=20570.8, bsz=256, num_updates=43400, lr=0.000169711, gnorm=1.658, clip=0, loss_scale=14418, train_wall=89, wall=39862
2022-07-12 04:40:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 04:42:16 | INFO | train_inner | epoch 039:    926 / 1122 loss=5.148, nll_loss=2.026, mask_ins=0.763, word_ins_ml=3.681, word_reposition=0.703, ppl=35.45, wps=22714.2, ups=1.1, wpb=20569.2, bsz=256, num_updates=43500, lr=0.000169516, gnorm=1.663, clip=0, loss_scale=8273, train_wall=90, wall=39953
2022-07-12 04:42:22 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 04:43:47 | INFO | train_inner | epoch 039:   1027 / 1122 loss=5.164, nll_loss=2.046, mask_ins=0.763, word_ins_ml=3.698, word_reposition=0.702, ppl=35.85, wps=22675.9, ups=1.1, wpb=20639.5, bsz=256, num_updates=43600, lr=0.000169321, gnorm=1.68, clip=0, loss_scale=4339, train_wall=90, wall=40044
2022-07-12 04:45:12 | INFO | train | epoch 039 | loss 5.169 | nll_loss 2.04 | mask_ins 0.771 | word_ins_ml 3.694 | word_reposition 0.704 | ppl 35.98 | wps 22209.9 | ups 1.08 | wpb 20521 | bsz 255.8 | num_updates 43695 | lr 0.000169137 | gnorm 1.675 | clip 0 | loss_scale 7195 | train_wall 1007 | wall 40129
2022-07-12 04:45:30 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.919 | nll_loss 5.718 | mask_ins 1.493 | word_ins_ml 7.161 | word_reposition 1.265 | ppl 968.2 | wps 55860 | wpb 2367.6 | bsz 32 | num_updates 43695 | best_loss 9.911
2022-07-12 04:45:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 39 @ 43695 updates, score 9.919) (writing took 2.8296450432389975 seconds)
2022-07-12 04:45:37 | INFO | train_inner | epoch 040:      5 / 1122 loss=5.17, nll_loss=2.043, mask_ins=0.767, word_ins_ml=3.696, word_reposition=0.707, ppl=36.01, wps=18438.6, ups=0.91, wpb=20356.2, bsz=253.8, num_updates=43700, lr=0.000169128, gnorm=1.734, clip=0, loss_scale=4096, train_wall=89, wall=40154
2022-07-12 04:47:07 | INFO | train_inner | epoch 040:    105 / 1122 loss=5.165, nll_loss=2.032, mask_ins=0.77, word_ins_ml=3.687, word_reposition=0.708, ppl=35.87, wps=23058.3, ups=1.12, wpb=20625, bsz=256, num_updates=43800, lr=0.000168934, gnorm=1.674, clip=0, loss_scale=4096, train_wall=89, wall=40244
2022-07-12 04:48:36 | INFO | train_inner | epoch 040:    205 / 1122 loss=5.143, nll_loss=2.019, mask_ins=0.766, word_ins_ml=3.675, word_reposition=0.702, ppl=35.34, wps=23051.8, ups=1.12, wpb=20552.3, bsz=256, num_updates=43900, lr=0.000168742, gnorm=1.679, clip=0, loss_scale=4096, train_wall=89, wall=40333
2022-07-12 04:50:05 | INFO | train_inner | epoch 040:    305 / 1122 loss=5.159, nll_loss=2.031, mask_ins=0.767, word_ins_ml=3.686, word_reposition=0.706, ppl=35.73, wps=23072.2, ups=1.12, wpb=20556.5, bsz=256, num_updates=44000, lr=0.00016855, gnorm=1.668, clip=0, loss_scale=4096, train_wall=88, wall=40422
2022-07-12 04:51:34 | INFO | train_inner | epoch 040:    405 / 1122 loss=5.103, nll_loss=1.995, mask_ins=0.759, word_ins_ml=3.653, word_reposition=0.691, ppl=34.37, wps=22997.8, ups=1.12, wpb=20459.5, bsz=256, num_updates=44100, lr=0.000168359, gnorm=1.639, clip=0, loss_scale=7496, train_wall=88, wall=40511
2022-07-12 04:53:05 | INFO | train_inner | epoch 040:    505 / 1122 loss=5.136, nll_loss=2.004, mask_ins=0.768, word_ins_ml=3.661, word_reposition=0.707, ppl=35.16, wps=22650, ups=1.1, wpb=20510.2, bsz=256, num_updates=44200, lr=0.000168168, gnorm=1.711, clip=0, loss_scale=8192, train_wall=90, wall=40602
2022-07-12 04:54:35 | INFO | train_inner | epoch 040:    605 / 1122 loss=5.168, nll_loss=2.048, mask_ins=0.768, word_ins_ml=3.7, word_reposition=0.699, ppl=35.94, wps=22649.7, ups=1.1, wpb=20556.2, bsz=256, num_updates=44300, lr=0.000167978, gnorm=1.656, clip=0, loss_scale=8192, train_wall=90, wall=40692
2022-07-12 04:54:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 04:56:07 | INFO | train_inner | epoch 040:    706 / 1122 loss=5.154, nll_loss=2.033, mask_ins=0.77, word_ins_ml=3.687, word_reposition=0.697, ppl=35.61, wps=22448.2, ups=1.09, wpb=20599.9, bsz=256, num_updates=44400, lr=0.000167789, gnorm=1.644, clip=0, loss_scale=4826, train_wall=91, wall=40784
2022-07-12 04:57:36 | INFO | train_inner | epoch 040:    806 / 1122 loss=5.143, nll_loss=2.029, mask_ins=0.764, word_ins_ml=3.683, word_reposition=0.696, ppl=35.34, wps=22921.5, ups=1.12, wpb=20441.3, bsz=256, num_updates=44500, lr=0.0001676, gnorm=1.676, clip=0, loss_scale=4096, train_wall=89, wall=40873
2022-07-12 04:59:05 | INFO | train_inner | epoch 040:    906 / 1122 loss=5.114, nll_loss=1.997, mask_ins=0.762, word_ins_ml=3.655, word_reposition=0.697, ppl=34.62, wps=23063, ups=1.12, wpb=20581.8, bsz=256, num_updates=44600, lr=0.000167412, gnorm=1.62, clip=0, loss_scale=4096, train_wall=89, wall=40963
2022-07-12 04:59:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 05:00:35 | INFO | train_inner | epoch 040:   1007 / 1122 loss=5.141, nll_loss=2.02, mask_ins=0.771, word_ins_ml=3.675, word_reposition=0.695, ppl=35.28, wps=22769.3, ups=1.11, wpb=20467.8, bsz=256, num_updates=44700, lr=0.000167225, gnorm=1.69, clip=0, loss_scale=2656, train_wall=89, wall=41053
2022-07-12 05:02:05 | INFO | train_inner | epoch 040:   1107 / 1122 loss=5.147, nll_loss=2.022, mask_ins=0.763, word_ins_ml=3.677, word_reposition=0.707, ppl=35.43, wps=23038.2, ups=1.12, wpb=20555.8, bsz=256, num_updates=44800, lr=0.000167038, gnorm=1.647, clip=0, loss_scale=2048, train_wall=89, wall=41142
2022-07-12 05:02:18 | INFO | train | epoch 040 | loss 5.143 | nll_loss 2.021 | mask_ins 0.766 | word_ins_ml 3.677 | word_reposition 0.701 | ppl 35.35 | wps 22413.4 | ups 1.09 | wpb 20521.9 | bsz 255.8 | num_updates 44815 | lr 0.00016701 | gnorm 1.672 | clip 0 | loss_scale 4855 | train_wall 998 | wall 41155
2022-07-12 05:02:35 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.978 | nll_loss 5.697 | mask_ins 1.484 | word_ins_ml 7.152 | word_reposition 1.342 | ppl 1008.23 | wps 55678.4 | wpb 2367.6 | bsz 32 | num_updates 44815 | best_loss 9.911
2022-07-12 05:02:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 40 @ 44815 updates, score 9.978) (writing took 2.978993134573102 seconds)
2022-07-12 05:03:54 | INFO | train_inner | epoch 041:     85 / 1122 loss=5.122, nll_loss=2.002, mask_ins=0.767, word_ins_ml=3.659, word_reposition=0.697, ppl=34.83, wps=18576.4, ups=0.91, wpb=20347.7, bsz=253.8, num_updates=44900, lr=0.000166852, gnorm=1.75, clip=0, loss_scale=2048, train_wall=88, wall=41251
2022-07-12 05:05:23 | INFO | train_inner | epoch 041:    185 / 1122 loss=5.145, nll_loss=2.022, mask_ins=0.766, word_ins_ml=3.677, word_reposition=0.701, ppl=35.38, wps=22958.6, ups=1.12, wpb=20514.6, bsz=256, num_updates=45000, lr=0.000166667, gnorm=1.707, clip=0, loss_scale=2048, train_wall=89, wall=41341
2022-07-12 05:06:53 | INFO | train_inner | epoch 041:    285 / 1122 loss=5.167, nll_loss=2.049, mask_ins=0.769, word_ins_ml=3.701, word_reposition=0.697, ppl=35.93, wps=23015.2, ups=1.11, wpb=20705.9, bsz=256, num_updates=45100, lr=0.000166482, gnorm=1.647, clip=0, loss_scale=2048, train_wall=89, wall=41431
2022-07-12 05:08:23 | INFO | train_inner | epoch 041:    385 / 1122 loss=5.152, nll_loss=2.027, mask_ins=0.767, word_ins_ml=3.682, word_reposition=0.703, ppl=35.56, wps=22896.9, ups=1.12, wpb=20493.2, bsz=256, num_updates=45200, lr=0.000166298, gnorm=1.678, clip=0, loss_scale=3256, train_wall=89, wall=41520
2022-07-12 05:09:54 | INFO | train_inner | epoch 041:    485 / 1122 loss=5.087, nll_loss=1.983, mask_ins=0.745, word_ins_ml=3.642, word_reposition=0.699, ppl=33.98, wps=22582.4, ups=1.1, wpb=20549.1, bsz=256, num_updates=45300, lr=0.000166114, gnorm=1.663, clip=0, loss_scale=4096, train_wall=90, wall=41611
2022-07-12 05:11:25 | INFO | train_inner | epoch 041:    585 / 1122 loss=5.111, nll_loss=1.997, mask_ins=0.76, word_ins_ml=3.655, word_reposition=0.696, ppl=34.56, wps=22479.5, ups=1.09, wpb=20575.8, bsz=256, num_updates=45400, lr=0.000165931, gnorm=1.655, clip=0, loss_scale=4096, train_wall=91, wall=41703
2022-07-12 05:12:55 | INFO | train_inner | epoch 041:    685 / 1122 loss=5.168, nll_loss=2.048, mask_ins=0.768, word_ins_ml=3.7, word_reposition=0.7, ppl=35.94, wps=23012.8, ups=1.12, wpb=20577.8, bsz=256, num_updates=45500, lr=0.000165748, gnorm=1.693, clip=0, loss_scale=4096, train_wall=89, wall=41792
2022-07-12 05:14:24 | INFO | train_inner | epoch 041:    785 / 1122 loss=5.143, nll_loss=2.016, mask_ins=0.771, word_ins_ml=3.671, word_reposition=0.701, ppl=35.33, wps=23016.4, ups=1.12, wpb=20549.7, bsz=256, num_updates=45600, lr=0.000165567, gnorm=1.681, clip=0, loss_scale=4096, train_wall=89, wall=41881
2022-07-12 05:15:53 | INFO | train_inner | epoch 041:    885 / 1122 loss=5.123, nll_loss=2.017, mask_ins=0.758, word_ins_ml=3.672, word_reposition=0.693, ppl=34.85, wps=22974.4, ups=1.12, wpb=20497.9, bsz=256, num_updates=45700, lr=0.000165385, gnorm=1.622, clip=0, loss_scale=6021, train_wall=89, wall=41971
2022-07-12 05:16:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 05:17:24 | INFO | train_inner | epoch 041:    986 / 1122 loss=5.124, nll_loss=2.002, mask_ins=0.766, word_ins_ml=3.659, word_reposition=0.699, ppl=34.87, wps=22559.3, ups=1.1, wpb=20478.2, bsz=256, num_updates=45800, lr=0.000165205, gnorm=1.613, clip=0, loss_scale=4826, train_wall=90, wall=42061
2022-07-12 05:18:54 | INFO | train_inner | epoch 041:   1086 / 1122 loss=5.114, nll_loss=1.994, mask_ins=0.768, word_ins_ml=3.652, word_reposition=0.694, ppl=34.64, wps=22903, ups=1.12, wpb=20527, bsz=256, num_updates=45900, lr=0.000165025, gnorm=1.597, clip=0, loss_scale=4096, train_wall=89, wall=42151
2022-07-12 05:19:26 | INFO | train | epoch 041 | loss 5.132 | nll_loss 2.014 | mask_ins 0.764 | word_ins_ml 3.67 | word_reposition 0.698 | ppl 35.06 | wps 22381.2 | ups 1.09 | wpb 20521.3 | bsz 255.8 | num_updates 45936 | lr 0.00016496 | gnorm 1.661 | clip 0 | loss_scale 3738 | train_wall 1000 | wall 42183
2022-07-12 05:19:43 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.884 | nll_loss 5.607 | mask_ins 1.454 | word_ins_ml 7.061 | word_reposition 1.369 | ppl 944.87 | wps 55483.3 | wpb 2367.6 | bsz 32 | num_updates 45936 | best_loss 9.884
2022-07-12 05:19:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 41 @ 45936 updates, score 9.884) (writing took 5.2750789653509855 seconds)
2022-07-12 05:20:46 | INFO | train_inner | epoch 042:     64 / 1122 loss=5.11, nll_loss=1.999, mask_ins=0.758, word_ins_ml=3.657, word_reposition=0.696, ppl=34.54, wps=18158.2, ups=0.89, wpb=20354, bsz=253.8, num_updates=46000, lr=0.000164845, gnorm=1.668, clip=0, loss_scale=4096, train_wall=88, wall=42263
2022-07-12 05:22:15 | INFO | train_inner | epoch 042:    164 / 1122 loss=5.113, nll_loss=1.996, mask_ins=0.762, word_ins_ml=3.654, word_reposition=0.697, ppl=34.6, wps=22858.6, ups=1.12, wpb=20390.3, bsz=256, num_updates=46100, lr=0.000164666, gnorm=1.628, clip=0, loss_scale=4096, train_wall=89, wall=42352
2022-07-12 05:23:44 | INFO | train_inner | epoch 042:    264 / 1122 loss=5.131, nll_loss=2.015, mask_ins=0.756, word_ins_ml=3.67, word_reposition=0.705, ppl=35.03, wps=23013, ups=1.12, wpb=20531.6, bsz=256, num_updates=46200, lr=0.000164488, gnorm=1.641, clip=0, loss_scale=4096, train_wall=89, wall=42441
2022-07-12 05:25:13 | INFO | train_inner | epoch 042:    364 / 1122 loss=5.108, nll_loss=1.985, mask_ins=0.764, word_ins_ml=3.643, word_reposition=0.701, ppl=34.49, wps=23060.9, ups=1.12, wpb=20538.2, bsz=256, num_updates=46300, lr=0.00016431, gnorm=1.655, clip=0, loss_scale=7004, train_wall=88, wall=42531
2022-07-12 05:25:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 05:26:43 | INFO | train_inner | epoch 042:    465 / 1122 loss=5.146, nll_loss=2.038, mask_ins=0.759, word_ins_ml=3.691, word_reposition=0.696, ppl=35.41, wps=22837, ups=1.11, wpb=20563.6, bsz=256, num_updates=46400, lr=0.000164133, gnorm=1.633, clip=0, loss_scale=5029, train_wall=89, wall=42621
2022-07-12 05:28:13 | INFO | train_inner | epoch 042:    565 / 1122 loss=5.112, nll_loss=1.999, mask_ins=0.76, word_ins_ml=3.656, word_reposition=0.695, ppl=34.57, wps=23060.5, ups=1.12, wpb=20576.6, bsz=256, num_updates=46500, lr=0.000163956, gnorm=1.669, clip=0, loss_scale=4096, train_wall=89, wall=42710
2022-07-12 05:29:42 | INFO | train_inner | epoch 042:    665 / 1122 loss=5.164, nll_loss=2.028, mask_ins=0.78, word_ins_ml=3.682, word_reposition=0.702, ppl=35.86, wps=22989.6, ups=1.12, wpb=20455.6, bsz=256, num_updates=46600, lr=0.00016378, gnorm=1.662, clip=0, loss_scale=4096, train_wall=88, wall=42799
2022-07-12 05:31:11 | INFO | train_inner | epoch 042:    765 / 1122 loss=5.119, nll_loss=2.011, mask_ins=0.76, word_ins_ml=3.666, word_reposition=0.693, ppl=34.76, wps=22909.7, ups=1.12, wpb=20546.1, bsz=256, num_updates=46700, lr=0.000163605, gnorm=1.68, clip=0, loss_scale=4096, train_wall=89, wall=42888
2022-07-12 05:32:41 | INFO | train_inner | epoch 042:    865 / 1122 loss=5.146, nll_loss=2.016, mask_ins=0.768, word_ins_ml=3.671, word_reposition=0.707, ppl=35.41, wps=23088.2, ups=1.12, wpb=20614.6, bsz=256, num_updates=46800, lr=0.00016343, gnorm=1.634, clip=0, loss_scale=4096, train_wall=89, wall=42978
2022-07-12 05:34:10 | INFO | train_inner | epoch 042:    965 / 1122 loss=5.115, nll_loss=1.998, mask_ins=0.761, word_ins_ml=3.656, word_reposition=0.698, ppl=34.65, wps=23019.1, ups=1.12, wpb=20587.1, bsz=256, num_updates=46900, lr=0.000163256, gnorm=1.628, clip=0, loss_scale=6799, train_wall=89, wall=43067
2022-07-12 05:35:40 | INFO | train_inner | epoch 042:   1065 / 1122 loss=5.125, nll_loss=2.009, mask_ins=0.759, word_ins_ml=3.665, word_reposition=0.701, ppl=34.9, wps=22942.8, ups=1.11, wpb=20586.3, bsz=256, num_updates=47000, lr=0.000163082, gnorm=1.626, clip=0, loss_scale=8192, train_wall=89, wall=43157
2022-07-12 05:36:30 | INFO | train | epoch 042 | loss 5.126 | nll_loss 2.008 | mask_ins 0.763 | word_ins_ml 3.664 | word_reposition 0.699 | ppl 34.93 | wps 22446.6 | ups 1.09 | wpb 20520.7 | bsz 255.8 | num_updates 47057 | lr 0.000162983 | gnorm 1.647 | clip 0 | loss_scale 5253 | train_wall 995 | wall 43208
2022-07-12 05:36:48 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.893 | nll_loss 5.727 | mask_ins 1.492 | word_ins_ml 7.171 | word_reposition 1.23 | ppl 951.03 | wps 55583.6 | wpb 2367.6 | bsz 32 | num_updates 47057 | best_loss 9.884
2022-07-12 05:36:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 42 @ 47057 updates, score 9.893) (writing took 2.9652259377762675 seconds)
2022-07-12 05:37:29 | INFO | train_inner | epoch 043:     43 / 1122 loss=5.132, nll_loss=2.019, mask_ins=0.761, word_ins_ml=3.674, word_reposition=0.696, ppl=35.06, wps=18483.3, ups=0.91, wpb=20273.3, bsz=253.8, num_updates=47100, lr=0.000162909, gnorm=1.684, clip=0, loss_scale=8192, train_wall=88, wall=43267
2022-07-12 05:38:59 | INFO | train_inner | epoch 043:    143 / 1122 loss=5.092, nll_loss=1.974, mask_ins=0.762, word_ins_ml=3.634, word_reposition=0.696, ppl=34.11, wps=22826.7, ups=1.11, wpb=20491.3, bsz=256, num_updates=47200, lr=0.000162736, gnorm=1.626, clip=0, loss_scale=8192, train_wall=89, wall=43356
2022-07-12 05:40:29 | INFO | train_inner | epoch 043:    243 / 1122 loss=5.113, nll_loss=2, mask_ins=0.758, word_ins_ml=3.657, word_reposition=0.698, ppl=34.6, wps=23133.9, ups=1.12, wpb=20724.5, bsz=256, num_updates=47300, lr=0.000162564, gnorm=1.617, clip=0, loss_scale=8192, train_wall=89, wall=43446
2022-07-12 05:41:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 05:41:59 | INFO | train_inner | epoch 043:    344 / 1122 loss=5.118, nll_loss=1.988, mask_ins=0.771, word_ins_ml=3.647, word_reposition=0.7, ppl=34.73, wps=22771.5, ups=1.1, wpb=20629.6, bsz=256, num_updates=47400, lr=0.000162392, gnorm=1.628, clip=0, loss_scale=8679, train_wall=90, wall=43537
2022-07-12 05:43:29 | INFO | train_inner | epoch 043:    444 / 1122 loss=5.122, nll_loss=1.992, mask_ins=0.771, word_ins_ml=3.65, word_reposition=0.701, ppl=34.82, wps=23005.5, ups=1.12, wpb=20542.4, bsz=256, num_updates=47500, lr=0.000162221, gnorm=1.612, clip=0, loss_scale=8192, train_wall=89, wall=43626
2022-07-12 05:44:58 | INFO | train_inner | epoch 043:    544 / 1122 loss=5.093, nll_loss=1.976, mask_ins=0.766, word_ins_ml=3.636, word_reposition=0.69, ppl=34.13, wps=22982, ups=1.12, wpb=20487.2, bsz=256, num_updates=47600, lr=0.000162051, gnorm=1.655, clip=0, loss_scale=8192, train_wall=89, wall=43715
2022-07-12 05:46:27 | INFO | train_inner | epoch 043:    644 / 1122 loss=5.127, nll_loss=2.02, mask_ins=0.754, word_ins_ml=3.675, word_reposition=0.698, ppl=34.94, wps=22825.7, ups=1.12, wpb=20406, bsz=256, num_updates=47700, lr=0.000161881, gnorm=1.652, clip=0, loss_scale=8192, train_wall=89, wall=43804
2022-07-12 05:47:57 | INFO | train_inner | epoch 043:    744 / 1122 loss=5.127, nll_loss=2.007, mask_ins=0.771, word_ins_ml=3.663, word_reposition=0.693, ppl=34.95, wps=22847.5, ups=1.12, wpb=20457.8, bsz=256, num_updates=47800, lr=0.000161712, gnorm=1.596, clip=0, loss_scale=8192, train_wall=89, wall=43894
2022-07-12 05:49:32 | INFO | train_inner | epoch 043:    844 / 1122 loss=5.097, nll_loss=1.991, mask_ins=0.755, word_ins_ml=3.648, word_reposition=0.693, ppl=34.22, wps=21722, ups=1.05, wpb=20604, bsz=256, num_updates=47900, lr=0.000161543, gnorm=1.647, clip=0, loss_scale=11223, train_wall=94, wall=43989
2022-07-12 05:50:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 05:51:09 | INFO | train_inner | epoch 043:    945 / 1122 loss=5.153, nll_loss=2.037, mask_ins=0.76, word_ins_ml=3.689, word_reposition=0.703, ppl=35.57, wps=21144.7, ups=1.03, wpb=20521.4, bsz=256, num_updates=48000, lr=0.000161374, gnorm=1.647, clip=0, loss_scale=13059, train_wall=96, wall=44086
2022-07-12 05:52:40 | INFO | train_inner | epoch 043:   1045 / 1122 loss=5.122, nll_loss=2.008, mask_ins=0.762, word_ins_ml=3.663, word_reposition=0.697, ppl=34.83, wps=22612.8, ups=1.1, wpb=20543.9, bsz=256, num_updates=48100, lr=0.000161206, gnorm=1.644, clip=0, loss_scale=8192, train_wall=90, wall=44177
2022-07-12 05:53:48 | INFO | train | epoch 043 | loss 5.116 | nll_loss 2 | mask_ins 0.763 | word_ins_ml 3.656 | word_reposition 0.697 | ppl 34.68 | wps 22154.7 | ups 1.08 | wpb 20523.1 | bsz 255.8 | num_updates 48177 | lr 0.000161078 | gnorm 1.642 | clip 0 | loss_scale 8944 | train_wall 1010 | wall 44245
2022-07-12 05:54:06 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.886 | nll_loss 5.662 | mask_ins 1.475 | word_ins_ml 7.112 | word_reposition 1.3 | ppl 946.18 | wps 55705.1 | wpb 2367.6 | bsz 32 | num_updates 48177 | best_loss 9.884
2022-07-12 05:54:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 43 @ 48177 updates, score 9.886) (writing took 3.1142973322421312 seconds)
2022-07-12 05:54:30 | INFO | train_inner | epoch 044:     23 / 1122 loss=5.108, nll_loss=1.993, mask_ins=0.767, word_ins_ml=3.651, word_reposition=0.691, ppl=34.49, wps=18525.3, ups=0.91, wpb=20366.2, bsz=253.8, num_updates=48200, lr=0.000161039, gnorm=1.737, clip=0, loss_scale=8192, train_wall=88, wall=44287
2022-07-12 05:55:59 | INFO | train_inner | epoch 044:    123 / 1122 loss=5.064, nll_loss=1.963, mask_ins=0.754, word_ins_ml=3.624, word_reposition=0.687, ppl=33.46, wps=23012.6, ups=1.12, wpb=20544.2, bsz=256, num_updates=48300, lr=0.000160872, gnorm=1.596, clip=0, loss_scale=8192, train_wall=89, wall=44376
2022-07-12 05:57:28 | INFO | train_inner | epoch 044:    223 / 1122 loss=5.09, nll_loss=1.99, mask_ins=0.751, word_ins_ml=3.648, word_reposition=0.691, ppl=34.05, wps=22969.3, ups=1.12, wpb=20459.6, bsz=256, num_updates=48400, lr=0.000160706, gnorm=1.592, clip=0, loss_scale=8192, train_wall=88, wall=44465
2022-07-12 05:58:57 | INFO | train_inner | epoch 044:    323 / 1122 loss=5.079, nll_loss=1.985, mask_ins=0.751, word_ins_ml=3.643, word_reposition=0.686, ppl=33.8, wps=22990.7, ups=1.12, wpb=20479.5, bsz=256, num_updates=48500, lr=0.00016054, gnorm=1.624, clip=0, loss_scale=10568, train_wall=88, wall=44554
2022-07-12 05:59:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 05:59:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 06:00:28 | INFO | train_inner | epoch 044:    425 / 1122 loss=5.112, nll_loss=2.006, mask_ins=0.759, word_ins_ml=3.661, word_reposition=0.692, ppl=34.58, wps=22519.3, ups=1.09, wpb=20611.2, bsz=256, num_updates=48600, lr=0.000160375, gnorm=1.625, clip=0, loss_scale=8072, train_wall=91, wall=44646
2022-07-12 06:01:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 06:01:59 | INFO | train_inner | epoch 044:    526 / 1122 loss=5.119, nll_loss=2.003, mask_ins=0.76, word_ins_ml=3.659, word_reposition=0.7, ppl=34.76, wps=22765.1, ups=1.11, wpb=20599.5, bsz=256, num_updates=48700, lr=0.00016021, gnorm=1.665, clip=0, loss_scale=2737, train_wall=90, wall=44736
2022-07-12 06:03:28 | INFO | train_inner | epoch 044:    626 / 1122 loss=5.087, nll_loss=1.97, mask_ins=0.761, word_ins_ml=3.63, word_reposition=0.696, ppl=34, wps=22987.2, ups=1.12, wpb=20458.7, bsz=256, num_updates=48800, lr=0.000160046, gnorm=1.621, clip=0, loss_scale=2048, train_wall=88, wall=44825
2022-07-12 06:04:57 | INFO | train_inner | epoch 044:    726 / 1122 loss=5.077, nll_loss=1.971, mask_ins=0.755, word_ins_ml=3.63, word_reposition=0.692, ppl=33.76, wps=23053, ups=1.12, wpb=20505.6, bsz=256, num_updates=48900, lr=0.000159882, gnorm=1.603, clip=0, loss_scale=2048, train_wall=88, wall=44914
2022-07-12 06:06:26 | INFO | train_inner | epoch 044:    826 / 1122 loss=5.089, nll_loss=1.993, mask_ins=0.748, word_ins_ml=3.65, word_reposition=0.69, ppl=34.03, wps=23034.1, ups=1.13, wpb=20467.5, bsz=256, num_updates=49000, lr=0.000159719, gnorm=1.706, clip=0, loss_scale=2048, train_wall=88, wall=45003
2022-07-12 06:07:59 | INFO | train_inner | epoch 044:    926 / 1122 loss=5.041, nll_loss=1.945, mask_ins=0.745, word_ins_ml=3.607, word_reposition=0.689, ppl=32.93, wps=22109.1, ups=1.08, wpb=20535.1, bsz=256, num_updates=49100, lr=0.000159556, gnorm=1.632, clip=0, loss_scale=2048, train_wall=92, wall=45096
2022-07-12 06:09:33 | INFO | train_inner | epoch 044:   1026 / 1122 loss=5.098, nll_loss=1.989, mask_ins=0.754, word_ins_ml=3.646, word_reposition=0.698, ppl=34.25, wps=21814.1, ups=1.06, wpb=20571, bsz=256, num_updates=49200, lr=0.000159394, gnorm=1.616, clip=0, loss_scale=3174, train_wall=94, wall=45190
2022-07-12 06:10:59 | INFO | train | epoch 044 | loss 5.087 | nll_loss 1.981 | mask_ins 0.754 | word_ins_ml 3.64 | word_reposition 0.693 | ppl 33.98 | wps 22259.6 | ups 1.08 | wpb 20521.1 | bsz 255.8 | num_updates 49296 | lr 0.000159239 | gnorm 1.633 | clip 0 | loss_scale 4914 | train_wall 1004 | wall 45277
2022-07-12 06:11:17 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.849 | nll_loss 5.607 | mask_ins 1.484 | word_ins_ml 7.055 | word_reposition 1.31 | ppl 922.33 | wps 55407.9 | wpb 2367.6 | bsz 32 | num_updates 49296 | best_loss 9.849
2022-07-12 06:11:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 44 @ 49296 updates, score 9.849) (writing took 5.141153691336513 seconds)
2022-07-12 06:11:26 | INFO | train_inner | epoch 045:      4 / 1122 loss=5.082, nll_loss=1.97, mask_ins=0.754, word_ins_ml=3.629, word_reposition=0.7, ppl=33.87, wps=18061, ups=0.88, wpb=20464.1, bsz=253.8, num_updates=49300, lr=0.000159232, gnorm=1.672, clip=0, loss_scale=4096, train_wall=90, wall=45303
2022-07-12 06:12:56 | INFO | train_inner | epoch 045:    104 / 1122 loss=5.042, nll_loss=1.937, mask_ins=0.749, word_ins_ml=3.601, word_reposition=0.692, ppl=32.94, wps=22757, ups=1.11, wpb=20518.1, bsz=256, num_updates=49400, lr=0.000159071, gnorm=1.668, clip=0, loss_scale=4096, train_wall=89, wall=45394
2022-07-12 06:14:26 | INFO | train_inner | epoch 045:    204 / 1122 loss=5.077, nll_loss=1.972, mask_ins=0.755, word_ins_ml=3.631, word_reposition=0.691, ppl=33.76, wps=23053.5, ups=1.12, wpb=20546.2, bsz=256, num_updates=49500, lr=0.00015891, gnorm=1.66, clip=0, loss_scale=4096, train_wall=88, wall=45483
2022-07-12 06:15:56 | INFO | train_inner | epoch 045:    304 / 1122 loss=5.077, nll_loss=1.965, mask_ins=0.755, word_ins_ml=3.625, word_reposition=0.696, ppl=33.75, wps=22632.5, ups=1.1, wpb=20542.2, bsz=256, num_updates=49600, lr=0.00015875, gnorm=1.596, clip=0, loss_scale=4096, train_wall=90, wall=45573
2022-07-12 06:17:27 | INFO | train_inner | epoch 045:    404 / 1122 loss=5.063, nll_loss=1.967, mask_ins=0.75, word_ins_ml=3.627, word_reposition=0.687, ppl=33.43, wps=22496.8, ups=1.1, wpb=20412.2, bsz=256, num_updates=49700, lr=0.00015859, gnorm=1.613, clip=0, loss_scale=5857, train_wall=90, wall=45664
2022-07-12 06:18:58 | INFO | train_inner | epoch 045:    504 / 1122 loss=5.081, nll_loss=1.979, mask_ins=0.75, word_ins_ml=3.638, word_reposition=0.693, ppl=33.85, wps=22599.6, ups=1.1, wpb=20513.5, bsz=256, num_updates=49800, lr=0.000158431, gnorm=1.616, clip=0, loss_scale=8192, train_wall=90, wall=45755
2022-07-12 06:20:30 | INFO | train_inner | epoch 045:    604 / 1122 loss=5.114, nll_loss=2.013, mask_ins=0.751, word_ins_ml=3.668, word_reposition=0.695, ppl=34.62, wps=22339.1, ups=1.09, wpb=20519.3, bsz=256, num_updates=49900, lr=0.000158272, gnorm=1.638, clip=0, loss_scale=8192, train_wall=91, wall=45847
2022-07-12 06:22:03 | INFO | train_inner | epoch 045:    704 / 1122 loss=5.063, nll_loss=1.969, mask_ins=0.743, word_ins_ml=3.629, word_reposition=0.691, ppl=33.42, wps=22101.9, ups=1.07, wpb=20623.7, bsz=256, num_updates=50000, lr=0.000158114, gnorm=1.601, clip=0, loss_scale=8192, train_wall=93, wall=45940
2022-07-12 06:23:35 | INFO | train_inner | epoch 045:    804 / 1122 loss=5.085, nll_loss=1.974, mask_ins=0.754, word_ins_ml=3.633, word_reposition=0.698, ppl=33.94, wps=22236.2, ups=1.08, wpb=20510.5, bsz=256, num_updates=50100, lr=0.000157956, gnorm=1.645, clip=0, loss_scale=8192, train_wall=92, wall=46032
2022-07-12 06:25:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 06:25:07 | INFO | train_inner | epoch 045:    905 / 1122 loss=5.091, nll_loss=1.986, mask_ins=0.755, word_ins_ml=3.643, word_reposition=0.693, ppl=34.07, wps=22347.9, ups=1.08, wpb=20618, bsz=256, num_updates=50200, lr=0.000157799, gnorm=1.639, clip=0, loss_scale=10220, train_wall=92, wall=46125
2022-07-12 06:26:39 | INFO | train_inner | epoch 045:   1005 / 1122 loss=5.116, nll_loss=2.004, mask_ins=0.755, word_ins_ml=3.66, word_reposition=0.701, ppl=34.68, wps=22623.1, ups=1.1, wpb=20609.1, bsz=256, num_updates=50300, lr=0.000157642, gnorm=1.645, clip=0, loss_scale=8192, train_wall=90, wall=46216
2022-07-12 06:28:08 | INFO | train_inner | epoch 045:   1105 / 1122 loss=5.07, nll_loss=1.964, mask_ins=0.755, word_ins_ml=3.624, word_reposition=0.691, ppl=33.59, wps=22912.8, ups=1.12, wpb=20465, bsz=256, num_updates=50400, lr=0.000157485, gnorm=1.617, clip=0, loss_scale=8192, train_wall=89, wall=46305
2022-07-12 06:28:23 | INFO | train | epoch 045 | loss 5.079 | nll_loss 1.975 | mask_ins 0.752 | word_ins_ml 3.634 | word_reposition 0.693 | ppl 33.8 | wps 22050 | ups 1.07 | wpb 20521.4 | bsz 255.8 | num_updates 50417 | lr 0.000157459 | gnorm 1.636 | clip 0 | loss_scale 7057 | train_wall 1013 | wall 46320
2022-07-12 06:28:41 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.975 | nll_loss 5.777 | mask_ins 1.487 | word_ins_ml 7.218 | word_reposition 1.269 | ppl 1006.07 | wps 55740.4 | wpb 2367.6 | bsz 32 | num_updates 50417 | best_loss 9.849
2022-07-12 06:28:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 45 @ 50417 updates, score 9.975) (writing took 2.9647487429901958 seconds)
2022-07-12 06:29:58 | INFO | train_inner | epoch 046:     83 / 1122 loss=5.077, nll_loss=1.966, mask_ins=0.755, word_ins_ml=3.626, word_reposition=0.696, ppl=33.75, wps=18564.3, ups=0.91, wpb=20509.1, bsz=253.8, num_updates=50500, lr=0.000157329, gnorm=1.683, clip=0, loss_scale=8192, train_wall=89, wall=46416
2022-07-12 06:31:28 | INFO | train_inner | epoch 046:    183 / 1122 loss=5.022, nll_loss=1.937, mask_ins=0.739, word_ins_ml=3.6, word_reposition=0.683, ppl=32.49, wps=22849.9, ups=1.12, wpb=20468, bsz=256, num_updates=50600, lr=0.000157174, gnorm=1.6, clip=0, loss_scale=8192, train_wall=89, wall=46505
2022-07-12 06:32:57 | INFO | train_inner | epoch 046:    283 / 1122 loss=5.077, nll_loss=1.979, mask_ins=0.756, word_ins_ml=3.637, word_reposition=0.685, ppl=33.76, wps=23188.3, ups=1.13, wpb=20594.8, bsz=256, num_updates=50700, lr=0.000157019, gnorm=1.667, clip=0, loss_scale=8192, train_wall=88, wall=46594
2022-07-12 06:33:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 06:34:27 | INFO | train_inner | epoch 046:    384 / 1122 loss=5.073, nll_loss=1.979, mask_ins=0.748, word_ins_ml=3.637, word_reposition=0.688, ppl=33.66, wps=22679.5, ups=1.11, wpb=20514.7, bsz=256, num_updates=50800, lr=0.000156864, gnorm=1.635, clip=0, loss_scale=13221, train_wall=90, wall=46684
2022-07-12 06:35:56 | INFO | train_inner | epoch 046:    484 / 1122 loss=5.009, nll_loss=1.92, mask_ins=0.739, word_ins_ml=3.585, word_reposition=0.684, ppl=32.19, wps=23043.7, ups=1.12, wpb=20528.9, bsz=256, num_updates=50900, lr=0.00015671, gnorm=1.58, clip=0, loss_scale=8192, train_wall=88, wall=46773
2022-07-12 06:37:25 | INFO | train_inner | epoch 046:    584 / 1122 loss=5.059, nll_loss=1.96, mask_ins=0.744, word_ins_ml=3.62, word_reposition=0.695, ppl=33.33, wps=23096.3, ups=1.12, wpb=20599.2, bsz=256, num_updates=51000, lr=0.000156556, gnorm=1.639, clip=0, loss_scale=8192, train_wall=89, wall=46863
2022-07-12 06:38:55 | INFO | train_inner | epoch 046:    684 / 1122 loss=5.047, nll_loss=1.956, mask_ins=0.744, word_ins_ml=3.617, word_reposition=0.686, ppl=33.05, wps=23000.5, ups=1.12, wpb=20526.5, bsz=256, num_updates=51100, lr=0.000156403, gnorm=1.6, clip=0, loss_scale=8192, train_wall=89, wall=46952
2022-07-12 06:40:24 | INFO | train_inner | epoch 046:    784 / 1122 loss=5.102, nll_loss=2.002, mask_ins=0.755, word_ins_ml=3.658, word_reposition=0.69, ppl=34.36, wps=23017.1, ups=1.12, wpb=20485.2, bsz=256, num_updates=51200, lr=0.00015625, gnorm=1.637, clip=0, loss_scale=8192, train_wall=88, wall=47041
2022-07-12 06:41:53 | INFO | train_inner | epoch 046:    884 / 1122 loss=5.079, nll_loss=1.97, mask_ins=0.751, word_ins_ml=3.63, word_reposition=0.697, ppl=33.79, wps=22816.7, ups=1.12, wpb=20389, bsz=256, num_updates=51300, lr=0.000156098, gnorm=1.632, clip=0, loss_scale=9994, train_wall=89, wall=47130
2022-07-12 06:42:36 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 06:43:23 | INFO | train_inner | epoch 046:    985 / 1122 loss=5.055, nll_loss=1.95, mask_ins=0.755, word_ins_ml=3.612, word_reposition=0.689, ppl=33.24, wps=22841.5, ups=1.11, wpb=20526.6, bsz=256, num_updates=51400, lr=0.000155946, gnorm=1.622, clip=0, loss_scale=12004, train_wall=89, wall=47220
2022-07-12 06:44:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 06:44:53 | INFO | train_inner | epoch 046:   1086 / 1122 loss=5.058, nll_loss=1.957, mask_ins=0.748, word_ins_ml=3.617, word_reposition=0.693, ppl=33.31, wps=22882.1, ups=1.11, wpb=20596.7, bsz=256, num_updates=51500, lr=0.000155794, gnorm=1.586, clip=0, loss_scale=6894, train_wall=89, wall=47310
2022-07-12 06:45:25 | INFO | train | epoch 046 | loss 5.06 | nll_loss 1.962 | mask_ins 0.749 | word_ins_ml 3.622 | word_reposition 0.69 | ppl 33.37 | wps 22467.3 | ups 1.09 | wpb 20520.4 | bsz 255.8 | num_updates 51536 | lr 0.00015574 | gnorm 1.625 | clip 0 | loss_scale 8900 | train_wall 994 | wall 47342
2022-07-12 06:45:43 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.885 | nll_loss 5.667 | mask_ins 1.484 | word_ins_ml 7.12 | word_reposition 1.281 | ppl 945.74 | wps 55788 | wpb 2367.6 | bsz 32 | num_updates 51536 | best_loss 9.849
2022-07-12 06:45:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 46 @ 51536 updates, score 9.885) (writing took 2.855808839201927 seconds)
2022-07-12 06:46:42 | INFO | train_inner | epoch 047:     64 / 1122 loss=5.051, nll_loss=1.951, mask_ins=0.75, word_ins_ml=3.612, word_reposition=0.688, ppl=33.14, wps=18563.8, ups=0.91, wpb=20329, bsz=253.8, num_updates=51600, lr=0.000155643, gnorm=1.646, clip=0, loss_scale=4096, train_wall=88, wall=47420
2022-07-12 06:48:12 | INFO | train_inner | epoch 047:    164 / 1122 loss=5.013, nll_loss=1.921, mask_ins=0.739, word_ins_ml=3.586, word_reposition=0.689, ppl=32.3, wps=22914.9, ups=1.12, wpb=20451.8, bsz=256, num_updates=51700, lr=0.000155493, gnorm=1.626, clip=0, loss_scale=4096, train_wall=89, wall=47509
2022-07-12 06:49:41 | INFO | train_inner | epoch 047:    264 / 1122 loss=5.059, nll_loss=1.953, mask_ins=0.752, word_ins_ml=3.615, word_reposition=0.692, ppl=33.33, wps=22926, ups=1.12, wpb=20517.2, bsz=256, num_updates=51800, lr=0.000155342, gnorm=1.613, clip=0, loss_scale=4096, train_wall=89, wall=47598
2022-07-12 06:51:10 | INFO | train_inner | epoch 047:    364 / 1122 loss=5.037, nll_loss=1.947, mask_ins=0.741, word_ins_ml=3.609, word_reposition=0.687, ppl=32.84, wps=23068.1, ups=1.12, wpb=20553.2, bsz=256, num_updates=51900, lr=0.000155193, gnorm=1.601, clip=0, loss_scale=4096, train_wall=88, wall=47687
2022-07-12 06:52:40 | INFO | train_inner | epoch 047:    464 / 1122 loss=5.041, nll_loss=1.938, mask_ins=0.75, word_ins_ml=3.601, word_reposition=0.691, ppl=32.93, wps=22905.2, ups=1.12, wpb=20451.4, bsz=256, num_updates=52000, lr=0.000155043, gnorm=1.597, clip=0, loss_scale=4915, train_wall=89, wall=47777
2022-07-12 06:54:10 | INFO | train_inner | epoch 047:    564 / 1122 loss=5.082, nll_loss=1.984, mask_ins=0.75, word_ins_ml=3.642, word_reposition=0.691, ppl=33.88, wps=22886.8, ups=1.11, wpb=20596.2, bsz=256, num_updates=52100, lr=0.000154895, gnorm=1.604, clip=0, loss_scale=8192, train_wall=89, wall=47867
2022-07-12 06:55:39 | INFO | train_inner | epoch 047:    664 / 1122 loss=5.027, nll_loss=1.947, mask_ins=0.735, word_ins_ml=3.608, word_reposition=0.683, ppl=32.6, wps=23037.5, ups=1.12, wpb=20638.9, bsz=256, num_updates=52200, lr=0.000154746, gnorm=1.636, clip=0, loss_scale=8192, train_wall=89, wall=47956
2022-07-12 06:55:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 06:57:12 | INFO | train_inner | epoch 047:    765 / 1122 loss=5.054, nll_loss=1.95, mask_ins=0.75, word_ins_ml=3.612, word_reposition=0.692, ppl=33.21, wps=22157, ups=1.08, wpb=20477.6, bsz=256, num_updates=52300, lr=0.000154598, gnorm=1.608, clip=0, loss_scale=4380, train_wall=92, wall=48049
2022-07-12 06:58:41 | INFO | train_inner | epoch 047:    865 / 1122 loss=5.051, nll_loss=1.948, mask_ins=0.752, word_ins_ml=3.609, word_reposition=0.69, ppl=33.14, wps=22861.3, ups=1.11, wpb=20516.4, bsz=256, num_updates=52400, lr=0.000154451, gnorm=1.626, clip=0, loss_scale=4096, train_wall=89, wall=48139
2022-07-12 07:00:11 | INFO | train_inner | epoch 047:    965 / 1122 loss=5.038, nll_loss=1.935, mask_ins=0.747, word_ins_ml=3.598, word_reposition=0.693, ppl=32.85, wps=23058.6, ups=1.11, wpb=20710.4, bsz=256, num_updates=52500, lr=0.000154303, gnorm=1.591, clip=0, loss_scale=4096, train_wall=89, wall=48228
2022-07-12 07:01:41 | INFO | train_inner | epoch 047:   1065 / 1122 loss=5.056, nll_loss=1.955, mask_ins=0.744, word_ins_ml=3.615, word_reposition=0.696, ppl=33.27, wps=22925.1, ups=1.12, wpb=20506.9, bsz=256, num_updates=52600, lr=0.000154157, gnorm=1.624, clip=0, loss_scale=4096, train_wall=89, wall=48318
2022-07-12 07:02:31 | INFO | train | epoch 047 | loss 5.047 | nll_loss 1.949 | mask_ins 0.746 | word_ins_ml 3.611 | word_reposition 0.69 | ppl 33.07 | wps 22410.6 | ups 1.09 | wpb 20519.2 | bsz 255.8 | num_updates 52657 | lr 0.000154073 | gnorm 1.618 | clip 0 | loss_scale 4925 | train_wall 999 | wall 48368
2022-07-12 07:02:49 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.844 | nll_loss 5.65 | mask_ins 1.47 | word_ins_ml 7.098 | word_reposition 1.277 | ppl 919.17 | wps 55591.1 | wpb 2367.6 | bsz 32 | num_updates 52657 | best_loss 9.844
2022-07-12 07:02:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_best.pt (epoch 47 @ 52657 updates, score 9.844) (writing took 4.985073254443705 seconds)
2022-07-12 07:03:33 | INFO | train_inner | epoch 048:     43 / 1122 loss=5.053, nll_loss=1.962, mask_ins=0.746, word_ins_ml=3.622, word_reposition=0.685, ppl=33.2, wps=18123.9, ups=0.89, wpb=20310.5, bsz=253.8, num_updates=52700, lr=0.00015401, gnorm=1.7, clip=0, loss_scale=4096, train_wall=88, wall=48430
2022-07-12 07:05:02 | INFO | train_inner | epoch 048:    143 / 1122 loss=4.987, nll_loss=1.905, mask_ins=0.732, word_ins_ml=3.571, word_reposition=0.684, ppl=31.71, wps=22973.9, ups=1.12, wpb=20521.5, bsz=256, num_updates=52800, lr=0.000153864, gnorm=1.611, clip=0, loss_scale=7455, train_wall=89, wall=48519
2022-07-12 07:06:32 | INFO | train_inner | epoch 048:    243 / 1122 loss=5.03, nll_loss=1.928, mask_ins=0.75, word_ins_ml=3.592, word_reposition=0.688, ppl=32.66, wps=22920.6, ups=1.11, wpb=20642.5, bsz=256, num_updates=52900, lr=0.000153719, gnorm=1.653, clip=0, loss_scale=8192, train_wall=89, wall=48609
2022-07-12 07:08:02 | INFO | train_inner | epoch 048:    343 / 1122 loss=5.051, nll_loss=1.954, mask_ins=0.747, word_ins_ml=3.614, word_reposition=0.69, ppl=33.15, wps=22894.4, ups=1.12, wpb=20522.9, bsz=256, num_updates=53000, lr=0.000153574, gnorm=1.648, clip=0, loss_scale=8192, train_wall=89, wall=48699
2022-07-12 07:09:32 | INFO | train_inner | epoch 048:    443 / 1122 loss=5.024, nll_loss=1.944, mask_ins=0.731, word_ins_ml=3.606, word_reposition=0.686, ppl=32.53, wps=23029.4, ups=1.11, wpb=20732.4, bsz=256, num_updates=53100, lr=0.000153429, gnorm=1.612, clip=0, loss_scale=8192, train_wall=89, wall=48789
2022-07-12 07:11:01 | INFO | train_inner | epoch 048:    543 / 1122 loss=5.012, nll_loss=1.922, mask_ins=0.741, word_ins_ml=3.587, word_reposition=0.684, ppl=32.26, wps=22828.3, ups=1.12, wpb=20453.1, bsz=256, num_updates=53200, lr=0.000153285, gnorm=1.613, clip=0, loss_scale=8192, train_wall=89, wall=48878
2022-07-12 07:12:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 07:12:32 | INFO | train_inner | epoch 048:    644 / 1122 loss=5.05, nll_loss=1.959, mask_ins=0.748, word_ins_ml=3.619, word_reposition=0.682, ppl=33.12, wps=22496.8, ups=1.1, wpb=20418.9, bsz=256, num_updates=53300, lr=0.000153141, gnorm=1.666, clip=0, loss_scale=13464, train_wall=90, wall=48969
2022-07-12 07:14:02 | INFO | train_inner | epoch 048:    744 / 1122 loss=5.018, nll_loss=1.928, mask_ins=0.741, word_ins_ml=3.592, word_reposition=0.685, ppl=32.41, wps=22612, ups=1.11, wpb=20433.6, bsz=256, num_updates=53400, lr=0.000152998, gnorm=1.619, clip=0, loss_scale=8192, train_wall=90, wall=49060
2022-07-12 07:15:33 | INFO | train_inner | epoch 048:    844 / 1122 loss=5.035, nll_loss=1.949, mask_ins=0.741, word_ins_ml=3.61, word_reposition=0.684, ppl=32.78, wps=22848.6, ups=1.11, wpb=20606.5, bsz=256, num_updates=53500, lr=0.000152854, gnorm=1.644, clip=0, loss_scale=8192, train_wall=90, wall=49150
2022-07-12 07:17:02 | INFO | train_inner | epoch 048:    944 / 1122 loss=5.023, nll_loss=1.936, mask_ins=0.741, word_ins_ml=3.599, word_reposition=0.684, ppl=32.52, wps=23019.9, ups=1.12, wpb=20549, bsz=256, num_updates=53600, lr=0.000152712, gnorm=1.644, clip=0, loss_scale=8192, train_wall=89, wall=49239
2022-07-12 07:18:31 | INFO | train_inner | epoch 048:   1044 / 1122 loss=5.009, nll_loss=1.913, mask_ins=0.744, word_ins_ml=3.577, word_reposition=0.687, ppl=32.2, wps=23050.5, ups=1.12, wpb=20544.5, bsz=256, num_updates=53700, lr=0.00015257, gnorm=1.608, clip=0, loss_scale=8192, train_wall=89, wall=49328
2022-07-12 07:19:37 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 07:19:40 | INFO | train | epoch 048 | loss 5.025 | nll_loss 1.935 | mask_ins 0.742 | word_ins_ml 3.597 | word_reposition 0.686 | ppl 32.55 | wps 22331.9 | ups 1.09 | wpb 20520.2 | bsz 255.8 | num_updates 53777 | lr 0.00015246 | gnorm 1.633 | clip 0 | loss_scale 8426 | train_wall 999 | wall 49397
2022-07-12 07:19:58 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.993 | nll_loss 5.725 | mask_ins 1.479 | word_ins_ml 7.173 | word_reposition 1.341 | ppl 1018.99 | wps 55597.6 | wpb 2367.6 | bsz 32 | num_updates 53777 | best_loss 9.844
2022-07-12 07:20:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 48 @ 53777 updates, score 9.993) (writing took 2.9731921767815948 seconds)
2022-07-12 07:20:22 | INFO | train_inner | epoch 049:     23 / 1122 loss=5.03, nll_loss=1.937, mask_ins=0.744, word_ins_ml=3.599, word_reposition=0.687, ppl=32.68, wps=18417.3, ups=0.9, wpb=20359.2, bsz=253.8, num_updates=53800, lr=0.000152428, gnorm=1.634, clip=0, loss_scale=7056, train_wall=89, wall=49439
2022-07-12 07:21:51 | INFO | train_inner | epoch 049:    123 / 1122 loss=5.053, nll_loss=1.956, mask_ins=0.747, word_ins_ml=3.617, word_reposition=0.689, ppl=33.2, wps=22954.8, ups=1.12, wpb=20444.6, bsz=256, num_updates=53900, lr=0.000152286, gnorm=1.654, clip=0, loss_scale=4096, train_wall=88, wall=49528
2022-07-12 07:23:20 | INFO | train_inner | epoch 049:    223 / 1122 loss=4.987, nll_loss=1.898, mask_ins=0.734, word_ins_ml=3.565, word_reposition=0.689, ppl=31.72, wps=23166, ups=1.12, wpb=20628, bsz=256, num_updates=54000, lr=0.000152145, gnorm=1.598, clip=0, loss_scale=4096, train_wall=88, wall=49617
2022-07-12 07:24:49 | INFO | train_inner | epoch 049:    323 / 1122 loss=5.036, nll_loss=1.945, mask_ins=0.745, word_ins_ml=3.607, word_reposition=0.685, ppl=32.81, wps=23121.2, ups=1.12, wpb=20631.9, bsz=256, num_updates=54100, lr=0.000152004, gnorm=1.626, clip=0, loss_scale=4096, train_wall=89, wall=49706
2022-07-12 07:26:18 | INFO | train_inner | epoch 049:    423 / 1122 loss=5.024, nll_loss=1.941, mask_ins=0.74, word_ins_ml=3.603, word_reposition=0.681, ppl=32.54, wps=23091.1, ups=1.12, wpb=20546.2, bsz=256, num_updates=54200, lr=0.000151864, gnorm=1.6, clip=0, loss_scale=4096, train_wall=88, wall=49795
2022-07-12 07:27:47 | INFO | train_inner | epoch 049:    523 / 1122 loss=5.048, nll_loss=1.963, mask_ins=0.74, word_ins_ml=3.622, word_reposition=0.687, ppl=33.09, wps=23030.3, ups=1.12, wpb=20486.9, bsz=256, num_updates=54300, lr=0.000151724, gnorm=1.626, clip=0, loss_scale=4751, train_wall=88, wall=49884
2022-07-12 07:29:16 | INFO | train_inner | epoch 049:    623 / 1122 loss=5.04, nll_loss=1.951, mask_ins=0.738, word_ins_ml=3.612, word_reposition=0.691, ppl=32.9, wps=23091.9, ups=1.12, wpb=20573.8, bsz=256, num_updates=54400, lr=0.000151585, gnorm=1.652, clip=0, loss_scale=8192, train_wall=88, wall=49973
2022-07-12 07:30:46 | INFO | train_inner | epoch 049:    723 / 1122 loss=5.016, nll_loss=1.924, mask_ins=0.739, word_ins_ml=3.588, word_reposition=0.689, ppl=32.36, wps=22830.6, ups=1.11, wpb=20554.2, bsz=256, num_updates=54500, lr=0.000151446, gnorm=1.617, clip=0, loss_scale=8192, train_wall=89, wall=50063
2022-07-12 07:32:16 | INFO | train_inner | epoch 049:    823 / 1122 loss=5.047, nll_loss=1.943, mask_ins=0.748, word_ins_ml=3.605, word_reposition=0.694, ppl=33.07, wps=22882.7, ups=1.11, wpb=20530.2, bsz=256, num_updates=54600, lr=0.000151307, gnorm=1.609, clip=0, loss_scale=8192, train_wall=89, wall=50153
2022-07-12 07:33:45 | INFO | train_inner | epoch 049:    923 / 1122 loss=4.972, nll_loss=1.894, mask_ins=0.728, word_ins_ml=3.56, word_reposition=0.684, ppl=31.39, wps=22956.4, ups=1.12, wpb=20483.2, bsz=256, num_updates=54700, lr=0.000151169, gnorm=1.599, clip=0, loss_scale=8192, train_wall=89, wall=50242
2022-07-12 07:35:14 | INFO | train_inner | epoch 049:   1023 / 1122 loss=5.001, nll_loss=1.918, mask_ins=0.735, word_ins_ml=3.582, word_reposition=0.684, ppl=32.02, wps=22939.4, ups=1.12, wpb=20453.7, bsz=256, num_updates=54800, lr=0.000151031, gnorm=1.661, clip=0, loss_scale=8520, train_wall=89, wall=50331
2022-07-12 07:35:40 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 07:36:42 | INFO | train | epoch 049 | loss 5.022 | nll_loss 1.933 | mask_ins 0.74 | word_ins_ml 3.595 | word_reposition 0.687 | ppl 32.49 | wps 22513 | ups 1.1 | wpb 20519.8 | bsz 255.8 | num_updates 54898 | lr 0.000150896 | gnorm 1.627 | clip 0 | loss_scale 6575 | train_wall 994 | wall 50419
2022-07-12 07:37:00 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.877 | nll_loss 5.639 | mask_ins 1.489 | word_ins_ml 7.085 | word_reposition 1.304 | ppl 940.21 | wps 55748.8 | wpb 2367.6 | bsz 32 | num_updates 54898 | best_loss 9.844
2022-07-12 07:37:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 49 @ 54898 updates, score 9.877) (writing took 2.886604360304773 seconds)
2022-07-12 07:37:05 | INFO | train_inner | epoch 050:      2 / 1122 loss=5.022, nll_loss=1.935, mask_ins=0.739, word_ins_ml=3.597, word_reposition=0.686, ppl=32.5, wps=18395.2, ups=0.91, wpb=20314.7, bsz=253.8, num_updates=54900, lr=0.000150893, gnorm=1.66, clip=0, loss_scale=10463, train_wall=89, wall=50442
2022-07-12 07:38:35 | INFO | train_inner | epoch 050:    102 / 1122 loss=5.015, nll_loss=1.93, mask_ins=0.733, word_ins_ml=3.593, word_reposition=0.689, ppl=32.35, wps=22658.2, ups=1.1, wpb=20528.1, bsz=256, num_updates=55000, lr=0.000150756, gnorm=1.636, clip=0, loss_scale=8192, train_wall=90, wall=50532
2022-07-12 07:40:06 | INFO | train_inner | epoch 050:    202 / 1122 loss=4.993, nll_loss=1.903, mask_ins=0.74, word_ins_ml=3.569, word_reposition=0.684, ppl=31.84, wps=22636.7, ups=1.1, wpb=20502.1, bsz=256, num_updates=55100, lr=0.000150619, gnorm=1.642, clip=0, loss_scale=8192, train_wall=90, wall=50623
2022-07-12 07:41:36 | INFO | train_inner | epoch 050:    302 / 1122 loss=4.995, nll_loss=1.912, mask_ins=0.737, word_ins_ml=3.577, word_reposition=0.682, ppl=31.9, wps=22665.7, ups=1.1, wpb=20524.6, bsz=256, num_updates=55200, lr=0.000150482, gnorm=1.615, clip=0, loss_scale=8192, train_wall=90, wall=50713
2022-07-12 07:43:07 | INFO | train_inner | epoch 050:    402 / 1122 loss=4.996, nll_loss=1.908, mask_ins=0.737, word_ins_ml=3.573, word_reposition=0.686, ppl=31.92, wps=22798, ups=1.11, wpb=20567.2, bsz=256, num_updates=55300, lr=0.000150346, gnorm=1.584, clip=0, loss_scale=8192, train_wall=90, wall=50804
2022-07-12 07:44:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-12 07:44:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-12 07:44:38 | INFO | train_inner | epoch 050:    504 / 1122 loss=5.034, nll_loss=1.934, mask_ins=0.746, word_ins_ml=3.596, word_reposition=0.691, ppl=32.75, wps=22625.8, ups=1.1, wpb=20613.5, bsz=256, num_updates=55400, lr=0.00015021, gnorm=1.608, clip=0, loss_scale=8594, train_wall=90, wall=50895
2022-07-12 07:45:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-12 07:46:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-12 07:46:08 | INFO | train_inner | epoch 050:    606 / 1122 loss=5.023, nll_loss=1.92, mask_ins=0.75, word_ins_ml=3.584, word_reposition=0.688, ppl=32.52, wps=22510.4, ups=1.1, wpb=20449.5, bsz=256, num_updates=55500, lr=0.000150075, gnorm=1.66, clip=0, loss_scale=3805, train_wall=90, wall=50986
2022-07-12 07:46:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-12 07:47:39 | INFO | train_inner | epoch 050:    707 / 1122 loss=5.005, nll_loss=1.919, mask_ins=0.737, word_ins_ml=3.583, word_reposition=0.685, ppl=32.11, wps=22669.3, ups=1.1, wpb=20563.2, bsz=256, num_updates=55600, lr=0.00014994, gnorm=2.507, clip=0, loss_scale=608, train_wall=90, wall=51076
2022-07-12 07:49:09 | INFO | train_inner | epoch 050:    807 / 1122 loss=5.032, nll_loss=1.943, mask_ins=0.736, word_ins_ml=3.604, word_reposition=0.692, ppl=32.72, wps=22986.4, ups=1.12, wpb=20574.9, bsz=256, num_updates=55700, lr=0.000149805, gnorm=1.603, clip=0, loss_scale=512, train_wall=89, wall=51166
2022-07-12 07:50:38 | INFO | train_inner | epoch 050:    907 / 1122 loss=5.017, nll_loss=1.934, mask_ins=0.737, word_ins_ml=3.597, word_reposition=0.684, ppl=32.38, wps=22936.7, ups=1.12, wpb=20470.3, bsz=256, num_updates=55800, lr=0.000149671, gnorm=1.626, clip=0, loss_scale=512, train_wall=89, wall=51255
2022-07-12 07:52:07 | INFO | train_inner | epoch 050:   1007 / 1122 loss=4.961, nll_loss=1.88, mask_ins=0.734, word_ins_ml=3.548, word_reposition=0.679, ppl=31.15, wps=23152.6, ups=1.12, wpb=20607.5, bsz=256, num_updates=55900, lr=0.000149537, gnorm=1.619, clip=0, loss_scale=512, train_wall=88, wall=51344
2022-07-12 07:53:36 | INFO | train_inner | epoch 050:   1107 / 1122 loss=5.002, nll_loss=1.923, mask_ins=0.736, word_ins_ml=3.586, word_reposition=0.68, ppl=32.05, wps=23042.3, ups=1.12, wpb=20530.9, bsz=256, num_updates=56000, lr=0.000149404, gnorm=1.645, clip=0, loss_scale=512, train_wall=88, wall=51433
2022-07-12 07:53:49 | INFO | train | epoch 050 | loss 5.008 | nll_loss 1.92 | mask_ins 0.739 | word_ins_ml 3.584 | word_reposition 0.686 | ppl 32.19 | wps 22318.1 | ups 1.09 | wpb 20520.6 | bsz 255.8 | num_updates 56015 | lr 0.000149384 | gnorm 1.707 | clip 0 | loss_scale 4306 | train_wall 999 | wall 51446
2022-07-12 07:54:07 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.886 | nll_loss 5.674 | mask_ins 1.497 | word_ins_ml 7.126 | word_reposition 1.263 | ppl 945.88 | wps 55209.5 | wpb 2367.6 | bsz 32 | num_updates 56015 | best_loss 9.844
2022-07-12 07:54:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_transformer_transformer_cased/checkpoint_last.pt (epoch 50 @ 56015 updates, score 9.886) (writing took 2.8734265035018325 seconds)
2022-07-12 07:55:27 | INFO | train_inner | epoch 051:     85 / 1122 loss=5.066, nll_loss=1.966, mask_ins=0.746, word_ins_ml=3.625, word_reposition=0.695, ppl=33.49, wps=18431.9, ups=0.9, wpb=20385.5, bsz=253.8, num_updates=56100, lr=0.00014927, gnorm=1.637, clip=0, loss_scale=870, train_wall=89, wall=51544
2022-07-12 07:56:56 | INFO | train_inner | epoch 051:    185 / 1122 loss=5.001, nll_loss=1.929, mask_ins=0.728, word_ins_ml=3.592, word_reposition=0.68, ppl=32.03, wps=22929.3, ups=1.12, wpb=20508, bsz=256, num_updates=56200, lr=0.000149137, gnorm=1.595, clip=0, loss_scale=1024, train_wall=89, wall=51633
2022-07-12 07:58:25 | INFO | train_inner | epoch 051:    285 / 1122 loss=5.006, nll_loss=1.917, mask_ins=0.737, word_ins_ml=3.581, word_reposition=0.688, ppl=32.13, wps=23143.6, ups=1.12, wpb=20630.6, bsz=256, num_updates=56300, lr=0.000149005, gnorm=1.625, clip=0, loss_scale=1024, train_wall=89, wall=51722
2022-07-12 07:59:55 | INFO | train_inner | epoch 051:    385 / 1122 loss=4.986, nll_loss=1.91, mask_ins=0.728, word_ins_ml=3.575, word_reposition=0.682, ppl=31.69, wps=23040.7, ups=1.12, wpb=20628.1, bsz=256, num_updates=56400, lr=0.000148873, gnorm=1.625, clip=0, loss_scale=1024, train_wall=89, wall=51812
2022-07-12 08:01:25 | INFO | train_inner | epoch 051:    485 / 1122 loss=4.998, nll_loss=1.92, mask_ins=0.73, word_ins_ml=3.584, word_reposition=0.684, ppl=31.95, wps=22817.6, ups=1.11, wpb=20597.2, bsz=256, num_updates=56500, lr=0.000148741, gnorm=1.572, clip=0, loss_scale=1024, train_wall=90, wall=51902
2022-07-12 08:02:55 | INFO | train_inner | epoch 051:    585 / 1122 loss=5.004, nll_loss=1.914, mask_ins=0.738, word_ins_ml=3.578, word_reposition=0.688, ppl=32.09, wps=22864.8, ups=1.11, wpb=20570.6, bsz=256, num_updates=56600, lr=0.00014861, gnorm=1.584, clip=0, loss_scale=1618, train_wall=89, wall=51992
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
