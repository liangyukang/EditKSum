nohup: ignoring input
2022-07-07 15:21:47 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:14268
2022-07-07 15:21:47 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:14268
2022-07-07 15:21:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-07 15:21:47 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:14268
2022-07-07 15:21:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-07 15:21:47 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:14268
2022-07-07 15:21:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-07 15:21:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-07 15:21:47 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-07 15:21:47 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-07 15:21:47 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-07 15:21:47 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-07 15:21:47 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-07 15:21:47 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-07 15:21:47 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-07 15:21:47 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-07 15:21:51 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:14268', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='12,12,12', layers_num='12,12,12', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=True, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_cased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-cased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, no_share_maskpredictor=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-cased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-cased-decoder', share_decoder_input_output_embed=False, encoder='bert_adaptor', decoder='bert_adaptor', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-cased')
2022-07-07 15:21:51 | INFO | fairseq.tasks.translation | [source] dictionary: 28996 types
2022-07-07 15:21:51 | INFO | fairseq.tasks.translation | [target] dictionary: 28996 types
2022-07-07 15:21:51 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.source
2022-07-07 15:21:51 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/valid.source-target.target
2022-07-07 15:21:51 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 valid source-target 13368 examples
start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]377it [00:00, 3762.84it/s]393it [00:00, 3923.34it/s]382it [00:00, 3813.79it/s]371it [00:00, 3703.30it/s]754it [00:00, 3274.76it/s]786it [00:00, 3415.28it/s]764it [00:00, 3317.57it/s]742it [00:00, 3224.76it/s]1100it [00:00, 3349.49it/s]1149it [00:00, 3501.72it/s]1122it [00:00, 3428.03it/s]1079it [00:00, 3284.27it/s]1503it [00:00, 3265.49it/s]1438it [00:00, 3090.49it/s]1468it [00:00, 3188.02it/s]1411it [00:00, 3050.65it/s]1885it [00:00, 3449.53it/s]1850it [00:00, 3397.37it/s]1815it [00:00, 3313.60it/s]1777it [00:00, 3250.99it/s]2176it [00:00, 3408.16it/s]2232it [00:00, 3530.97it/s]2158it [00:00, 3427.80it/s]2234it [00:00, 3303.31it/s]2628it [00:00, 3496.10it/s]2589it [00:00, 3360.70it/s]2521it [00:00, 3225.02it/s]2505it [00:00, 3250.67it/s]3017it [00:00, 3614.45it/s]2980it [00:00, 3523.23it/s]2900it [00:00, 3392.30it/s]2890it [00:00, 3427.61it/s]3382it [00:00, 3434.29it/s]3336it [00:00, 3360.30it/s]3243it [00:00, 3244.12it/s]3237it [00:00, 3275.81it/s]3764it [00:01, 3546.09it/s]3714it [00:01, 3480.25it/s]3625it [00:01, 3408.44it/s]3617it [00:01, 3426.90it/s]4122it [00:01, 3389.60it/s]3970it [00:01, 3257.83it/s]4066it [00:01, 3314.26it/s]3964it [00:01, 3277.15it/s]4499it [00:01, 3497.62it/s]4347it [00:01, 3402.74it/s]4443it [00:01, 3442.27it/s]4334it [00:01, 3394.46it/s]4718it [00:01, 3491.38it/s]4704it [00:01, 3480.23it/s]4852it [00:01, 3339.56it/s]4791it [00:01, 3269.67it/s]5221it [00:01, 3437.76it/s]5070it [00:01, 3317.38it/s]5161it [00:01, 3389.92it/s]5055it [00:01, 3271.74it/s]5438it [00:01, 3418.58it/s]5524it [00:01, 3457.25it/s]5424it [00:01, 3388.39it/s]5578it [00:01, 3268.10it/s]5922it [00:01, 3313.47it/s]5783it [00:01, 3217.63it/s]5873it [00:01, 3230.21it/s]5767it [00:01, 3198.47it/s]6273it [00:01, 3368.77it/s]6129it [00:01, 3285.09it/s]6216it [00:01, 3285.81it/s]6107it [00:01, 3252.34it/s]6461it [00:01, 3096.63it/s]6436it [00:01, 3072.81it/s]6613it [00:02, 1741.03it/s]6549it [00:02, 1629.55it/s]6953it [00:02, 2031.77it/s]6775it [00:02, 1602.74it/s]6888it [00:02, 1923.99it/s]6747it [00:02, 1584.59it/s]7269it [00:02, 2257.46it/s]7073it [00:02, 1835.23it/s]7183it [00:02, 2120.90it/s]7056it [00:02, 1838.15it/s]7564it [00:02, 2287.68it/s]7467it [00:02, 2167.85it/s]7344it [00:02, 1909.16it/s]7344it [00:02, 1941.95it/s]7871it [00:02, 2467.34it/s]7772it [00:02, 2366.53it/s]7633it [00:02, 2113.99it/s]7653it [00:02, 2180.93it/s]8157it [00:02, 2554.09it/s]8076it [00:02, 2529.92it/s]7939it [00:02, 2330.92it/s]7975it [00:02, 2419.34it/s]8442it [00:02, 2534.06it/s]8363it [00:02, 2523.06it/s]8212it [00:02, 2343.57it/s]8259it [00:02, 2466.08it/s]8788it [00:03, 2777.63it/s]8696it [00:03, 2735.01it/s]8555it [00:03, 2619.47it/s]8601it [00:03, 2710.00it/s]9084it [00:03, 2718.64it/s]8902it [00:03, 2844.57it/s]8948it [00:03, 2912.90it/s]9024it [00:03, 2708.80it/s]9430it [00:03, 2919.81it/s]9207it [00:03, 2779.74it/s]9260it [00:03, 2807.36it/s]9360it [00:03, 2882.43it/s]9768it [00:03, 3047.91it/s]9552it [00:03, 2962.35it/s]9604it [00:03, 2979.47it/s]9705it [00:03, 3037.42it/s]10081it [00:03, 2926.36it/s]9864it [00:03, 2857.94it/s]9914it [00:03, 2870.99it/s]10019it [00:03, 2907.22it/s]10424it [00:03, 3066.92it/s]10211it [00:03, 3026.34it/s]10234it [00:03, 2961.22it/s]10361it [00:03, 3049.11it/s]10737it [00:03, 2922.95it/s]10553it [00:03, 3137.13it/s]10575it [00:03, 3086.93it/s]10704it [00:03, 2940.20it/s]11078it [00:03, 3057.60it/s]10873it [00:03, 2964.35it/s]10890it [00:03, 2936.02it/s]11040it [00:03, 3052.91it/s]11417it [00:03, 3151.53it/s]11215it [00:03, 3089.12it/s]11209it [00:03, 3005.15it/s]11382it [00:03, 3153.65it/s]11736it [00:03, 3002.93it/s]11544it [00:04, 2942.93it/s]11702it [00:04, 2980.02it/s]11544it [00:04, 2851.02it/s]12080it [00:04, 3123.74it/s]11882it [00:04, 3062.40it/s]12045it [00:04, 3104.34it/s]11889it [00:04, 3013.33it/s]12396it [00:04, 2981.45it/s]12227it [00:04, 3171.07it/s]12234it [00:04, 3134.63it/s]12384it [00:04, 2965.74it/s]12741it [00:04, 3112.17it/s]12548it [00:04, 2979.91it/s]12552it [00:04, 2942.46it/s]12730it [00:04, 3101.04it/s]13083it [00:04, 3197.70it/s]12890it [00:04, 3100.42it/s]12892it [00:04, 3068.76it/s]13069it [00:04, 3181.80it/s]13368it [00:04, 2970.77it/s]
2022-07-07 15:21:55 | INFO | root | success load 13368 data
2022-07-07 15:21:55 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-07 15:21:55 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-07 15:21:55 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-07 15:21:55 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-07 15:21:55 | INFO | transformer.tokenization_utils | loading file None
2022-07-07 15:21:55 | INFO | transformer.tokenization_utils | loading file None
2022-07-07 15:21:55 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
13222it [00:04, 3133.53it/s]13368it [00:04, 2920.82it/s]
13224it [00:04, 2933.96it/s]13368it [00:04, 2891.85it/s]
13368it [00:04, 2886.82it/s]
2022-07-07 15:21:56 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-07 15:21:56 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-07 15:21:56 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased/pytorch_model.bin
2022-07-07 15:22:00 | INFO | transformer.modeling_utils | Weights of BertEncoderWithAdaptor not initialized from pretrained model: ['bert.encoder.layer.0.adapter_ln.weight', 'bert.encoder.layer.0.adapter_ln.bias', 'bert.encoder.layer.0.adapter_w1.weight', 'bert.encoder.layer.0.adapter_w2.weight', 'bert.encoder.layer.1.adapter_ln.weight', 'bert.encoder.layer.1.adapter_ln.bias', 'bert.encoder.layer.1.adapter_w1.weight', 'bert.encoder.layer.1.adapter_w2.weight', 'bert.encoder.layer.2.adapter_ln.weight', 'bert.encoder.layer.2.adapter_ln.bias', 'bert.encoder.layer.2.adapter_w1.weight', 'bert.encoder.layer.2.adapter_w2.weight', 'bert.encoder.layer.3.adapter_ln.weight', 'bert.encoder.layer.3.adapter_ln.bias', 'bert.encoder.layer.3.adapter_w1.weight', 'bert.encoder.layer.3.adapter_w2.weight', 'bert.encoder.layer.4.adapter_ln.weight', 'bert.encoder.layer.4.adapter_ln.bias', 'bert.encoder.layer.4.adapter_w1.weight', 'bert.encoder.layer.4.adapter_w2.weight', 'bert.encoder.layer.5.adapter_ln.weight', 'bert.encoder.layer.5.adapter_ln.bias', 'bert.encoder.layer.5.adapter_w1.weight', 'bert.encoder.layer.5.adapter_w2.weight', 'bert.encoder.layer.6.adapter_ln.weight', 'bert.encoder.layer.6.adapter_ln.bias', 'bert.encoder.layer.6.adapter_w1.weight', 'bert.encoder.layer.6.adapter_w2.weight', 'bert.encoder.layer.7.adapter_ln.weight', 'bert.encoder.layer.7.adapter_ln.bias', 'bert.encoder.layer.7.adapter_w1.weight', 'bert.encoder.layer.7.adapter_w2.weight', 'bert.encoder.layer.8.adapter_ln.weight', 'bert.encoder.layer.8.adapter_ln.bias', 'bert.encoder.layer.8.adapter_w1.weight', 'bert.encoder.layer.8.adapter_w2.weight', 'bert.encoder.layer.9.adapter_ln.weight', 'bert.encoder.layer.9.adapter_ln.bias', 'bert.encoder.layer.9.adapter_w1.weight', 'bert.encoder.layer.9.adapter_w2.weight', 'bert.encoder.layer.10.adapter_ln.weight', 'bert.encoder.layer.10.adapter_ln.bias', 'bert.encoder.layer.10.adapter_w1.weight', 'bert.encoder.layer.10.adapter_w2.weight', 'bert.encoder.layer.11.adapter_ln.weight', 'bert.encoder.layer.11.adapter_ln.bias', 'bert.encoder.layer.11.adapter_w1.weight', 'bert.encoder.layer.11.adapter_w2.weight', 'kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-07 15:22:00 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoderWithAdaptor: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2022-07-07 15:22:00 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-cased/config.json
2022-07-07 15:22:00 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 28996
}

2022-07-07 15:22:00 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-cased-decoder/pytorch_model.bin
Trained parameters: len 319
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-07 15:22:03 | INFO | transformer.modeling_utils | Weights of BertDecoderWithAdaptor not initialized from pretrained model: ['embed_mask_ins.weight', 'layers.0.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.0.encoder_attn.out_proj.bias', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.bias', 'layers.0.adapter.encoder_attn_fc1.weight', 'layers.0.adapter.encoder_attn_fc2.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.weight', 'layers.0.adapter.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_msk.encoder_attn_fc1.weight', 'layers.0.adapter_msk.encoder_attn_fc2.weight', 'layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.0.adapter_reposition.encoder_attn_fc1.weight', 'layers.0.adapter_reposition.encoder_attn_fc2.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.1.encoder_attn.k_proj.weight', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.encoder_attn.v_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.encoder_attn.out_proj.bias', 'layers.1.encoder_attn_layer_norm.weight', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.adapter.encoder_attn_fc1.weight', 'layers.1.adapter.encoder_attn_fc2.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.weight', 'layers.1.adapter.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_msk.encoder_attn_fc1.weight', 'layers.1.adapter_msk.encoder_attn_fc2.weight', 'layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.1.adapter_reposition.encoder_attn_fc1.weight', 'layers.1.adapter_reposition.encoder_attn_fc2.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.2.encoder_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.weight', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.adapter.encoder_attn_fc1.weight', 'layers.2.adapter.encoder_attn_fc2.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.weight', 'layers.2.adapter.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_msk.encoder_attn_fc1.weight', 'layers.2.adapter_msk.encoder_attn_fc2.weight', 'layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.2.adapter_reposition.encoder_attn_fc1.weight', 'layers.2.adapter_reposition.encoder_attn_fc2.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.3.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.3.encoder_attn.out_proj.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.3.encoder_attn_layer_norm.bias', 'layers.3.adapter.encoder_attn_fc1.weight', 'layers.3.adapter.encoder_attn_fc2.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.weight', 'layers.3.adapter.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_msk.encoder_attn_fc1.weight', 'layers.3.adapter_msk.encoder_attn_fc2.weight', 'layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.3.adapter_reposition.encoder_attn_fc1.weight', 'layers.3.adapter_reposition.encoder_attn_fc2.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.4.encoder_attn.q_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.4.adapter.encoder_attn_fc1.weight', 'layers.4.adapter.encoder_attn_fc2.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.weight', 'layers.4.adapter.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_msk.encoder_attn_fc1.weight', 'layers.4.adapter_msk.encoder_attn_fc2.weight', 'layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.4.adapter_reposition.encoder_attn_fc1.weight', 'layers.4.adapter_reposition.encoder_attn_fc2.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.5.encoder_attn.v_proj.bias', 'layers.5.encoder_attn.q_proj.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn_layer_norm.bias', 'layers.5.adapter.encoder_attn_fc1.weight', 'layers.5.adapter.encoder_attn_fc2.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.weight', 'layers.5.adapter.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_msk.encoder_attn_fc1.weight', 'layers.5.adapter_msk.encoder_attn_fc2.weight', 'layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.5.adapter_reposition.encoder_attn_fc1.weight', 'layers.5.adapter_reposition.encoder_attn_fc2.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.6.encoder_attn.k_proj.weight', 'layers.6.encoder_attn.k_proj.bias', 'layers.6.encoder_attn.v_proj.weight', 'layers.6.encoder_attn.v_proj.bias', 'layers.6.encoder_attn.q_proj.weight', 'layers.6.encoder_attn.q_proj.bias', 'layers.6.encoder_attn.out_proj.weight', 'layers.6.encoder_attn.out_proj.bias', 'layers.6.encoder_attn_layer_norm.weight', 'layers.6.encoder_attn_layer_norm.bias', 'layers.6.adapter.encoder_attn_fc1.weight', 'layers.6.adapter.encoder_attn_fc2.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.weight', 'layers.6.adapter.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_msk.encoder_attn_fc1.weight', 'layers.6.adapter_msk.encoder_attn_fc2.weight', 'layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.6.adapter_reposition.encoder_attn_fc1.weight', 'layers.6.adapter_reposition.encoder_attn_fc2.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.7.encoder_attn.k_proj.weight', 'layers.7.encoder_attn.k_proj.bias', 'layers.7.encoder_attn.v_proj.weight', 'layers.7.encoder_attn.v_proj.bias', 'layers.7.encoder_attn.q_proj.weight', 'layers.7.encoder_attn.q_proj.bias', 'layers.7.encoder_attn.out_proj.weight', 'layers.7.encoder_attn.out_proj.bias', 'layers.7.encoder_attn_layer_norm.weight', 'layers.7.encoder_attn_layer_norm.bias', 'layers.7.adapter.encoder_attn_fc1.weight', 'layers.7.adapter.encoder_attn_fc2.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.weight', 'layers.7.adapter.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_msk.encoder_attn_fc1.weight', 'layers.7.adapter_msk.encoder_attn_fc2.weight', 'layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.7.adapter_reposition.encoder_attn_fc1.weight', 'layers.7.adapter_reposition.encoder_attn_fc2.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.8.encoder_attn.k_proj.weight', 'layers.8.encoder_attn.k_proj.bias', 'layers.8.encoder_attn.v_proj.weight', 'layers.8.encoder_attn.v_proj.bias', 'layers.8.encoder_attn.q_proj.weight', 'layers.8.encoder_attn.q_proj.bias', 'layers.8.encoder_attn.out_proj.weight', 'layers.8.encoder_attn.out_proj.bias', 'layers.8.encoder_attn_layer_norm.weight', 'layers.8.encoder_attn_layer_norm.bias', 'layers.8.adapter.encoder_attn_fc1.weight', 'layers.8.adapter.encoder_attn_fc2.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.weight', 'layers.8.adapter.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_msk.encoder_attn_fc1.weight', 'layers.8.adapter_msk.encoder_attn_fc2.weight', 'layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.8.adapter_reposition.encoder_attn_fc1.weight', 'layers.8.adapter_reposition.encoder_attn_fc2.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.9.encoder_attn.k_proj.weight', 'layers.9.encoder_attn.k_proj.bias', 'layers.9.encoder_attn.v_proj.weight', 'layers.9.encoder_attn.v_proj.bias', 'layers.9.encoder_attn.q_proj.weight', 'layers.9.encoder_attn.q_proj.bias', 'layers.9.encoder_attn.out_proj.weight', 'layers.9.encoder_attn.out_proj.bias', 'layers.9.encoder_attn_layer_norm.weight', 'layers.9.encoder_attn_layer_norm.bias', 'layers.9.adapter.encoder_attn_fc1.weight', 'layers.9.adapter.encoder_attn_fc2.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.weight', 'layers.9.adapter.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_msk.encoder_attn_fc1.weight', 'layers.9.adapter_msk.encoder_attn_fc2.weight', 'layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.9.adapter_reposition.encoder_attn_fc1.weight', 'layers.9.adapter_reposition.encoder_attn_fc2.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.10.encoder_attn.k_proj.weight', 'layers.10.encoder_attn.k_proj.bias', 'layers.10.encoder_attn.v_proj.weight', 'layers.10.encoder_attn.v_proj.bias', 'layers.10.encoder_attn.q_proj.weight', 'layers.10.encoder_attn.q_proj.bias', 'layers.10.encoder_attn.out_proj.weight', 'layers.10.encoder_attn.out_proj.bias', 'layers.10.encoder_attn_layer_norm.weight', 'layers.10.encoder_attn_layer_norm.bias', 'layers.10.adapter.encoder_attn_fc1.weight', 'layers.10.adapter.encoder_attn_fc2.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.weight', 'layers.10.adapter.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_msk.encoder_attn_fc1.weight', 'layers.10.adapter_msk.encoder_attn_fc2.weight', 'layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.10.adapter_reposition.encoder_attn_fc1.weight', 'layers.10.adapter_reposition.encoder_attn_fc2.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'layers.11.encoder_attn.k_proj.weight', 'layers.11.encoder_attn.k_proj.bias', 'layers.11.encoder_attn.v_proj.weight', 'layers.11.encoder_attn.v_proj.bias', 'layers.11.encoder_attn.q_proj.weight', 'layers.11.encoder_attn.q_proj.bias', 'layers.11.encoder_attn.out_proj.weight', 'layers.11.encoder_attn.out_proj.bias', 'layers.11.encoder_attn_layer_norm.weight', 'layers.11.encoder_attn_layer_norm.bias', 'layers.11.adapter.encoder_attn_fc1.weight', 'layers.11.adapter.encoder_attn_fc2.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.weight', 'layers.11.adapter.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_msk.encoder_attn_fc1.weight', 'layers.11.adapter_msk.encoder_attn_fc2.weight', 'layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'layers.11.adapter_reposition.encoder_attn_fc1.weight', 'layers.11.adapter_reposition.encoder_attn_fc2.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias', 'output_projection.weight']
2022-07-07 15:22:03 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertDecoderWithAdaptor: ['pooler.dense.weight', 'pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 319
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
2022-07-07 15:22:03 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoderWithAdaptor(
    (bert): BertModelWithAdapter(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderWithAdapter(
        (layer): ModuleList(
          (0): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (1): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (2): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (3): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (4): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (5): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (6): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (7): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (8): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (9): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (10): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
          (11): BertLayerWithAdapter(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (adapter_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (adapter_w1): Linear(in_features=768, out_features=2048, bias=False)
            (adapter_w2): Linear(in_features=2048, out_features=768, bias=False)
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): BertDecoderWithAdaptor(
    (embed_mask_ins): Embedding(256, 1536)
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): BertAdapterDecoderLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (adapter): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_msk): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (adapter_reposition): DecoderAdapter(
          (encoder_attn_fc1): Linear(in_features=768, out_features=2048, bias=False)
          (encoder_attn_fc2): Linear(in_features=2048, out_features=768, bias=False)
          (encoder_attn_final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_projection): Linear(in_features=768, out_features=28996, bias=False)
  )
)
2022-07-07 15:22:03 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-07 15:22:03 | INFO | fairseq_cli.train | num. model params: 418522883 (num. trained: 180224003)
2022-07-07 15:22:03 | INFO | fairseq_cli.train | num. Encoder model params: 146472707 (Encoder num. trained: 38162435)
2022-07-07 15:22:03 | INFO | fairseq_cli.train | num. Decoder model params: 272050176 (Decoder num. trained: 142061568)
Trained parameters: len 319
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']Trained parameters: len 319
Trained parameters: ['encoder.bert.encoder.layer.0.adapter_ln.weight', 'encoder.bert.encoder.layer.0.adapter_ln.bias', 'encoder.bert.encoder.layer.0.adapter_w1.weight', 'encoder.bert.encoder.layer.0.adapter_w2.weight', 'encoder.bert.encoder.layer.1.adapter_ln.weight', 'encoder.bert.encoder.layer.1.adapter_ln.bias', 'encoder.bert.encoder.layer.1.adapter_w1.weight', 'encoder.bert.encoder.layer.1.adapter_w2.weight', 'encoder.bert.encoder.layer.2.adapter_ln.weight', 'encoder.bert.encoder.layer.2.adapter_ln.bias', 'encoder.bert.encoder.layer.2.adapter_w1.weight', 'encoder.bert.encoder.layer.2.adapter_w2.weight', 'encoder.bert.encoder.layer.3.adapter_ln.weight', 'encoder.bert.encoder.layer.3.adapter_ln.bias', 'encoder.bert.encoder.layer.3.adapter_w1.weight', 'encoder.bert.encoder.layer.3.adapter_w2.weight', 'encoder.bert.encoder.layer.4.adapter_ln.weight', 'encoder.bert.encoder.layer.4.adapter_ln.bias', 'encoder.bert.encoder.layer.4.adapter_w1.weight', 'encoder.bert.encoder.layer.4.adapter_w2.weight', 'encoder.bert.encoder.layer.5.adapter_ln.weight', 'encoder.bert.encoder.layer.5.adapter_ln.bias', 'encoder.bert.encoder.layer.5.adapter_w1.weight', 'encoder.bert.encoder.layer.5.adapter_w2.weight', 'encoder.bert.encoder.layer.6.adapter_ln.weight', 'encoder.bert.encoder.layer.6.adapter_ln.bias', 'encoder.bert.encoder.layer.6.adapter_w1.weight', 'encoder.bert.encoder.layer.6.adapter_w2.weight', 'encoder.bert.encoder.layer.7.adapter_ln.weight', 'encoder.bert.encoder.layer.7.adapter_ln.bias', 'encoder.bert.encoder.layer.7.adapter_w1.weight', 'encoder.bert.encoder.layer.7.adapter_w2.weight', 'encoder.bert.encoder.layer.8.adapter_ln.weight', 'encoder.bert.encoder.layer.8.adapter_ln.bias', 'encoder.bert.encoder.layer.8.adapter_w1.weight', 'encoder.bert.encoder.layer.8.adapter_w2.weight', 'encoder.bert.encoder.layer.9.adapter_ln.weight', 'encoder.bert.encoder.layer.9.adapter_ln.bias', 'encoder.bert.encoder.layer.9.adapter_w1.weight', 'encoder.bert.encoder.layer.9.adapter_w2.weight', 'encoder.bert.encoder.layer.10.adapter_ln.weight', 'encoder.bert.encoder.layer.10.adapter_ln.bias', 'encoder.bert.encoder.layer.10.adapter_w1.weight', 'encoder.bert.encoder.layer.10.adapter_w2.weight', 'encoder.bert.encoder.layer.11.adapter_ln.weight', 'encoder.bert.encoder.layer.11.adapter_ln.bias', 'encoder.bert.encoder.layer.11.adapter_w1.weight', 'encoder.bert.encoder.layer.11.adapter_w2.weight', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.adapter.encoder_attn_fc1.weight', 'decoder.layers.0.adapter.encoder_attn_fc2.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.0.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.0.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.adapter.encoder_attn_fc1.weight', 'decoder.layers.1.adapter.encoder_attn_fc2.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.1.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.1.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.adapter.encoder_attn_fc1.weight', 'decoder.layers.2.adapter.encoder_attn_fc2.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.2.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.2.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.adapter.encoder_attn_fc1.weight', 'decoder.layers.3.adapter.encoder_attn_fc2.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.3.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.3.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.adapter.encoder_attn_fc1.weight', 'decoder.layers.4.adapter.encoder_attn_fc2.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.4.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.4.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.adapter.encoder_attn_fc1.weight', 'decoder.layers.5.adapter.encoder_attn_fc2.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.5.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.5.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.adapter.encoder_attn_fc1.weight', 'decoder.layers.6.adapter.encoder_attn_fc2.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.6.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.6.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.adapter.encoder_attn_fc1.weight', 'decoder.layers.7.adapter.encoder_attn_fc2.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.7.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.7.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.8.adapter.encoder_attn_fc1.weight', 'decoder.layers.8.adapter.encoder_attn_fc2.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.8.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.8.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.9.adapter.encoder_attn_fc1.weight', 'decoder.layers.9.adapter.encoder_attn_fc2.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.9.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.9.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.10.adapter.encoder_attn_fc1.weight', 'decoder.layers.10.adapter.encoder_attn_fc2.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.10.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.10.adapter_reposition.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.11.adapter.encoder_attn_fc1.weight', 'decoder.layers.11.adapter.encoder_attn_fc2.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_msk.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_msk.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_msk.encoder_attn_final_layer_norm.bias', 'decoder.layers.11.adapter_reposition.encoder_attn_fc1.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_fc2.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.weight', 'decoder.layers.11.adapter_reposition.encoder_attn_final_layer_norm.bias']2022-07-07 15:22:10 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-07 15:22:10 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-07 15:22:10 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt
2022-07-07 15:22:10 | INFO | fairseq.trainer | loading train data for epoch 1

Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]2022-07-07 15:22:10 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.source

Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]2022-07-07 15:22:10 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510/train.source-target.target
2022-07-07 15:22:10 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-cased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]299it [00:00, 2988.35it/s]
Trained parameters not adapter: len 7
Trained parameters not adapter: ['encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_mask_ins.weight']
start load cached examples train ...
0it [00:00, ?it/s]329it [00:00, 3288.38it/s]313it [00:00, 3120.57it/s]326it [00:00, 3251.40it/s]598it [00:00, 2895.21it/s]658it [00:00, 2967.77it/s]626it [00:00, 2956.10it/s]920it [00:00, 3038.00it/s]652it [00:00, 2933.97it/s]967it [00:00, 3019.52it/s]1245it [00:00, 3118.93it/s]988it [00:00, 3116.49it/s]923it [00:00, 2704.22it/s]1328it [00:00, 3243.68it/s]1558it [00:00, 3058.01it/s]1357it [00:00, 3332.63it/s]1279it [00:00, 3015.31it/s]1655it [00:00, 3181.77it/s]1930it [00:00, 3275.99it/s]1585it [00:00, 3014.42it/s]1693it [00:00, 3206.86it/s]2026it [00:00, 3354.44it/s]2260it [00:00, 3238.54it/s]1967it [00:00, 3277.89it/s]2069it [00:00, 3386.57it/s]2363it [00:00, 3288.80it/s]2638it [00:00, 3405.93it/s]2298it [00:00, 3259.35it/s]2410it [00:00, 3297.52it/s]2738it [00:00, 3429.02it/s]3021it [00:00, 3535.61it/s]2676it [00:00, 3419.30it/s]2785it [00:00, 3434.61it/s]3100it [00:00, 3349.56it/s]3376it [00:01, 3424.74it/s]3055it [00:00, 3532.69it/s]3131it [00:00, 3328.85it/s]3481it [00:01, 3483.28it/s]3748it [00:01, 3509.91it/s]3410it [00:01, 3429.30it/s]3500it [00:01, 3435.06it/s]3855it [00:01, 3559.01it/s]4100it [00:01, 3412.27it/s]3785it [00:01, 3522.31it/s]3866it [00:01, 3500.41it/s]4213it [00:01, 3454.57it/s]4480it [00:01, 3523.80it/s]4139it [00:01, 3415.16it/s]4218it [00:01, 3376.49it/s]4590it [00:01, 3546.20it/s]4834it [00:01, 3405.47it/s]4521it [00:01, 3530.69it/s]4586it [00:01, 3462.30it/s]4946it [00:01, 3430.13it/s]5207it [00:01, 3497.61it/s]4876it [00:01, 3422.85it/s]4934it [00:01, 3338.58it/s]5325it [00:01, 3532.77it/s]5587it [00:01, 3585.77it/s]5254it [00:01, 3524.66it/s]5302it [00:01, 3434.34it/s]5680it [00:01, 3422.24it/s]5947it [00:01, 3466.80it/s]5619it [00:01, 3418.86it/s]5648it [00:01, 3311.34it/s]6059it [00:01, 3527.74it/s]6322it [00:01, 3547.20it/s]5987it [00:01, 3492.74it/s]6013it [00:01, 3406.51it/s]6439it [00:01, 3606.89it/s]6679it [00:01, 3442.78it/s]6338it [00:01, 3474.53it/s]6365it [00:01, 3436.83it/s]6802it [00:01, 3478.26it/s]7055it [00:02, 3531.88it/s]6687it [00:01, 3353.64it/s]6711it [00:02, 3166.73it/s]7179it [00:02, 3560.37it/s]7060it [00:02, 3458.85it/s]7070it [00:02, 3281.95it/s]7410it [00:03, 953.01it/s] 7537it [00:03, 958.69it/s] 7718it [00:03, 1167.28it/s]7867it [00:03, 1194.14it/s]8040it [00:03, 1425.57it/s]7403it [00:03, 862.15it/s] 8168it [00:03, 1421.77it/s]8328it [00:03, 1621.24it/s]7741it [00:03, 1103.52it/s]7408it [00:03, 803.42it/s] 8466it [00:03, 1658.65it/s]8697it [00:03, 1985.45it/s]8114it [00:03, 1419.65it/s]7777it [00:03, 1056.38it/s]8839it [00:03, 2027.04it/s]8416it [00:03, 1638.75it/s]9050it [00:03, 2240.21it/s]8150it [00:03, 1353.73it/s]9154it [00:03, 2231.39it/s]8792it [00:03, 2002.91it/s]9423it [00:03, 2565.80it/s]8463it [00:03, 1593.12it/s]9531it [00:03, 2574.52it/s]9114it [00:03, 2242.07it/s]9804it [00:03, 2861.25it/s]8838it [00:03, 1942.94it/s]9889it [00:03, 2693.59it/s]9460it [00:03, 2508.22it/s]10148it [00:03, 2937.98it/s]9167it [00:03, 2174.34it/s]10260it [00:03, 2944.25it/s]9840it [00:03, 2816.02it/s]10527it [00:04, 3159.71it/s]9543it [00:03, 2507.76it/s]10635it [00:04, 3153.48it/s]10184it [00:04, 2919.19it/s]10875it [00:04, 3139.85it/s]9888it [00:04, 2661.22it/s]10983it [00:04, 3157.81it/s]10565it [00:04, 3141.03it/s]11257it [00:04, 3324.87it/s]10265it [00:04, 2931.67it/s]11362it [00:04, 3330.33it/s]10914it [00:04, 3149.15it/s]10641it [00:04, 3144.07it/s]11607it [00:04, 3267.99it/s]11713it [00:04, 3254.53it/s]11294it [00:04, 3326.69it/s]11987it [00:04, 3416.06it/s]10995it [00:04, 3104.67it/s]12088it [00:04, 3390.95it/s]11645it [00:04, 3287.82it/s]12362it [00:04, 3510.38it/s]11367it [00:04, 3269.37it/s]12437it [00:04, 3320.84it/s]12022it [00:04, 3422.32it/s]12721it [00:04, 3432.30it/s]11716it [00:04, 3224.28it/s]12811it [00:04, 3438.08it/s]12408it [00:04, 3545.25it/s]13105it [00:04, 3548.96it/s]12097it [00:04, 3386.54it/s]13191it [00:04, 3540.68it/s]12770it [00:04, 3435.81it/s]13465it [00:04, 3451.83it/s]12448it [00:04, 3313.49it/s]13550it [00:04, 3444.82it/s]13160it [00:04, 3568.20it/s]13845it [00:04, 3550.96it/s]12827it [00:04, 3446.72it/s]13943it [00:04, 3583.30it/s]13522it [00:04, 3467.20it/s]14203it [00:05, 3464.62it/s]13209it [00:05, 3552.73it/s]14305it [00:05, 3452.95it/s]13903it [00:05, 3564.09it/s]14581it [00:05, 3552.67it/s]13570it [00:05, 3432.52it/s]14694it [00:05, 3575.06it/s]14263it [00:05, 3480.38it/s]14939it [00:05, 3469.72it/s]13960it [00:05, 3564.93it/s]15055it [00:05, 3475.61it/s]14635it [00:05, 3547.75it/s]15315it [00:05, 3553.15it/s]14321it [00:05, 3475.23it/s]15433it [00:05, 3562.50it/s]15693it [00:05, 3616.70it/s]14709it [00:05, 3591.14it/s]14992it [00:05, 3289.37it/s]15792it [00:05, 3428.14it/s]15345it [00:05, 3353.70it/s]16056it [00:05, 3446.29it/s]15071it [00:05, 3491.83it/s]16163it [00:05, 3507.78it/s]15697it [00:05, 3400.36it/s]16439it [00:05, 3555.37it/s]15426it [00:05, 3505.58it/s]16529it [00:05, 3551.30it/s]16040it [00:05, 3111.39it/s]15779it [00:05, 3290.70it/s]16120it [00:05, 3323.74it/s]16358it [00:05, 3095.01it/s]16455it [00:05, 3316.20it/s]16672it [00:05, 2995.32it/s]16797it [00:06, 893.24it/s] 17187it [00:06, 1175.68it/s]17519it [00:07, 1420.89it/s]16886it [00:07, 799.21it/s] 16975it [00:06, 838.66it/s] 17909it [00:07, 1777.42it/s]17263it [00:07, 1053.87it/s]17365it [00:07, 1143.18it/s]18289it [00:07, 2119.54it/s]17587it [00:07, 1292.19it/s]17689it [00:07, 1403.75it/s]18633it [00:07, 2354.88it/s]16789it [00:07, 737.51it/s] 17974it [00:07, 1639.72it/s]18077it [00:07, 1777.49it/s]19003it [00:07, 2646.43it/s]17162it [00:07, 988.62it/s]18358it [00:07, 1994.59it/s]18399it [00:07, 2032.70it/s]19352it [00:07, 2793.40it/s]17518it [00:07, 1250.41it/s]18703it [00:07, 2238.84it/s]18780it [00:07, 2391.61it/s]19726it [00:07, 3028.30it/s]17897it [00:07, 1583.78it/s]19079it [00:07, 2555.68it/s]19166it [00:07, 2719.88it/s]18287it [00:07, 1949.24it/s]20077it [00:07, 3080.29it/s]19429it [00:07, 2721.21it/s]19518it [00:07, 2857.18it/s]20448it [00:07, 3248.77it/s]18628it [00:07, 2180.66it/s]19805it [00:07, 2972.64it/s]19897it [00:07, 3091.69it/s]20826it [00:07, 3394.17it/s]19007it [00:07, 2510.62it/s]20157it [00:07, 3042.30it/s]20252it [00:07, 3120.09it/s]19353it [00:08, 2677.29it/s]21185it [00:08, 3299.12it/s]20530it [00:08, 3217.47it/s]20623it [00:08, 3278.73it/s]19737it [00:08, 2957.89it/s]21569it [00:08, 3447.97it/s]20881it [00:08, 3192.13it/s]20975it [00:08, 3223.69it/s]20089it [00:08, 3014.83it/s]21925it [00:08, 3358.04it/s]21261it [00:08, 3358.77it/s]21351it [00:08, 3371.66it/s]20459it [00:08, 3194.34it/s]22304it [00:08, 3479.43it/s]21646it [00:08, 3495.05it/s]21720it [00:08, 3317.01it/s]20832it [00:08, 3340.17it/s]22659it [00:08, 3393.35it/s]22008it [00:08, 3380.56it/s]22092it [00:08, 3428.77it/s]21189it [00:08, 3293.56it/s]23034it [00:08, 3494.77it/s]22379it [00:08, 3470.77it/s]22463it [00:08, 3508.68it/s]21545it [00:08, 3367.19it/s]23388it [00:08, 3470.25it/s]22733it [00:08, 3294.71it/s]22820it [00:08, 3330.27it/s]21894it [00:08, 3257.22it/s]23738it [00:08, 3209.29it/s]23073it [00:08, 3323.75it/s]23159it [00:08, 3311.87it/s]22229it [00:08, 3282.74it/s]24073it [00:08, 3246.51it/s]23410it [00:08, 3088.92it/s]23494it [00:08, 3100.94it/s]22564it [00:08, 3140.30it/s]23735it [00:09, 3130.82it/s]24402it [00:09, 2955.17it/s]23815it [00:08, 3129.85it/s]22907it [00:09, 3220.97it/s]24053it [00:09, 3135.61it/s]24735it [00:09, 3053.27it/s]24145it [00:09, 3176.88it/s]23266it [00:09, 3326.32it/s]24370it [00:09, 3101.75it/s]25079it [00:09, 3078.53it/s]24466it [00:09, 3109.41it/s]23603it [00:09, 3258.01it/s]24733it [00:09, 3253.01it/s]25454it [00:09, 3263.81it/s]24835it [00:09, 3273.23it/s]23980it [00:09, 3404.05it/s]25835it [00:09, 3417.46it/s]25080it [00:09, 3203.29it/s]25165it [00:09, 3230.21it/s]24323it [00:09, 3277.60it/s]25448it [00:09, 3337.70it/s]26181it [00:09, 3306.90it/s]25543it [00:09, 3388.84it/s]24703it [00:09, 3425.76it/s]25817it [00:09, 3438.11it/s]26559it [00:09, 3440.73it/s]25920it [00:09, 3341.82it/s]25074it [00:09, 3507.32it/s]26163it [00:09, 3320.43it/s]26907it [00:09, 3312.22it/s]26285it [00:09, 3428.94it/s]25427it [00:09, 3400.91it/s]26540it [00:09, 3427.71it/s]27294it [00:09, 3469.26it/s]26665it [00:09, 3534.33it/s]25800it [00:09, 3494.34it/s]26885it [00:09, 3298.85it/s]27644it [00:10, 3362.40it/s]27020it [00:09, 3442.43it/s]26152it [00:10, 3385.38it/s]27268it [00:10, 3449.26it/s]28026it [00:10, 3490.36it/s]27405it [00:10, 3558.23it/s]26522it [00:10, 3475.33it/s]28384it [00:10, 3515.98it/s]27616it [00:10, 3379.88it/s]27763it [00:10, 3376.30it/s]26872it [00:10, 3372.63it/s]28002it [00:10, 3516.49it/s]28123it [00:10, 3438.76it/s]27249it [00:10, 3482.58it/s]28375it [00:10, 3578.25it/s]28470it [00:10, 3288.31it/s]27599it [00:10, 3308.53it/s]27977it [00:10, 3439.83it/s]28349it [00:10, 3517.86it/s]28738it [00:11, 618.26it/s] 29056it [00:12, 793.10it/s]29348it [00:12, 964.50it/s]28802it [00:12, 593.61it/s] 29669it [00:12, 1213.30it/s]28735it [00:12, 585.06it/s] 29168it [00:12, 800.74it/s]30045it [00:12, 1561.19it/s]29109it [00:12, 787.22it/s]29444it [00:12, 968.09it/s]30355it [00:12, 1810.58it/s]29416it [00:12, 978.54it/s]29820it [00:12, 1278.45it/s]30731it [00:12, 2178.65it/s]29792it [00:12, 1275.80it/s]30188it [00:12, 1572.18it/s]31060it [00:12, 2375.38it/s]30162it [00:12, 1596.59it/s]30565it [00:12, 1924.97it/s]31436it [00:12, 2692.49it/s]30497it [00:12, 1861.50it/s]30941it [00:12, 2267.03it/s]31816it [00:12, 2964.31it/s]30865it [00:12, 2195.74it/s]31283it [00:12, 2467.07it/s]28703it [00:12, 492.31it/s] 32167it [00:12, 3027.66it/s]31207it [00:12, 2414.86it/s]31659it [00:12, 2761.90it/s]29072it [00:12, 667.72it/s]32535it [00:13, 3200.16it/s]31580it [00:13, 2712.64it/s]32006it [00:13, 2877.81it/s]29362it [00:13, 829.90it/s]32884it [00:13, 3171.17it/s]31927it [00:13, 2839.42it/s]32386it [00:13, 3113.08it/s]29728it [00:13, 1096.04it/s]33243it [00:13, 3285.03it/s]32296it [00:13, 3055.80it/s]32738it [00:13, 3112.29it/s]30095it [00:13, 1400.68it/s]32663it [00:13, 3218.77it/s]33587it [00:13, 3230.94it/s]33113it [00:13, 3282.75it/s]30423it [00:13, 1632.92it/s]33921it [00:13, 3232.00it/s]33017it [00:13, 3092.57it/s]33483it [00:13, 3397.89it/s]30786it [00:13, 1967.69it/s]34301it [00:13, 3393.67it/s]33389it [00:13, 3259.46it/s]33839it [00:13, 3326.65it/s]31115it [00:13, 2145.96it/s]34647it [00:13, 3319.66it/s]33733it [00:13, 3246.60it/s]34227it [00:13, 3482.51it/s]31479it [00:13, 2459.72it/s]35016it [00:13, 3423.33it/s]34113it [00:13, 3399.27it/s]34585it [00:13, 3383.70it/s]31836it [00:13, 2715.00it/s]35362it [00:13, 3335.48it/s]34463it [00:13, 3346.61it/s]34949it [00:13, 3452.61it/s]32173it [00:13, 2761.30it/s]35733it [00:14, 3442.97it/s]34829it [00:13, 3434.99it/s]32528it [00:14, 2959.51it/s]35300it [00:13, 3206.39it/s]36080it [00:14, 3352.81it/s]35206it [00:14, 3531.14it/s]35666it [00:14, 3328.71it/s]32860it [00:14, 2991.37it/s]36426it [00:14, 3381.62it/s]35564it [00:14, 3392.15it/s]36019it [00:14, 3382.06it/s]33236it [00:14, 3199.61it/s]36798it [00:14, 3479.93it/s]35941it [00:14, 3497.64it/s]36362it [00:14, 3114.02it/s]33576it [00:14, 2936.64it/s]37148it [00:14, 3275.93it/s]36294it [00:14, 3265.01it/s]36711it [00:14, 3215.21it/s]33944it [00:14, 3132.17it/s]37502it [00:14, 3348.45it/s]36633it [00:14, 3291.70it/s]34295it [00:14, 3235.12it/s]37039it [00:14, 2999.08it/s]37840it [00:14, 3160.97it/s]36966it [00:14, 3134.02it/s]37366it [00:14, 3072.12it/s]34630it [00:14, 3041.15it/s]38164it [00:14, 3180.23it/s]37298it [00:14, 3185.03it/s]37685it [00:14, 3104.06it/s]34944it [00:14, 3066.02it/s]38485it [00:14, 3170.29it/s]37620it [00:14, 3179.65it/s]38000it [00:14, 3000.37it/s]35258it [00:14, 2942.69it/s]38804it [00:15, 3078.90it/s]37940it [00:14, 3113.89it/s]38375it [00:14, 3199.45it/s]35637it [00:14, 3174.64it/s]39177it [00:15, 3263.31it/s]38312it [00:15, 3285.18it/s]35998it [00:15, 3296.45it/s]38699it [00:15, 3058.81it/s]39506it [00:15, 3205.65it/s]38643it [00:15, 3191.62it/s]39076it [00:15, 3255.75it/s]36333it [00:15, 3232.51it/s]39884it [00:15, 3370.37it/s]39018it [00:15, 3351.24it/s]36701it [00:15, 3358.01it/s]39429it [00:15, 3222.22it/s]40262it [00:15, 3487.50it/s]39402it [00:15, 3491.75it/s]39800it [00:15, 3359.33it/s]37040it [00:15, 3283.82it/s]40613it [00:15, 3371.42it/s]39753it [00:15, 3373.32it/s]40171it [00:15, 3459.97it/s]37405it [00:15, 3388.35it/s]40983it [00:15, 3465.29it/s]40129it [00:15, 3476.07it/s]40520it [00:15, 3332.17it/s]37748it [00:15, 3274.67it/s]41332it [00:15, 3382.07it/s]40479it [00:15, 3376.80it/s]40891it [00:15, 3438.40it/s]38120it [00:15, 3400.72it/s]41706it [00:15, 3483.38it/s]40854it [00:15, 3482.41it/s]38485it [00:15, 3463.84it/s]41238it [00:15, 3340.70it/s]42056it [00:15, 3388.22it/s]41204it [00:15, 3390.65it/s]41600it [00:15, 3418.95it/s]38833it [00:15, 3356.24it/s]42429it [00:16, 3485.49it/s]41579it [00:16, 3492.40it/s]39206it [00:16, 3461.54it/s]41949it [00:16, 3332.82it/s]42789it [00:16, 3387.75it/s]41949it [00:16, 3348.53it/s]42322it [00:16, 3444.58it/s]39554it [00:16, 3311.86it/s]43161it [00:16, 3480.17it/s]42331it [00:16, 3479.46it/s]42693it [00:16, 3519.57it/s]39929it [00:16, 3436.17it/s]43532it [00:16, 3545.35it/s]42704it [00:16, 3550.24it/s]43047it [00:16, 3406.58it/s]40275it [00:16, 3335.70it/s]43062it [00:16, 3427.88it/s]43419it [00:16, 3485.22it/s]40647it [00:16, 3444.43it/s]43424it [00:16, 3482.61it/s]41026it [00:16, 3542.20it/s]41382it [00:16, 3427.47it/s]41742it [00:16, 3477.06it/s]42092it [00:16, 3340.22it/s]42465it [00:16, 3449.12it/s]42812it [00:17, 3329.90it/s]43177it [00:17, 3419.21it/s]43536it [00:17, 3445.79it/s]43888it [00:18, 571.83it/s] 44266it [00:18, 774.30it/s]44560it [00:18, 953.46it/s]44941it [00:18, 1255.26it/s]45317it [00:18, 1583.44it/s]43775it [00:18, 526.25it/s] 45653it [00:18, 1845.89it/s]44157it [00:18, 719.72it/s]46011it [00:18, 2162.07it/s]44527it [00:18, 950.43it/s]43769it [00:18, 469.28it/s] 44841it [00:18, 1168.41it/s]46350it [00:18, 2370.15it/s]44152it [00:18, 647.10it/s]45203it [00:18, 1473.55it/s]46721it [00:19, 2670.10it/s]44533it [00:18, 869.25it/s]45532it [00:19, 1735.59it/s]47063it [00:19, 2786.66it/s]44850it [00:19, 1076.99it/s]45906it [00:19, 2089.05it/s]47438it [00:19, 3029.59it/s]45228it [00:19, 1387.84it/s]46245it [00:19, 2309.61it/s]47812it [00:19, 3217.91it/s]45562it [00:19, 1637.68it/s]46617it [00:19, 2618.96it/s]48166it [00:19, 3198.87it/s]45935it [00:19, 1983.83it/s]46992it [00:19, 2887.96it/s]48542it [00:19, 3351.10it/s]46274it [00:19, 2211.71it/s]43882it [00:19, 474.46it/s] 47345it [00:19, 2986.07it/s]48895it [00:19, 3289.22it/s]46645it [00:19, 2527.54it/s]44260it [00:19, 653.18it/s]47720it [00:19, 3186.30it/s]49272it [00:19, 3422.46it/s]47022it [00:19, 2815.44it/s]44580it [00:19, 835.65it/s]48074it [00:19, 3187.98it/s]49624it [00:19, 3351.90it/s]47375it [00:19, 2918.83it/s]44958it [00:19, 1109.02it/s]48454it [00:19, 3355.63it/s]49998it [00:19, 3461.19it/s]47751it [00:19, 3134.32it/s]45339it [00:19, 1425.00it/s]48809it [00:20, 3308.73it/s]50364it [00:20, 3517.46it/s]48104it [00:20, 3140.72it/s]45678it [00:20, 1653.98it/s]49184it [00:20, 3431.27it/s]50720it [00:20, 3404.94it/s]48480it [00:20, 3307.99it/s]46019it [00:20, 1944.20it/s]49563it [00:20, 3532.71it/s]51100it [00:20, 3517.83it/s]48832it [00:20, 3261.67it/s]46346it [00:20, 2134.90it/s]49924it [00:20, 3362.71it/s]49192it [00:20, 3355.73it/s]51455it [00:20, 3299.16it/s]46700it [00:20, 2428.93it/s]50272it [00:20, 3393.10it/s]49550it [00:20, 3418.82it/s]51798it [00:20, 3333.33it/s]47026it [00:20, 2601.33it/s]50617it [00:20, 3140.89it/s]49900it [00:20, 3171.79it/s]52135it [00:20, 2924.32it/s]47349it [00:20, 2574.54it/s]50946it [00:20, 3180.56it/s]50227it [00:20, 3175.84it/s]52466it [00:20, 3023.35it/s]47674it [00:20, 2740.89it/s]51270it [00:20, 3044.79it/s]50551it [00:20, 3013.47it/s]52790it [00:20, 3081.27it/s]47983it [00:20, 2784.11it/s]51633it [00:20, 3205.83it/s]50901it [00:20, 3145.12it/s]53105it [00:20, 3073.65it/s]48360it [00:20, 3047.07it/s]52009it [00:20, 3360.77it/s]53477it [00:21, 3256.00it/s]51258it [00:20, 3119.23it/s]48738it [00:21, 3096.58it/s]52349it [00:21, 3240.49it/s]53807it [00:21, 3217.02it/s]51576it [00:21, 3135.92it/s]49116it [00:21, 3281.90it/s]52723it [00:21, 3381.32it/s]54191it [00:21, 3395.80it/s]51931it [00:21, 3252.82it/s]49494it [00:21, 3421.65it/s]53065it [00:21, 3311.31it/s]54559it [00:21, 3477.53it/s]52259it [00:21, 3221.35it/s]49846it [00:21, 3347.18it/s]53437it [00:21, 3428.28it/s]54909it [00:21, 3390.83it/s]52635it [00:21, 3375.73it/s]50217it [00:21, 3449.94it/s]53782it [00:21, 3329.57it/s]55279it [00:21, 3479.35it/s]52975it [00:21, 3313.93it/s]50568it [00:21, 3351.47it/s]54165it [00:21, 3470.82it/s]53319it [00:21, 3348.30it/s]55629it [00:21, 3373.63it/s]50943it [00:21, 3464.54it/s]54543it [00:21, 3559.01it/s]53692it [00:21, 3459.89it/s]55997it [00:21, 3460.01it/s]51293it [00:21, 3373.56it/s]54901it [00:21, 3430.64it/s]54039it [00:21, 3377.01it/s]56345it [00:21, 3385.34it/s]51668it [00:21, 3479.81it/s]55272it [00:21, 3510.55it/s]54421it [00:21, 3503.53it/s]56719it [00:22, 3487.51it/s]52029it [00:21, 3515.65it/s]55625it [00:22, 3387.77it/s]57101it [00:22, 3583.70it/s]54773it [00:22, 3368.38it/s]52383it [00:22, 3353.95it/s]55998it [00:22, 3485.25it/s]55147it [00:22, 3473.09it/s]57461it [00:22, 3446.50it/s]52757it [00:22, 3462.77it/s]56349it [00:22, 3375.18it/s]57838it [00:22, 3530.71it/s]55496it [00:22, 3358.69it/s]56723it [00:22, 3477.87it/s]55868it [00:22, 3460.77it/s]58193it [00:22, 3417.26it/s]53106it [00:22, 3120.80it/s]57103it [00:22, 3570.45it/s]56245it [00:22, 3549.70it/s]58556it [00:22, 3477.96it/s]53471it [00:22, 3262.90it/s]57462it [00:22, 3450.68it/s]53804it [00:22, 3194.77it/s]58906it [00:22, 3376.93it/s]56602it [00:22, 3428.28it/s]57833it [00:22, 3524.13it/s]54181it [00:22, 3353.24it/s]59280it [00:22, 3480.31it/s]56980it [00:22, 3528.13it/s]58187it [00:22, 3404.26it/s]54547it [00:22, 3440.17it/s]59647it [00:22, 3533.99it/s]57335it [00:22, 3440.84it/s]58556it [00:22, 3483.99it/s]57698it [00:22, 3489.22it/s]60002it [00:22, 3393.34it/s]54895it [00:22, 3283.36it/s]58906it [00:23, 3364.73it/s]60370it [00:23, 3475.09it/s]55249it [00:22, 3352.79it/s]58049it [00:22, 3282.48it/s]59264it [00:23, 3424.12it/s]58399it [00:23, 3340.90it/s]60720it [00:23, 3288.24it/s]55588it [00:23, 3149.78it/s]59614it [00:23, 3443.63it/s]58755it [00:23, 3401.42it/s]61082it [00:23, 3379.83it/s]55946it [00:23, 3269.17it/s]59960it [00:23, 3102.71it/s]56277it [00:23, 3201.95it/s]61423it [00:23, 3184.52it/s]59098it [00:23, 3012.98it/s]61746it [00:23, 3194.77it/s]56600it [00:23, 3026.49it/s]60277it [00:23, 2918.82it/s]59409it [00:23, 2826.66it/s]62069it [00:23, 3182.83it/s]56929it [00:23, 3094.98it/s]60575it [00:23, 2629.86it/s]59700it [00:23, 2549.38it/s]57242it [00:23, 3031.49it/s]60849it [00:23, 2651.39it/s]60035it [00:23, 2750.36it/s]57607it [00:23, 3205.20it/s]61224it [00:23, 2943.10it/s]60396it [00:23, 2975.76it/s]57974it [00:23, 3337.19it/s]61526it [00:23, 2950.70it/s]60704it [00:23, 2970.66it/s]58310it [00:23, 3236.44it/s]61899it [00:24, 3161.30it/s]61066it [00:24, 3151.42it/s]58674it [00:24, 3350.45it/s]62221it [00:24, 3136.30it/s]61388it [00:24, 3125.59it/s]59011it [00:24, 3217.19it/s]61748it [00:24, 3259.01it/s]59366it [00:24, 3310.86it/s]62106it [00:24, 3351.87it/s]59700it [00:24, 3171.70it/s]60055it [00:24, 3276.63it/s]60403it [00:24, 3333.35it/s]60739it [00:24, 3160.23it/s]61058it [00:24, 3143.25it/s]61375it [00:24, 2998.44it/s]61723it [00:25, 3132.83it/s]62079it [00:25, 3253.15it/s]62390it [00:26, 417.32it/s] 62750it [00:26, 579.42it/s]63088it [00:26, 763.61it/s]63421it [00:26, 988.71it/s]63732it [00:26, 1224.79it/s]64032it [00:26, 1443.19it/s]64371it [00:26, 1757.61it/s]64744it [00:26, 2126.97it/s]65069it [00:26, 2339.99it/s]65439it [00:26, 2652.51it/s]62539it [00:26, 351.45it/s] 65775it [00:27, 2765.18it/s]62909it [00:27, 497.69it/s]66152it [00:27, 3021.87it/s]63213it [00:27, 647.33it/s]66494it [00:27, 3041.90it/s]63572it [00:27, 875.55it/s]66868it [00:27, 3229.13it/s]63928it [00:27, 1123.30it/s]67237it [00:27, 3357.47it/s]64298it [00:27, 1439.37it/s]67589it [00:27, 3292.85it/s]62445it [00:27, 333.80it/s] 64672it [00:27, 1782.43it/s]67969it [00:27, 3434.03it/s]62797it [00:27, 460.88it/s]65010it [00:27, 2026.95it/s]68322it [00:27, 3346.79it/s]63099it [00:27, 598.84it/s]65379it [00:27, 2354.64it/s]68692it [00:27, 3445.73it/s]63468it [00:27, 819.80it/s]65720it [00:27, 2521.51it/s]63835it [00:27, 1083.71it/s]69042it [00:28, 3346.86it/s]66094it [00:28, 2805.69it/s]69416it [00:28, 3457.46it/s]64161it [00:28, 1327.52it/s]66448it [00:28, 2876.52it/s]69781it [00:28, 3512.97it/s]64528it [00:28, 1660.02it/s]66820it [00:28, 3092.56it/s]70135it [00:28, 3416.54it/s]64862it [00:28, 1916.91it/s]67196it [00:28, 3269.79it/s]70508it [00:28, 3501.04it/s]65229it [00:28, 2253.46it/s]67550it [00:28, 3216.42it/s]65591it [00:28, 2546.45it/s]70860it [00:28, 3390.04it/s]67927it [00:28, 3368.35it/s]71230it [00:28, 3478.29it/s]65937it [00:28, 2695.43it/s]66302it [00:28, 2930.01it/s]68279it [00:28, 3250.35it/s]71580it [00:28, 3381.52it/s]68644it [00:28, 3359.11it/s]71954it [00:28, 3483.74it/s]66647it [00:28, 2971.09it/s]72323it [00:28, 3541.88it/s]68989it [00:28, 3283.57it/s]67019it [00:28, 3167.71it/s]62407it [00:28, 282.87it/s] 69337it [00:28, 3338.34it/s]62725it [00:28, 383.21it/s]67364it [00:28, 3032.30it/s]72679it [00:29, 3219.34it/s]69690it [00:29, 3392.05it/s]63085it [00:29, 534.87it/s]67723it [00:29, 3180.11it/s]73048it [00:29, 3347.44it/s]70033it [00:29, 3210.86it/s]68058it [00:29, 3185.42it/s]63374it [00:29, 668.49it/s]73389it [00:29, 3163.03it/s]70364it [00:29, 3237.32it/s]63680it [00:29, 862.24it/s]68388it [00:29, 3041.35it/s]73711it [00:29, 3101.58it/s]68711it [00:29, 3092.85it/s]70691it [00:29, 2937.38it/s]63958it [00:29, 1039.00it/s]74025it [00:29, 2897.62it/s]69027it [00:29, 3029.90it/s]64222it [00:29, 1231.71it/s]70992it [00:29, 2871.77it/s]74380it [00:29, 3071.74it/s]69395it [00:29, 3212.88it/s]64586it [00:29, 1597.89it/s]71331it [00:29, 3011.63it/s]74745it [00:29, 3231.99it/s]69761it [00:29, 3338.81it/s]64886it [00:29, 1847.74it/s]71637it [00:29, 3008.65it/s]75073it [00:29, 3171.56it/s]65249it [00:29, 2208.76it/s]70099it [00:29, 3275.86it/s]72013it [00:29, 3220.88it/s]75438it [00:29, 3306.40it/s]70471it [00:29, 3403.98it/s]72339it [00:29, 3160.86it/s]65608it [00:29, 2420.34it/s]75772it [00:30, 3231.58it/s]72715it [00:30, 3332.93it/s]70814it [00:30, 3298.77it/s]65974it [00:30, 2712.38it/s]76132it [00:30, 3337.06it/s]73098it [00:30, 3470.60it/s]71183it [00:30, 3410.94it/s]66339it [00:30, 2946.95it/s]76474it [00:30, 3359.32it/s]73448it [00:30, 3350.08it/s]71527it [00:30, 3301.92it/s]66678it [00:30, 2990.46it/s]76812it [00:30, 3143.80it/s]73823it [00:30, 3464.42it/s]71900it [00:30, 3422.23it/s]67047it [00:30, 3177.58it/s]77174it [00:30, 3277.45it/s]72255it [00:30, 3458.64it/s]74172it [00:30, 3340.97it/s]67389it [00:30, 3123.50it/s]77506it [00:30, 3190.20it/s]72603it [00:30, 3338.49it/s]67753it [00:30, 3264.27it/s]74509it [00:30, 3283.32it/s]77859it [00:30, 3283.76it/s]72972it [00:30, 3437.94it/s]68120it [00:30, 3378.94it/s]74848it [00:30, 3218.13it/s]78209it [00:30, 3129.41it/s]73318it [00:30, 3324.09it/s]68468it [00:30, 3306.34it/s]75217it [00:30, 3351.92it/s]78567it [00:30, 3252.91it/s]68841it [00:30, 3424.75it/s]73653it [00:30, 3313.13it/s]75585it [00:30, 3446.29it/s]78919it [00:31, 3328.09it/s]69189it [00:30, 3329.80it/s]74008it [00:30, 3264.52it/s]75932it [00:31, 3332.87it/s]79255it [00:31, 3156.37it/s]69559it [00:31, 3435.26it/s]74374it [00:31, 3376.83it/s]76267it [00:31, 3283.79it/s]79603it [00:31, 3244.63it/s]74744it [00:31, 3468.59it/s]69906it [00:31, 3357.03it/s]76597it [00:31, 3046.18it/s]79931it [00:31, 3183.49it/s]70285it [00:31, 3479.93it/s]75093it [00:31, 3342.70it/s]76950it [00:31, 3178.65it/s]80294it [00:31, 3309.62it/s]75429it [00:31, 3346.99it/s]70648it [00:31, 3383.87it/s]77299it [00:31, 3264.82it/s]80659it [00:31, 3398.95it/s]71014it [00:31, 3462.37it/s]75765it [00:31, 3266.72it/s]77629it [00:31, 3095.83it/s]81001it [00:31, 3299.02it/s]71385it [00:31, 3533.62it/s]76129it [00:31, 3372.74it/s]77977it [00:31, 3201.53it/s]81359it [00:31, 3379.58it/s]76484it [00:31, 3423.42it/s]71740it [00:31, 3390.88it/s]78301it [00:31, 3140.70it/s]81699it [00:31, 3271.60it/s]72097it [00:31, 3441.93it/s]76828it [00:31, 3286.32it/s]78654it [00:31, 3249.17it/s]82054it [00:31, 3349.88it/s]77159it [00:31, 3278.33it/s]78997it [00:31, 3299.73it/s]72443it [00:31, 3113.72it/s]82408it [00:32, 3180.06it/s]77488it [00:32, 3121.34it/s]72761it [00:32, 3096.60it/s]79329it [00:32, 3046.85it/s]82736it [00:32, 3207.48it/s]77803it [00:32, 3084.00it/s]73092it [00:32, 3154.78it/s]79639it [00:32, 3054.64it/s]83059it [00:32, 2996.09it/s]78113it [00:32, 2983.54it/s]73412it [00:32, 2954.45it/s]79948it [00:32, 2845.82it/s]83363it [00:32, 2675.19it/s]78413it [00:32, 2889.16it/s]73712it [00:32, 2881.70it/s]80238it [00:32, 2823.96it/s]83731it [00:32, 2934.04it/s]78773it [00:32, 3086.61it/s]74020it [00:32, 2936.00it/s]80607it [00:32, 3064.35it/s]79084it [00:32, 3038.79it/s]84088it [00:32, 2969.78it/s]74381it [00:32, 3126.37it/s]80918it [00:32, 3066.68it/s]79448it [00:32, 3210.83it/s]84456it [00:32, 3160.56it/s]74741it [00:32, 3262.14it/s]81275it [00:32, 3210.88it/s]79814it [00:32, 3339.84it/s]84822it [00:32, 3297.99it/s]75070it [00:32, 3186.02it/s]81599it [00:32, 3113.12it/s]80150it [00:32, 3230.66it/s]85158it [00:32, 3218.88it/s]75428it [00:32, 3297.91it/s]81961it [00:32, 3257.40it/s]80518it [00:32, 3348.11it/s]85518it [00:33, 3325.38it/s]75760it [00:33, 3225.14it/s]82320it [00:33, 3354.05it/s]80855it [00:33, 3252.81it/s]76120it [00:33, 3332.92it/s]82658it [00:33, 3257.45it/s]81211it [00:33, 3340.04it/s]76481it [00:33, 3413.02it/s]83020it [00:33, 3360.84it/s]81553it [00:33, 3362.76it/s]76824it [00:33, 3295.64it/s]83358it [00:33, 3264.26it/s]81891it [00:33, 3231.97it/s]77182it [00:33, 3376.95it/s]83719it [00:33, 3363.76it/s]82239it [00:33, 3302.82it/s]77522it [00:33, 3282.83it/s]84085it [00:33, 3448.17it/s]82571it [00:33, 3181.29it/s]77886it [00:33, 3383.63it/s]84432it [00:33, 3328.53it/s]82919it [00:33, 3266.33it/s]78226it [00:33, 3276.64it/s]84788it [00:33, 3375.00it/s]83248it [00:33, 3148.63it/s]78584it [00:33, 3362.48it/s]85127it [00:33, 3096.94it/s]83595it [00:33, 3237.95it/s]78951it [00:33, 3450.74it/s]85466it [00:34, 3174.35it/s]83949it [00:34, 3322.91it/s]79298it [00:34, 3340.37it/s]85788it [00:34, 3014.96it/s]84283it [00:34, 3220.63it/s]79666it [00:34, 3437.56it/s]84648it [00:34, 3338.86it/s]80012it [00:34, 3345.66it/s]84984it [00:34, 3243.29it/s]80383it [00:34, 3448.60it/s]85348it [00:34, 3355.93it/s]80730it [00:34, 3357.32it/s]85709it [00:34, 3428.95it/s]81099it [00:34, 3452.81it/s]81463it [00:34, 3506.91it/s]81815it [00:34, 3278.70it/s]82169it [00:34, 3349.37it/s]82507it [00:35, 3104.97it/s]82823it [00:35, 3060.91it/s]83147it [00:35, 3109.97it/s]83461it [00:35, 3045.92it/s]83836it [00:35, 3245.35it/s]84163it [00:35, 3193.65it/s]84538it [00:35, 3353.11it/s]84915it [00:35, 3473.00it/s]85264it [00:35, 3356.25it/s]85638it [00:35, 3465.93it/s]85855it [00:36, 305.48it/s] 86235it [00:36, 432.09it/s]86607it [00:36, 593.89it/s]86916it [00:36, 756.69it/s]87291it [00:37, 1012.97it/s]87618it [00:37, 1251.38it/s]87994it [00:37, 1587.23it/s]88359it [00:37, 1858.07it/s]88737it [00:37, 2207.33it/s]89108it [00:37, 2515.78it/s]89459it [00:37, 2659.30it/s]89808it [00:37, 2857.26it/s]90150it [00:37, 2853.84it/s]90475it [00:38, 2772.19it/s]90793it [00:38, 2874.55it/s]86054it [00:38, 307.76it/s] 91102it [00:38, 2799.03it/s]86397it [00:38, 420.77it/s]86094it [00:38, 250.11it/s] 91445it [00:38, 2966.76it/s]86689it [00:38, 543.60it/s]86455it [00:38, 356.09it/s]91754it [00:38, 2968.58it/s]87042it [00:38, 739.50it/s]86715it [00:38, 451.94it/s]92112it [00:38, 3139.68it/s]87404it [00:38, 986.16it/s]87067it [00:38, 629.62it/s]92469it [00:38, 3261.23it/s]87723it [00:38, 1219.88it/s]87424it [00:38, 854.20it/s]92801it [00:38, 3182.47it/s]88079it [00:38, 1533.33it/s]87731it [00:38, 1068.47it/s]93137it [00:38, 3231.24it/s]88406it [00:38, 1793.18it/s]88101it [00:38, 1391.48it/s]93464it [00:38, 3165.07it/s]88766it [00:38, 2127.67it/s]88425it [00:38, 1655.58it/s]93821it [00:39, 3280.32it/s]89110it [00:39, 2401.56it/s]88785it [00:39, 1994.48it/s]94169it [00:39, 3331.70it/s]89155it [00:39, 2334.39it/s]89446it [00:39, 2498.74it/s]94504it [00:39, 3240.13it/s]89770it [00:39, 2674.32it/s]89498it [00:39, 2491.59it/s]94854it [00:39, 3315.25it/s]89867it [00:39, 2771.32it/s]90090it [00:39, 2724.25it/s]95187it [00:39, 3210.78it/s]90424it [00:39, 2883.55it/s]90208it [00:39, 2833.64it/s]95538it [00:39, 3295.00it/s]90762it [00:39, 3017.53it/s]90564it [00:39, 3019.53it/s]95894it [00:39, 3371.62it/s]91085it [00:39, 2953.69it/s]90901it [00:39, 2990.75it/s]96233it [00:39, 3236.28it/s]91422it [00:39, 3068.24it/s]91258it [00:39, 3145.05it/s]96590it [00:39, 3331.59it/s]91607it [00:39, 3239.09it/s]91741it [00:39, 2953.39it/s]96925it [00:40, 3204.61it/s]91945it [00:40, 3155.46it/s]92086it [00:40, 3089.85it/s]97281it [00:40, 3304.62it/s]92296it [00:40, 3253.30it/s]92432it [00:40, 3193.23it/s]97614it [00:40, 3186.66it/s]92629it [00:40, 3162.82it/s]92757it [00:40, 3028.27it/s]97971it [00:40, 3294.81it/s]85987it [00:40, 261.83it/s] 92981it [00:40, 3262.12it/s]93085it [00:40, 3098.02it/s]98321it [00:40, 3351.90it/s]86351it [00:40, 364.72it/s]93340it [00:40, 3356.33it/s]93400it [00:40, 3055.78it/s]98658it [00:40, 3240.86it/s]86678it [00:40, 482.88it/s]93749it [00:40, 3174.63it/s]93680it [00:40, 3175.16it/s]98997it [00:40, 3281.88it/s]87024it [00:40, 649.27it/s]94096it [00:40, 3259.55it/s]94026it [00:40, 3255.33it/s]99327it [00:40, 3175.66it/s]87346it [00:40, 839.29it/s]94425it [00:40, 3045.84it/s]94356it [00:40, 3063.01it/s]99647it [00:40, 3154.58it/s]87654it [00:40, 993.10it/s]94737it [00:40, 3066.02it/s]99964it [00:40, 3102.34it/s]94667it [00:40, 2819.32it/s]87926it [00:40, 1181.06it/s]95047it [00:40, 2931.28it/s]94955it [00:41, 2811.01it/s]100275it [00:41, 2882.41it/s]88193it [00:41, 1368.29it/s]95344it [00:41, 2832.11it/s]100605it [00:41, 2997.63it/s]95241it [00:41, 2700.52it/s]88466it [00:41, 1590.75it/s]95693it [00:41, 3014.42it/s]100942it [00:41, 3101.64it/s]95586it [00:41, 2903.33it/s]88828it [00:41, 1974.41it/s]95998it [00:41, 2976.03it/s]101255it [00:41, 2994.77it/s]95918it [00:41, 2897.34it/s]89198it [00:41, 2261.10it/s]96348it [00:41, 3124.25it/s]101602it [00:41, 3128.04it/s]96268it [00:41, 3054.91it/s]89571it [00:41, 2593.75it/s]96699it [00:41, 3234.43it/s]101918it [00:41, 3081.33it/s]96614it [00:41, 3167.38it/s]89945it [00:41, 2873.61it/s]97025it [00:41, 3142.41it/s]102268it [00:41, 3201.07it/s]96934it [00:41, 3070.58it/s]90283it [00:41, 2935.24it/s]97380it [00:41, 3258.54it/s]102624it [00:41, 3304.80it/s]97280it [00:41, 3178.93it/s]90634it [00:41, 3087.40it/s]97708it [00:41, 3160.72it/s]102956it [00:41, 3188.92it/s]97601it [00:41, 3077.74it/s]90970it [00:41, 3061.24it/s]98058it [00:41, 3256.73it/s]103313it [00:42, 3297.96it/s]97948it [00:42, 3181.78it/s]91324it [00:42, 3191.99it/s]98409it [00:42, 3329.69it/s]103645it [00:42, 3208.76it/s]98295it [00:42, 3262.88it/s]91677it [00:42, 3285.91it/s]98744it [00:42, 3202.31it/s]104002it [00:42, 3310.71it/s]98623it [00:42, 3145.18it/s]92017it [00:42, 3184.55it/s]99095it [00:42, 3290.51it/s]104335it [00:42, 3209.41it/s]98969it [00:42, 3233.41it/s]92370it [00:42, 3280.89it/s]99426it [00:42, 3187.86it/s]104694it [00:42, 3317.35it/s]99295it [00:42, 3133.88it/s]92705it [00:42, 3185.89it/s]99775it [00:42, 3272.78it/s]105050it [00:42, 3386.50it/s]99641it [00:42, 3226.34it/s]93058it [00:42, 3283.00it/s]100118it [00:42, 3165.07it/s]105390it [00:42, 3136.83it/s]99984it [00:42, 3284.40it/s]93398it [00:42, 3181.59it/s]100468it [00:42, 3253.69it/s]105733it [00:42, 3218.22it/s]100314it [00:42, 3154.19it/s]93751it [00:42, 3278.80it/s]100820it [00:42, 3329.86it/s]100655it [00:42, 3225.85it/s]106059it [00:42, 3017.22it/s]94110it [00:42, 3366.31it/s]101155it [00:42, 3216.25it/s]106402it [00:43, 3130.50it/s]100980it [00:42, 3095.42it/s]94449it [00:42, 3217.49it/s]101503it [00:42, 3290.39it/s]106731it [00:43, 3173.84it/s]101322it [00:43, 3186.84it/s]94809it [00:43, 3323.71it/s]101834it [00:43, 3153.51it/s]101657it [00:43, 3231.91it/s]107052it [00:43, 2999.62it/s]102182it [00:43, 3245.41it/s]95144it [00:43, 3033.16it/s]101982it [00:43, 3116.77it/s]107398it [00:43, 3119.42it/s]102525it [00:43, 3297.03it/s]95456it [00:43, 3055.30it/s]102326it [00:43, 3207.97it/s]107714it [00:43, 3057.16it/s]95767it [00:43, 3068.61it/s]102857it [00:43, 3192.06it/s]108067it [00:43, 3189.56it/s]102649it [00:43, 3097.65it/s]103178it [00:43, 3130.02it/s]96078it [00:43, 2861.79it/s]108393it [00:43, 3208.10it/s]102983it [00:43, 3165.16it/s]103493it [00:43, 3033.28it/s]96417it [00:43, 3006.94it/s]103313it [00:43, 3203.22it/s]108716it [00:43, 2906.95it/s]103801it [00:43, 3043.88it/s]96723it [00:43, 2930.25it/s]103635it [00:43, 2960.85it/s]109013it [00:43, 2617.97it/s]104111it [00:43, 3059.05it/s]103937it [00:43, 2974.93it/s]97020it [00:43, 2627.24it/s]109309it [00:44, 2706.42it/s]104418it [00:43, 2903.64it/s]104239it [00:44, 2986.76it/s]97290it [00:44, 2522.67it/s]109587it [00:44, 2639.28it/s]104738it [00:44, 2985.90it/s]104540it [00:44, 2891.41it/s]97598it [00:44, 2613.49it/s]109932it [00:44, 2858.99it/s]105066it [00:44, 3069.13it/s]104880it [00:44, 3036.21it/s]97954it [00:44, 2868.90it/s]110224it [00:44, 2858.89it/s]105375it [00:44, 3018.92it/s]105186it [00:44, 3000.79it/s]98311it [00:44, 3062.26it/s]110574it [00:44, 3040.90it/s]105733it [00:44, 3181.31it/s]105541it [00:44, 3159.32it/s]98623it [00:44, 3029.15it/s]110927it [00:44, 3181.14it/s]106053it [00:44, 3112.31it/s]105895it [00:44, 3268.26it/s]98970it [00:44, 3154.44it/s]111249it [00:44, 3113.48it/s]106413it [00:44, 3251.55it/s]106224it [00:44, 3166.34it/s]99289it [00:44, 3089.47it/s]111604it [00:44, 3237.61it/s]106761it [00:44, 3316.51it/s]106580it [00:44, 3279.29it/s]99635it [00:44, 3194.12it/s]111930it [00:44, 3154.95it/s]107094it [00:44, 3220.92it/s]106910it [00:44, 3183.02it/s]99993it [00:44, 3305.22it/s]112281it [00:44, 3256.01it/s]107445it [00:44, 3302.26it/s]107267it [00:44, 3292.15it/s]100326it [00:44, 3201.19it/s]112633it [00:45, 3332.73it/s]107777it [00:44, 3217.84it/s]107624it [00:45, 3372.05it/s]100682it [00:45, 3303.22it/s]112968it [00:45, 3212.42it/s]108130it [00:45, 3307.56it/s]107963it [00:45, 3243.60it/s]101015it [00:45, 3201.94it/s]113321it [00:45, 3302.50it/s]108490it [00:45, 3390.59it/s]108321it [00:45, 3338.47it/s]101374it [00:45, 3311.41it/s]113653it [00:45, 3193.58it/s]108831it [00:45, 3265.53it/s]101725it [00:45, 3368.90it/s]108657it [00:45, 3101.52it/s]114004it [00:45, 3282.83it/s]109188it [00:45, 3352.68it/s]109006it [00:45, 3209.04it/s]102064it [00:45, 3240.25it/s]114353it [00:45, 3342.83it/s]109525it [00:45, 3228.12it/s]109350it [00:45, 3272.64it/s]102418it [00:45, 3324.91it/s]114689it [00:45, 3219.28it/s]109882it [00:45, 3323.22it/s]102753it [00:45, 3209.90it/s]109681it [00:45, 3076.94it/s]115039it [00:45, 3298.97it/s]110216it [00:45, 3206.89it/s]103110it [00:45, 3310.72it/s]110028it [00:45, 3184.07it/s]110573it [00:45, 3309.41it/s]103466it [00:45, 3382.00it/s]110351it [00:45, 3018.51it/s]110925it [00:45, 3368.53it/s]103806it [00:46, 3196.48it/s]110688it [00:46, 3115.88it/s]111264it [00:46, 3256.57it/s]104162it [00:46, 3299.04it/s]111029it [00:46, 3198.35it/s]111613it [00:46, 3321.86it/s]104495it [00:46, 3182.93it/s]111352it [00:46, 2951.38it/s]111947it [00:46, 3221.33it/s]104816it [00:46, 3105.94it/s]111690it [00:46, 3067.39it/s]112300it [00:46, 3309.32it/s]112644it [00:46, 3344.82it/s]105159it [00:46, 2964.94it/s]112002it [00:46, 2835.71it/s]105498it [00:46, 3078.55it/s]112338it [00:46, 2976.20it/s]112980it [00:46, 3124.33it/s]105823it [00:46, 3124.52it/s]112642it [00:46, 2952.17it/s]113296it [00:46, 3130.78it/s]106138it [00:46, 2862.00it/s]112942it [00:46, 2822.45it/s]113612it [00:46, 2964.29it/s]106436it [00:46, 2892.61it/s]113925it [00:46, 3009.34it/s]113228it [00:46, 2727.06it/s]106751it [00:46, 2962.68it/s]114254it [00:46, 3087.45it/s]113558it [00:47, 2748.92it/s]107051it [00:47, 2909.04it/s]114565it [00:47, 3042.54it/s]113917it [00:47, 2977.79it/s]107399it [00:47, 3069.18it/s]114922it [00:47, 3194.62it/s]114277it [00:47, 3152.11it/s]107709it [00:47, 3035.02it/s]115244it [00:47, 3109.44it/s]114596it [00:47, 3115.88it/s]108053it [00:47, 3150.90it/s]114955it [00:47, 3250.70it/s]108408it [00:47, 3267.24it/s]115283it [00:47, 3167.20it/s]108737it [00:47, 3176.17it/s]109085it [00:47, 3262.11it/s]109413it [00:47, 3174.44it/s]109762it [00:47, 3264.53it/s]110123it [00:48, 3363.29it/s]110461it [00:48, 3215.99it/s]110823it [00:48, 3330.03it/s]111158it [00:48, 3194.45it/s]111516it [00:48, 3303.94it/s]111868it [00:48, 3364.63it/s]112207it [00:48, 3237.20it/s]112560it [00:48, 3320.04it/s]112894it [00:48, 3200.62it/s]113249it [00:48, 3299.75it/s]113581it [00:49, 3188.42it/s]113936it [00:49, 3290.48it/s]114298it [00:49, 3383.47it/s]114638it [00:49, 3097.03it/s]114985it [00:49, 3199.56it/s]115371it [00:50, 226.26it/s] 115728it [00:50, 318.65it/s]116078it [00:50, 439.52it/s]116372it [00:50, 566.89it/s]116721it [00:50, 766.60it/s]117029it [00:51, 965.13it/s]117376it [00:51, 1245.39it/s]117726it [00:51, 1555.88it/s]118051it [00:51, 1797.62it/s]118401it [00:51, 2116.09it/s]118726it [00:51, 2294.80it/s]119074it [00:51, 2563.16it/s]119421it [00:51, 2783.86it/s]119752it [00:51, 2810.45it/s]120110it [00:52, 3011.74it/s]120440it [00:52, 2972.07it/s]120796it [00:52, 3130.90it/s]121151it [00:52, 3247.90it/s]121488it [00:52, 3104.34it/s]121828it [00:52, 3184.80it/s]122154it [00:52, 3025.43it/s]122475it [00:52, 3075.86it/s]122788it [00:52, 3072.51it/s]123099it [00:53, 2889.61it/s]115602it [00:52, 195.99it/s] 115557it [00:52, 183.29it/s] 123444it [00:53, 3044.72it/s]115947it [00:53, 276.48it/s]115912it [00:53, 262.92it/s]116224it [00:53, 361.56it/s]123753it [00:53, 2986.06it/s]116191it [00:53, 345.52it/s]116572it [00:53, 506.49it/s]124096it [00:53, 3110.87it/s]116550it [00:53, 490.25it/s]116927it [00:53, 696.01it/s]124441it [00:53, 3206.54it/s]116902it [00:53, 671.77it/s]117237it [00:53, 888.46it/s]124764it [00:53, 3101.93it/s]117214it [00:53, 860.20it/s]117570it [00:53, 1142.51it/s]125114it [00:53, 3215.62it/s]117563it [00:53, 1125.00it/s]117884it [00:53, 1373.46it/s]125438it [00:53, 3101.67it/s]117882it [00:53, 1373.02it/s]118222it [00:53, 1681.02it/s]125798it [00:53, 3242.48it/s]118238it [00:53, 1703.75it/s]118560it [00:53, 1986.37it/s]126149it [00:53, 3318.70it/s]118596it [00:53, 2036.74it/s]118880it [00:53, 2166.54it/s]126483it [00:54, 3190.63it/s]118932it [00:53, 2245.21it/s]119229it [00:54, 2457.56it/s]126828it [00:54, 3262.95it/s]119286it [00:54, 2529.26it/s]119549it [00:54, 2575.81it/s]127157it [00:54, 3148.57it/s]119619it [00:54, 2637.62it/s]119902it [00:54, 2813.26it/s]127507it [00:54, 3247.90it/s]119974it [00:54, 2862.81it/s]120248it [00:54, 2981.80it/s]127856it [00:54, 3316.66it/s]120329it [00:54, 3041.67it/s]120578it [00:54, 2915.35it/s]128190it [00:54, 3162.98it/s]120668it [00:54, 3011.32it/s]120928it [00:54, 3073.15it/s]128538it [00:54, 3252.64it/s]121014it [00:54, 3131.23it/s]121253it [00:54, 3046.35it/s]128866it [00:54, 3126.03it/s]121345it [00:54, 3067.95it/s]121610it [00:54, 3191.53it/s]129215it [00:54, 3228.21it/s]121665it [00:54, 3080.71it/s]121970it [00:54, 3306.33it/s]129563it [00:55, 3300.04it/s]122021it [00:54, 3214.53it/s]122308it [00:55, 3205.34it/s]129895it [00:55, 3165.21it/s]122350it [00:55, 3139.45it/s]122669it [00:55, 3318.92it/s]130237it [00:55, 3236.61it/s]122709it [00:55, 3267.64it/s]123006it [00:55, 3224.82it/s]130563it [00:55, 3146.10it/s]123041it [00:55, 3156.92it/s]123356it [00:55, 3293.03it/s]130896it [00:55, 3196.99it/s]123370it [00:55, 3147.85it/s]131235it [00:55, 3251.55it/s]123706it [00:55, 3175.09it/s]123690it [00:55, 3160.99it/s]115310it [00:55, 182.03it/s] 124027it [00:55, 3180.53it/s]131562it [00:55, 2994.39it/s]124008it [00:55, 2955.15it/s]115612it [00:55, 245.53it/s]124348it [00:55, 3165.68it/s]131868it [00:55, 3012.55it/s]124319it [00:55, 2997.09it/s]115918it [00:55, 332.71it/s]124666it [00:55, 2953.15it/s]132173it [00:55, 2877.60it/s]124622it [00:55, 2833.84it/s]116186it [00:55, 430.13it/s]125008it [00:55, 3081.78it/s]132516it [00:55, 3029.45it/s]124969it [00:55, 3007.96it/s]116534it [00:55, 603.38it/s]125351it [00:56, 3178.96it/s]132870it [00:56, 3172.48it/s]125321it [00:56, 3151.21it/s]116881it [00:56, 818.59it/s]125672it [00:56, 3071.40it/s]133191it [00:56, 3098.09it/s]125640it [00:56, 3066.61it/s]117186it [00:56, 1030.42it/s]126019it [00:56, 3184.01it/s]133549it [00:56, 3234.42it/s]126000it [00:56, 3218.21it/s]117546it [00:56, 1341.25it/s]126340it [00:56, 3090.02it/s]133875it [00:56, 3127.42it/s]126325it [00:56, 3113.97it/s]117865it [00:56, 1598.32it/s]126683it [00:56, 3185.55it/s]134232it [00:56, 3252.05it/s]126682it [00:56, 3242.79it/s]118223it [00:56, 1940.22it/s]127021it [00:56, 3241.52it/s]134581it [00:56, 3319.74it/s]127038it [00:56, 3332.42it/s]118584it [00:56, 2269.77it/s]127347it [00:56, 3117.00it/s]134915it [00:56, 3207.47it/s]127374it [00:56, 3214.12it/s]118921it [00:56, 2441.57it/s]127671it [00:56, 3151.35it/s]135265it [00:56, 3289.97it/s]127729it [00:56, 3308.02it/s]119277it [00:56, 2702.23it/s]127988it [00:56, 2980.43it/s]135596it [00:56, 3200.15it/s]119611it [00:56, 2782.16it/s]128062it [00:56, 3191.72it/s]128330it [00:56, 3103.42it/s]135918it [00:57, 3153.58it/s]119963it [00:56, 2972.52it/s]128414it [00:56, 3284.85it/s]128643it [00:57, 3103.36it/s]136275it [00:57, 3272.85it/s]120321it [00:57, 3134.36it/s]128748it [00:57, 3175.45it/s]136604it [00:57, 3171.67it/s]128956it [00:57, 2932.40it/s]120661it [00:57, 3084.95it/s]129106it [00:57, 3288.92it/s]136961it [00:57, 3283.92it/s]129300it [00:57, 3073.26it/s]121017it [00:57, 3214.85it/s]129456it [00:57, 3349.07it/s]137291it [00:57, 3180.69it/s]129611it [00:57, 2921.52it/s]129793it [00:57, 3227.98it/s]121353it [00:57, 3125.02it/s]137649it [00:57, 3293.60it/s]129939it [00:57, 3020.82it/s]130142it [00:57, 3302.81it/s]121705it [00:57, 3235.28it/s]130286it [00:57, 3139.86it/s]137988it [00:57, 3142.28it/s]130474it [00:57, 3187.91it/s]122037it [00:57, 3119.60it/s]138346it [00:57, 3263.54it/s]130603it [00:57, 2934.00it/s]130823it [00:57, 3272.45it/s]122393it [00:57, 3242.68it/s]138701it [00:57, 3345.14it/s]130946it [00:57, 3069.43it/s]131180it [00:57, 3356.96it/s]122752it [00:57, 3339.57it/s]139038it [00:57, 3196.11it/s]123091it [00:57, 3240.05it/s]131518it [00:57, 3210.68it/s]131266it [00:57, 2919.03it/s]139397it [00:58, 3306.72it/s]131874it [00:58, 3307.47it/s]131591it [00:58, 3008.13it/s]123419it [00:58, 3168.54it/s]139731it [00:58, 3138.41it/s]131912it [00:58, 3064.43it/s]123739it [00:58, 3032.22it/s]132207it [00:58, 3077.15it/s]140062it [00:58, 3185.06it/s]124083it [00:58, 3144.94it/s]132548it [00:58, 3167.45it/s]132222it [00:58, 2763.34it/s]140398it [00:58, 3233.88it/s]132872it [00:58, 3185.57it/s]124400it [00:58, 3130.87it/s]132506it [00:58, 2734.24it/s]140724it [00:58, 3004.60it/s]124715it [00:58, 2971.30it/s]133194it [00:58, 2987.51it/s]132785it [00:58, 2484.69it/s]141043it [00:58, 3055.05it/s]125030it [00:58, 3019.18it/s]133497it [00:58, 2979.57it/s]133040it [00:58, 2249.48it/s]141352it [00:58, 2915.97it/s]125334it [00:58, 3022.97it/s]133798it [00:58, 2790.38it/s]133373it [00:58, 2519.90it/s]141698it [00:58, 3065.89it/s]125638it [00:58, 3001.14it/s]134148it [00:58, 2983.49it/s]133727it [00:58, 2787.94it/s]142049it [00:58, 3191.89it/s]125995it [00:58, 3164.90it/s]134493it [00:58, 3112.77it/s]134017it [00:58, 2811.43it/s]142372it [00:59, 3125.06it/s]126313it [00:58, 3089.16it/s]134809it [00:59, 3035.49it/s]134371it [00:59, 3014.85it/s]142718it [00:59, 3219.00it/s]126662it [00:59, 3205.40it/s]135158it [00:59, 3163.22it/s]134680it [00:59, 2991.83it/s]127019it [00:59, 3311.45it/s]143042it [00:59, 3133.00it/s]135478it [00:59, 3006.29it/s]135028it [00:59, 3129.74it/s]143391it [00:59, 3234.97it/s]127352it [00:59, 3190.62it/s]135830it [00:59, 3148.90it/s]135376it [00:59, 3230.92it/s]143749it [00:59, 3332.74it/s]127713it [00:59, 3309.05it/s]136183it [00:59, 3256.92it/s]135703it [00:59, 3136.70it/s]144084it [00:59, 3207.32it/s]128046it [00:59, 3205.40it/s]136512it [00:59, 3159.54it/s]136056it [00:59, 3248.05it/s]144442it [00:59, 3312.48it/s]128403it [00:59, 3307.79it/s]136865it [00:59, 3262.96it/s]136384it [00:59, 3120.58it/s]144775it [00:59, 3214.91it/s]128748it [00:59, 3212.03it/s]137194it [00:59, 3154.94it/s]136736it [00:59, 3234.00it/s]145132it [00:59, 3314.77it/s]129102it [00:59, 3304.15it/s]137547it [00:59, 3260.37it/s]137088it [00:59, 3315.02it/s]145486it [01:00, 3378.88it/s]129463it [00:59, 3392.20it/s]137898it [00:59, 3332.55it/s]137422it [01:00, 3196.92it/s]145826it [01:00, 3269.39it/s]129804it [01:00, 3142.41it/s]138233it [01:00, 3209.54it/s]137765it [01:00, 3262.36it/s]146177it [01:00, 3337.42it/s]130150it [01:00, 3229.11it/s]138583it [01:00, 3290.64it/s]138093it [01:00, 3183.00it/s]146513it [01:00, 3217.86it/s]130477it [01:00, 3029.63it/s]138914it [01:00, 3161.96it/s]138444it [01:00, 3274.84it/s]146862it [01:00, 3294.06it/s]130823it [01:00, 3147.93it/s]139268it [01:00, 3267.41it/s]138796it [01:00, 3343.87it/s]147219it [01:00, 3373.47it/s]131159it [01:00, 3207.35it/s]139618it [01:00, 3332.93it/s]139132it [01:00, 3202.44it/s]147558it [01:00, 3213.24it/s]131484it [01:00, 3026.57it/s]139953it [01:00, 3197.64it/s]139483it [01:00, 3288.41it/s]147916it [01:00, 3317.19it/s]131825it [01:00, 3131.98it/s]140304it [01:00, 3285.93it/s]139814it [01:00, 3178.35it/s]148250it [01:00, 3179.19it/s]140635it [01:00, 3176.94it/s]132142it [01:00, 2967.82it/s]140166it [01:00, 3275.10it/s]148606it [01:00, 3286.42it/s]140977it [01:00, 3244.18it/s]132467it [01:00, 3045.87it/s]140504it [01:00, 3302.92it/s]148937it [01:01, 3035.13it/s]141319it [01:01, 3293.64it/s]132802it [01:01, 3130.46it/s]140836it [01:01, 3168.18it/s]149286it [01:01, 3158.59it/s]141650it [01:01, 3116.71it/s]133118it [01:01, 2971.42it/s]141171it [01:01, 3219.44it/s]149609it [01:01, 3174.79it/s]141977it [01:01, 3159.12it/s]133464it [01:01, 3106.33it/s]141495it [01:01, 3021.61it/s]149930it [01:01, 3034.70it/s]133778it [01:01, 3012.11it/s]142295it [01:01, 2992.24it/s]141801it [01:01, 2946.37it/s]150243it [01:01, 3061.26it/s]142598it [01:01, 2986.43it/s]134082it [01:01, 2760.50it/s]142098it [01:01, 2928.24it/s]150552it [01:01, 3043.18it/s]142899it [01:01, 2939.00it/s]134368it [01:01, 2785.19it/s]142393it [01:01, 2805.18it/s]150858it [01:01, 2833.11it/s]143195it [01:01, 2826.45it/s]134651it [01:01, 2704.26it/s]142722it [01:01, 2940.23it/s]151175it [01:01, 2924.59it/s]143545it [01:01, 3014.14it/s]135004it [01:01, 2932.50it/s]143027it [01:01, 2837.26it/s]151471it [01:01, 2894.46it/s]143868it [01:01, 2976.98it/s]135360it [01:01, 3110.32it/s]143381it [01:01, 3032.96it/s]151827it [01:02, 3082.82it/s]144218it [01:01, 3121.36it/s]135675it [01:02, 3046.41it/s]143742it [01:02, 3196.21it/s]152180it [01:02, 3210.76it/s]144571it [01:02, 3236.83it/s]136026it [01:02, 3178.67it/s]144065it [01:02, 3127.72it/s]144897it [01:02, 3141.01it/s]136347it [01:02, 3076.87it/s]144416it [01:02, 3236.03it/s]145251it [01:02, 3254.92it/s]136698it [01:02, 3200.47it/s]144742it [01:02, 3162.33it/s]145579it [01:02, 3158.79it/s]137049it [01:02, 3287.72it/s]145096it [01:02, 3269.03it/s]145935it [01:02, 3268.23it/s]137380it [01:02, 3166.81it/s]145447it [01:02, 3337.28it/s]146293it [01:02, 3357.32it/s]137732it [01:02, 3267.63it/s]145782it [01:02, 3191.78it/s]146631it [01:02, 3193.41it/s]138061it [01:02, 3150.62it/s]146137it [01:02, 3292.55it/s]146992it [01:02, 3309.82it/s]138411it [01:02, 3250.10it/s]146469it [01:02, 3076.08it/s]147326it [01:02, 3198.06it/s]138765it [01:02, 3333.14it/s]146829it [01:03, 3219.96it/s]147688it [01:03, 3317.04it/s]139100it [01:03, 3128.45it/s]147189it [01:03, 3325.68it/s]148050it [01:03, 3404.24it/s]139454it [01:03, 3242.58it/s]147525it [01:03, 3189.51it/s]148393it [01:03, 3246.51it/s]139782it [01:03, 3074.49it/s]147868it [01:03, 3255.35it/s]148741it [01:03, 3310.82it/s]140128it [01:03, 3179.90it/s]148197it [01:03, 3123.68it/s]149075it [01:03, 3179.84it/s]140468it [01:03, 3241.06it/s]148529it [01:03, 3178.69it/s]149431it [01:03, 3285.80it/s]140795it [01:03, 3145.96it/s]148885it [01:03, 3287.59it/s]149762it [01:03, 3187.15it/s]141147it [01:03, 3252.29it/s]149216it [01:03, 3181.29it/s]150107it [01:03, 3260.50it/s]141475it [01:03, 3146.64it/s]149575it [01:03, 3296.45it/s]150455it [01:03, 3322.44it/s]141821it [01:03, 3234.75it/s]149907it [01:03, 3189.46it/s]150789it [01:03, 3199.24it/s]142174it [01:04, 3318.37it/s]150262it [01:04, 3291.44it/s]151125it [01:04, 3243.28it/s]142508it [01:04, 3164.03it/s]150593it [01:04, 3075.87it/s]142835it [01:04, 3192.86it/s]151451it [01:04, 2977.31it/s]150939it [01:04, 3182.52it/s]151778it [01:04, 3053.55it/s]151261it [01:04, 3153.03it/s]143157it [01:04, 2874.96it/s]152092it [01:04, 3076.48it/s]151579it [01:04, 2959.00it/s]143452it [01:04, 2722.56it/s]151883it [01:04, 2979.35it/s]143742it [01:04, 2769.39it/s]152224it [01:04, 3099.07it/s]144024it [01:04, 2703.13it/s]144359it [01:04, 2879.96it/s]144708it [01:04, 2898.27it/s]145058it [01:05, 3054.39it/s]145407it [01:05, 3177.15it/s]145728it [01:05, 3127.53it/s]146092it [01:05, 3272.63it/s]146422it [01:05, 3181.63it/s]146779it [01:05, 3292.50it/s]147134it [01:05, 3366.89it/s]147473it [01:05, 3191.79it/s]147817it [01:05, 3261.15it/s]148146it [01:05, 3082.60it/s]148498it [01:06, 3204.16it/s]148848it [01:06, 3286.36it/s]149180it [01:06, 3109.67it/s]149523it [01:06, 3199.27it/s]149846it [01:06, 3022.39it/s]150198it [01:06, 3160.22it/s]150547it [01:06, 3253.37it/s]150876it [01:06, 3127.09it/s]151233it [01:06, 3250.54it/s]151561it [01:07, 3072.16it/s]151905it [01:07, 3128.00it/s]152221it [01:07, 3129.69it/s]152504it [01:09, 152.13it/s] 152839it [01:09, 214.04it/s]153172it [01:09, 297.88it/s]153454it [01:09, 389.70it/s]153784it [01:09, 535.31it/s]154075it [01:09, 688.69it/s]154414it [01:09, 922.04it/s]154764it [01:09, 1205.04it/s]155078it [01:09, 1426.06it/s]155424it [01:10, 1748.00it/s]155736it [01:10, 1939.33it/s]156035it [01:10, 2138.09it/s]156332it [01:10, 2322.78it/s]156628it [01:10, 2285.17it/s]156949it [01:10, 2507.89it/s]157284it [01:10, 2723.96it/s]157586it [01:10, 2610.99it/s]152403it [01:10, 161.17it/s] 157939it [01:10, 2852.15it/s]152744it [01:10, 229.29it/s]158243it [01:11, 2893.56it/s]153104it [01:11, 327.30it/s]158610it [01:11, 3110.17it/s]153391it [01:11, 426.98it/s]158980it [01:11, 3277.70it/s]153749it [01:11, 596.27it/s]159317it [01:11, 3233.05it/s]154052it [01:11, 764.06it/s]159666it [01:11, 3306.36it/s]154396it [01:11, 1009.61it/s]160002it [01:11, 3153.02it/s]154726it [01:11, 1274.93it/s]160344it [01:11, 3227.80it/s]152537it [01:11, 150.64it/s] 155042it [01:11, 1499.36it/s]160696it [01:11, 3309.48it/s]152874it [01:11, 213.58it/s]155374it [01:11, 1798.88it/s]161030it [01:11, 3112.67it/s]153176it [01:11, 288.61it/s]161382it [01:12, 3225.62it/s]155697it [01:11, 1976.88it/s]153517it [01:11, 404.77it/s]156040it [01:12, 2276.11it/s]153865it [01:12, 559.99it/s]161709it [01:12, 3066.70it/s]156383it [01:12, 2537.83it/s]154167it [01:12, 717.18it/s]162067it [01:12, 3207.50it/s]154519it [01:12, 960.22it/s]156702it [01:12, 2568.17it/s]162419it [01:12, 3107.72it/s]157055it [01:12, 2808.59it/s]154856it [01:12, 1204.97it/s]162787it [01:12, 3265.47it/s]157377it [01:12, 2877.99it/s]155219it [01:12, 1529.63it/s]163156it [01:12, 3384.63it/s]157733it [01:12, 3062.48it/s]155573it [01:12, 1852.07it/s]163498it [01:12, 3310.34it/s]158096it [01:12, 3219.66it/s]155904it [01:12, 2088.84it/s]163868it [01:12, 3419.39it/s]158434it [01:12, 3141.44it/s]156246it [01:12, 2364.99it/s]164213it [01:12, 3272.39it/s]158783it [01:12, 3238.46it/s]164565it [01:12, 3341.44it/s]156575it [01:12, 2427.77it/s]159116it [01:12, 3064.85it/s]164921it [01:13, 3404.22it/s]156919it [01:13, 2665.99it/s]159430it [01:13, 3064.03it/s]157238it [01:13, 2727.20it/s]165264it [01:13, 2677.40it/s]159746it [01:13, 3089.40it/s]157548it [01:13, 2623.16it/s]165576it [01:13, 2784.92it/s]160059it [01:13, 2850.26it/s]157848it [01:13, 2718.78it/s]152536it [01:13, 172.52it/s] 165875it [01:13, 2710.75it/s]160381it [01:13, 2951.51it/s]158160it [01:13, 2825.00it/s]152850it [01:13, 237.95it/s]166218it [01:13, 2898.41it/s]160719it [01:13, 3071.82it/s]158458it [01:13, 2817.04it/s]153177it [01:13, 329.52it/s]166567it [01:13, 3057.74it/s]161031it [01:13, 3059.13it/s]158819it [01:13, 3038.37it/s]153520it [01:13, 458.70it/s]166883it [01:13, 3036.38it/s]161392it [01:13, 3216.08it/s]159132it [01:13, 3000.03it/s]153870it [01:13, 629.51it/s]167240it [01:13, 3186.00it/s]161717it [01:13, 3098.49it/s]159483it [01:13, 3145.20it/s]154176it [01:13, 797.99it/s]167565it [01:14, 3069.92it/s]162073it [01:13, 3229.60it/s]159828it [01:13, 3230.86it/s]154517it [01:13, 1045.01it/s]167923it [01:14, 3212.72it/s]162417it [01:14, 3055.60it/s]160156it [01:14, 3061.03it/s]154857it [01:14, 1291.47it/s]168249it [01:14, 3185.34it/s]162776it [01:14, 3203.70it/s]160504it [01:14, 3177.26it/s]155203it [01:14, 1599.26it/s]168571it [01:14, 3121.45it/s]163129it [01:14, 3293.72it/s]160826it [01:14, 3098.18it/s]155531it [01:14, 1882.40it/s]168913it [01:14, 3206.09it/s]163462it [01:14, 3157.32it/s]161156it [01:14, 3149.62it/s]155848it [01:14, 2053.44it/s]169236it [01:14, 3061.87it/s]163804it [01:14, 3230.20it/s]161504it [01:14, 3242.68it/s]156204it [01:14, 2370.50it/s]169591it [01:14, 3197.98it/s]164130it [01:14, 3053.79it/s]161831it [01:14, 3061.71it/s]156537it [01:14, 2519.30it/s]169950it [01:14, 3309.02it/s]164495it [01:14, 3218.28it/s]162193it [01:14, 3218.58it/s]156887it [01:14, 2749.18it/s]170284it [01:14, 3193.06it/s]164867it [01:14, 3356.44it/s]162519it [01:14, 3142.14it/s]157230it [01:14, 2923.33it/s]170629it [01:14, 3264.86it/s]165206it [01:14, 3283.18it/s]162885it [01:14, 3287.53it/s]157558it [01:14, 2945.88it/s]170958it [01:15, 3214.85it/s]165576it [01:15, 3400.53it/s]163247it [01:15, 3382.99it/s]157920it [01:15, 3127.68it/s]171326it [01:15, 3348.48it/s]165919it [01:15, 3315.28it/s]163588it [01:15, 3295.11it/s]158252it [01:15, 3046.04it/s]171663it [01:15, 3227.86it/s]166277it [01:15, 3390.82it/s]163931it [01:15, 3332.47it/s]158614it [01:15, 3203.24it/s]172022it [01:15, 3330.87it/s]164266it [01:15, 3190.24it/s]166618it [01:15, 3210.64it/s]158971it [01:15, 3305.97it/s]172369it [01:15, 3370.93it/s]164620it [01:15, 3288.04it/s]166977it [01:15, 3316.84it/s]159310it [01:15, 3174.15it/s]172708it [01:15, 3248.04it/s]167350it [01:15, 3433.11it/s]164951it [01:15, 3232.14it/s]159661it [01:15, 3268.39it/s]173079it [01:15, 3379.22it/s]165317it [01:15, 3353.12it/s]167696it [01:15, 3338.73it/s]159994it [01:15, 3170.70it/s]173419it [01:15, 3290.36it/s]165689it [01:15, 3458.78it/s]168071it [01:15, 3455.39it/s]160343it [01:15, 3259.32it/s]173773it [01:15, 3360.30it/s]166037it [01:15, 3266.28it/s]168419it [01:15, 3273.56it/s]160695it [01:15, 3331.63it/s]174124it [01:16, 3403.41it/s]166375it [01:16, 3296.32it/s]168753it [01:15, 3289.36it/s]161031it [01:16, 2949.11it/s]174466it [01:16, 3100.69it/s]169085it [01:16, 3183.22it/s]166707it [01:16, 2994.44it/s]174782it [01:16, 3115.66it/s]161336it [01:16, 2920.86it/s]167036it [01:16, 3073.21it/s]169406it [01:16, 2959.93it/s]175098it [01:16, 2922.69it/s]161635it [01:16, 2791.95it/s]167358it [01:16, 3112.40it/s]169706it [01:16, 2850.24it/s]161947it [01:16, 2879.91it/s]175395it [01:16, 2856.77it/s]167674it [01:16, 2996.09it/s]169994it [01:16, 2799.01it/s]162294it [01:16, 3043.64it/s]175752it [01:16, 3052.40it/s]168003it [01:16, 3076.62it/s]170327it [01:16, 2940.92it/s]162603it [01:16, 2959.09it/s]176062it [01:16, 2952.95it/s]168314it [01:16, 2949.57it/s]170662it [01:16, 3053.80it/s]162953it [01:16, 3111.77it/s]176400it [01:16, 3071.12it/s]168665it [01:16, 3104.84it/s]170970it [01:16, 2940.21it/s]163268it [01:16, 3020.84it/s]176711it [01:16, 3015.71it/s]169032it [01:16, 3265.71it/s]171338it [01:16, 3147.48it/s]163636it [01:16, 3207.20it/s]177067it [01:16, 3170.98it/s]169362it [01:16, 3137.50it/s]171657it [01:16, 3069.21it/s]163983it [01:16, 3282.17it/s]177411it [01:17, 3248.00it/s]169708it [01:17, 3227.59it/s]172008it [01:17, 3193.43it/s]164314it [01:17, 3142.22it/s]177738it [01:17, 3088.91it/s]172349it [01:17, 3254.46it/s]170034it [01:17, 3037.51it/s]164657it [01:17, 3217.29it/s]178088it [01:17, 3203.67it/s]170380it [01:17, 3154.29it/s]172677it [01:17, 3086.24it/s]164981it [01:17, 3045.00it/s]178411it [01:17, 3018.90it/s]170728it [01:17, 3245.63it/s]173024it [01:17, 3194.36it/s]165331it [01:17, 3170.89it/s]178758it [01:17, 3142.60it/s]171056it [01:17, 3041.07it/s]173346it [01:17, 3036.22it/s]165680it [01:17, 3260.66it/s]179092it [01:17, 3198.68it/s]171404it [01:17, 3161.94it/s]173689it [01:17, 3145.53it/s]166009it [01:17, 3173.26it/s]179415it [01:17, 3014.74it/s]174041it [01:17, 3251.68it/s]171725it [01:17, 3003.96it/s]166360it [01:17, 3269.45it/s]179757it [01:17, 3127.36it/s]172076it [01:17, 3143.56it/s]174369it [01:17, 3072.81it/s]166689it [01:17, 3180.78it/s]180074it [01:17, 2946.90it/s]172419it [01:17, 3223.38it/s]174727it [01:17, 3213.53it/s]167042it [01:17, 3280.29it/s]180414it [01:18, 3071.84it/s]172745it [01:18, 3062.02it/s]175052it [01:18, 3034.77it/s]167394it [01:18, 3348.21it/s]180759it [01:18, 3177.24it/s]173095it [01:18, 3183.83it/s]175400it [01:18, 3158.37it/s]167731it [01:18, 3236.55it/s]181080it [01:18, 3006.29it/s]175748it [01:18, 3247.46it/s]173417it [01:18, 3028.45it/s]168087it [01:18, 3320.59it/s]181424it [01:18, 3126.64it/s]173759it [01:18, 3136.91it/s]176076it [01:18, 3052.04it/s]168421it [01:18, 3225.82it/s]174107it [01:18, 3224.84it/s]181741it [01:18, 2702.74it/s]176419it [01:18, 3156.14it/s]168765it [01:18, 3285.16it/s]182104it [01:18, 2942.12it/s]174433it [01:18, 3072.87it/s]169124it [01:18, 3372.24it/s]176739it [01:18, 3016.76it/s]182436it [01:18, 3042.01it/s]174778it [01:18, 3178.08it/s]177085it [01:18, 3138.39it/s]169463it [01:18, 3167.29it/s]182750it [01:18, 2929.43it/s]177415it [01:18, 3182.93it/s]169790it [01:18, 3196.19it/s]175099it [01:18, 2972.58it/s]183072it [01:18, 3009.12it/s]175437it [01:18, 3075.95it/s]170112it [01:18, 2921.47it/s]177736it [01:18, 2788.67it/s]183388it [01:19, 3049.27it/s]175749it [01:19, 3063.25it/s]170418it [01:19, 2958.16it/s]178046it [01:19, 2868.93it/s]183698it [01:19, 2763.82it/s]176058it [01:19, 2842.21it/s]170730it [01:19, 3002.17it/s]178366it [01:19, 2959.45it/s]184008it [01:19, 2852.63it/s]176349it [01:19, 2859.44it/s]178669it [01:19, 2769.43it/s]171034it [01:19, 2792.60it/s]176692it [01:19, 3017.62it/s]184300it [01:19, 2755.80it/s]179016it [01:19, 2957.96it/s]171370it [01:19, 2946.25it/s]184655it [01:19, 2973.09it/s]176997it [01:19, 2844.31it/s]179319it [01:19, 2861.99it/s]171670it [01:19, 2826.65it/s]185008it [01:19, 3129.49it/s]177346it [01:19, 3022.39it/s]179670it [01:19, 3039.87it/s]172014it [01:19, 2994.60it/s]185326it [01:19, 2943.57it/s]177653it [01:19, 2908.09it/s]180009it [01:19, 3138.76it/s]172357it [01:19, 3112.26it/s]185655it [01:19, 3039.74it/s]177998it [01:19, 3059.14it/s]180327it [01:19, 3028.61it/s]172672it [01:19, 2973.91it/s]178352it [01:19, 3193.95it/s]185964it [01:19, 2929.92it/s]180662it [01:19, 3118.22it/s]173013it [01:19, 3094.37it/s]186303it [01:20, 3057.90it/s]178675it [01:20, 3025.78it/s]180977it [01:19, 3000.64it/s]173337it [01:20, 2958.07it/s]186655it [01:20, 3188.09it/s]179017it [01:20, 3135.30it/s]181313it [01:20, 3101.59it/s]173675it [01:20, 3074.09it/s]186977it [01:20, 3013.15it/s]181656it [01:20, 3195.53it/s]179334it [01:20, 2966.34it/s]174008it [01:20, 3146.29it/s]187322it [01:20, 3135.35it/s]179671it [01:20, 3077.27it/s]181978it [01:20, 3009.06it/s]174326it [01:20, 2971.27it/s]180009it [01:20, 3162.28it/s]187639it [01:20, 2957.24it/s]182326it [01:20, 3139.42it/s]174672it [01:20, 3107.51it/s]187982it [01:20, 3086.95it/s]180329it [01:20, 3019.17it/s]175009it [01:20, 3180.71it/s]182644it [01:20, 2999.86it/s]188323it [01:20, 3177.10it/s]180664it [01:20, 3112.01it/s]182990it [01:20, 3127.01it/s]175330it [01:20, 2986.64it/s]188644it [01:20, 3018.63it/s]183329it [01:20, 3201.86it/s]180978it [01:20, 2993.72it/s]175666it [01:20, 3089.97it/s]188980it [01:20, 3113.50it/s]181317it [01:20, 3092.53it/s]183652it [01:20, 3041.87it/s]175979it [01:20, 2940.08it/s]181671it [01:20, 3218.47it/s]189299it [01:21, 2977.14it/s]183996it [01:20, 3153.43it/s]176321it [01:20, 3072.92it/s]189653it [01:21, 3132.80it/s]181995it [01:21, 3105.85it/s]184315it [01:21, 3082.32it/s]176671it [01:21, 3193.73it/s]190023it [01:21, 3293.82it/s]182366it [01:21, 3278.09it/s]184686it [01:21, 3260.61it/s]176994it [01:21, 3087.14it/s]182697it [01:21, 3259.44it/s]190356it [01:21, 3250.32it/s]185063it [01:21, 3405.68it/s]177366it [01:21, 3266.31it/s]183051it [01:21, 3338.98it/s]190711it [01:21, 3335.41it/s]185406it [01:21, 3278.54it/s]177696it [01:21, 3135.02it/s]183409it [01:21, 3408.93it/s]191047it [01:21, 3185.74it/s]185764it [01:21, 3363.65it/s]178060it [01:21, 3275.79it/s]191396it [01:21, 3270.88it/s]183751it [01:21, 3222.76it/s]186103it [01:21, 3178.55it/s]178391it [01:21, 3095.67it/s]191747it [01:21, 3337.38it/s]184102it [01:21, 3303.52it/s]186443it [01:21, 3239.74it/s]178727it [01:21, 3154.80it/s]192083it [01:21, 3107.66it/s]184435it [01:21, 3040.32it/s]186777it [01:21, 3130.92it/s]179046it [01:21, 2976.24it/s]192398it [01:22, 3118.22it/s]187095it [01:21, 3144.34it/s]184759it [01:21, 2946.04it/s]179348it [01:21, 2864.96it/s]192713it [01:22, 3017.06it/s]187425it [01:22, 3188.81it/s]185088it [01:22, 3038.64it/s]179639it [01:22, 2875.77it/s]193023it [01:22, 3040.26it/s]187746it [01:22, 3049.58it/s]185396it [01:22, 2926.17it/s]179929it [01:22, 2855.13it/s]193356it [01:22, 3121.84it/s]188105it [01:22, 3202.87it/s]185749it [01:22, 3091.25it/s]180216it [01:22, 2790.20it/s]188447it [01:22, 3265.18it/s]193670it [01:22, 2917.75it/s]186062it [01:22, 2971.40it/s]180547it [01:22, 2933.67it/s]194034it [01:22, 3119.20it/s]188776it [01:22, 3105.52it/s]186427it [01:22, 3157.00it/s]180876it [01:22, 3034.92it/s]194350it [01:22, 3058.29it/s]189130it [01:22, 3228.33it/s]186777it [01:22, 3185.28it/s]181181it [01:22, 2829.81it/s]194719it [01:22, 3236.58it/s]189456it [01:22, 3199.03it/s]187127it [01:22, 3267.10it/s]181547it [01:22, 3061.72it/s]195101it [01:22, 3404.22it/s]189824it [01:22, 3336.81it/s]187504it [01:22, 3409.89it/s]181858it [01:22, 3057.93it/s]195445it [01:22, 3235.37it/s]190160it [01:22, 3184.80it/s]187847it [01:22, 3245.38it/s]182212it [01:22, 3196.61it/s]195800it [01:23, 3322.89it/s]190505it [01:22, 3260.02it/s]188204it [01:23, 3337.00it/s]182564it [01:23, 3290.68it/s]190845it [01:23, 3300.32it/s]196136it [01:23, 3071.64it/s]188541it [01:23, 3147.04it/s]182896it [01:23, 3097.72it/s]191177it [01:23, 3138.84it/s]196494it [01:23, 3209.33it/s]188903it [01:23, 3278.07it/s]183265it [01:23, 3265.17it/s]191538it [01:23, 3265.81it/s]189267it [01:23, 3381.10it/s]196859it [01:23, 3193.17it/s]183596it [01:23, 3163.33it/s]191867it [01:23, 3233.04it/s]197219it [01:23, 3306.03it/s]189609it [01:23, 3293.69it/s]183965it [01:23, 3312.57it/s]192236it [01:23, 3362.54it/s]197587it [01:23, 3397.38it/s]189974it [01:23, 3394.89it/s]184300it [01:23, 3183.29it/s]192585it [01:23, 3399.26it/s]197930it [01:23, 3237.29it/s]190316it [01:23, 3237.81it/s]184649it [01:23, 3269.47it/s]192927it [01:23, 3215.94it/s]198275it [01:23, 3296.53it/s]190651it [01:23, 3266.97it/s]184990it [01:23, 3309.87it/s]193284it [01:23, 3314.29it/s]190980it [01:23, 3101.31it/s]185323it [01:23, 3119.44it/s]193618it [01:23, 3158.41it/s]191345it [01:23, 3254.86it/s]185680it [01:23, 3245.83it/s]193993it [01:24, 3324.79it/s]191713it [01:24, 3374.37it/s]186008it [01:24, 3205.52it/s]194338it [01:24, 3271.30it/s]192054it [01:24, 3306.22it/s]186375it [01:24, 3337.54it/s]194704it [01:24, 3380.65it/s]192420it [01:24, 3407.22it/s]186759it [01:24, 3480.90it/s]195045it [01:24, 3271.09it/s]192763it [01:24, 3241.34it/s]187109it [01:24, 3276.65it/s]195375it [01:24, 3142.69it/s]193094it [01:24, 3260.58it/s]187451it [01:24, 3316.12it/s]195707it [01:24, 3192.03it/s]193425it [01:24, 3272.81it/s]187786it [01:24, 2994.95it/s]196028it [01:24, 2871.50it/s]193754it [01:24, 2930.88it/s]188093it [01:24, 2807.30it/s]196332it [01:24, 2915.92it/s]194057it [01:24, 2945.85it/s]188380it [01:24, 2566.58it/s]196646it [01:24, 2976.10it/s]194357it [01:24, 2712.21it/s]188644it [01:25, 2540.75it/s]196948it [01:25, 2861.70it/s]194693it [01:25, 2884.04it/s]188991it [01:25, 2784.97it/s]197277it [01:25, 2981.07it/s]194991it [01:25, 2909.78it/s]189297it [01:25, 2796.60it/s]197632it [01:25, 3141.53it/s]195287it [01:25, 2922.93it/s]189658it [01:25, 3020.30it/s]197950it [01:25, 3113.77it/s]195643it [01:25, 3104.36it/s]190021it [01:25, 3191.09it/s]198318it [01:25, 3276.18it/s]196002it [01:25, 3244.85it/s]190345it [01:25, 3179.45it/s]196330it [01:25, 3161.21it/s]190709it [01:25, 3312.22it/s]196711it [01:25, 3348.39it/s]191043it [01:25, 3256.89it/s]197049it [01:25, 3260.03it/s]191412it [01:25, 3382.43it/s]197423it [01:25, 3396.84it/s]191787it [01:25, 3486.17it/s]197765it [01:26, 3291.21it/s]192138it [01:26, 3292.51it/s]198126it [01:26, 3382.21it/s]192502it [01:26, 3390.75it/s]198466it [01:26, 3378.59it/s]192844it [01:26, 3268.86it/s]193212it [01:26, 3383.57it/s]193553it [01:26, 3259.65it/s]193920it [01:26, 3374.61it/s]194287it [01:26, 3459.18it/s]194635it [01:26, 3331.82it/s]194993it [01:26, 3401.53it/s]195335it [01:27, 3340.32it/s]195684it [01:27, 3382.65it/s]196024it [01:27, 3256.67it/s]196375it [01:27, 3327.80it/s]196748it [01:27, 3443.84it/s]197094it [01:27, 2983.96it/s]197415it [01:27, 3043.98it/s]197729it [01:27, 2910.96it/s]198105it [01:27, 3140.30it/s]198462it [01:28, 3258.79it/s]198648it [01:32, 146.78it/s] 198994it [01:32, 207.60it/s]199323it [01:33, 286.35it/s]199608it [01:33, 375.37it/s]199940it [01:33, 515.90it/s]200277it [01:33, 698.02it/s]200584it [01:33, 874.50it/s]198608it [01:33, 3121.21it/s]200870it [01:33, 1078.68it/s]198924it [01:33, 111.72it/s] 201154it [01:33, 1267.51it/s]199172it [01:33, 144.88it/s]201439it [01:33, 1506.83it/s]199448it [01:33, 195.13it/s]201769it [01:33, 1825.91it/s]199774it [01:34, 278.22it/s]202059it [01:34, 1998.50it/s]200111it [01:34, 393.47it/s]202387it [01:34, 2276.49it/s]200391it [01:34, 512.93it/s]202737it [01:34, 2562.62it/s]200743it [01:34, 714.65it/s]203047it [01:34, 2623.45it/s]201094it [01:34, 958.96it/s]203384it [01:34, 2817.97it/s]201407it [01:34, 1176.35it/s]203696it [01:34, 2766.34it/s]201758it [01:34, 1490.06it/s]204022it [01:34, 2897.94it/s]202072it [01:34, 1713.69it/s]204365it [01:34, 3045.59it/s]202453it [01:34, 2097.72it/s]204682it [01:34, 2988.92it/s]202808it [01:35, 2339.53it/s]205047it [01:34, 3171.10it/s]203176it [01:35, 2640.03it/s]205372it [01:35, 3149.78it/s]203542it [01:35, 2886.42it/s]205716it [01:35, 3231.36it/s]203887it [01:35, 2955.17it/s]206090it [01:35, 3378.71it/s]204255it [01:35, 3142.58it/s]206431it [01:35, 3297.09it/s]204600it [01:35, 3143.05it/s]206801it [01:35, 3413.82it/s]204977it [01:35, 3315.77it/s]207145it [01:35, 3351.76it/s]205328it [01:35, 3266.08it/s]207502it [01:35, 3395.82it/s]205688it [01:35, 3358.22it/s]207847it [01:35, 3258.98it/s]206033it [01:35, 3376.01it/s]208201it [01:35, 3337.95it/s]198806it [01:36, 115.25it/s] 206377it [01:36, 3185.52it/s]208550it [01:35, 3381.88it/s]199156it [01:36, 162.82it/s]206715it [01:36, 3239.48it/s]208890it [01:36, 3170.74it/s]199445it [01:36, 216.41it/s]209211it [01:36, 3174.09it/s]207044it [01:36, 3001.16it/s]199795it [01:36, 306.16it/s]207358it [01:36, 2965.93it/s]209531it [01:36, 2925.47it/s]200093it [01:36, 404.90it/s]207659it [01:36, 2932.43it/s]209835it [01:36, 2955.30it/s]200382it [01:36, 522.35it/s]210137it [01:36, 2968.19it/s]207956it [01:36, 2687.84it/s]200650it [01:36, 657.57it/s]208254it [01:36, 2765.83it/s]210437it [01:36, 2763.76it/s]200929it [01:36, 840.52it/s]208608it [01:36, 2978.17it/s]210764it [01:36, 2902.43it/s]201215it [01:36, 1061.00it/s]208911it [01:36, 2961.63it/s]211120it [01:36, 3087.65it/s]201584it [01:36, 1415.70it/s]209278it [01:37, 3156.10it/s]211433it [01:36, 3073.50it/s]201957it [01:37, 1789.54it/s]209597it [01:37, 3137.28it/s]211796it [01:37, 3233.70it/s]202279it [01:37, 2046.76it/s]209953it [01:37, 3257.54it/s]212122it [01:37, 3187.35it/s]202659it [01:37, 2415.27it/s]210305it [01:37, 3333.14it/s]212475it [01:37, 3236.40it/s]202996it [01:37, 2573.76it/s]212809it [01:37, 3264.50it/s]210640it [01:37, 3161.42it/s]203332it [01:37, 2765.32it/s]210973it [01:37, 3207.80it/s]213137it [01:37, 3083.32it/s]203662it [01:37, 2728.41it/s]213479it [01:37, 3177.30it/s]211296it [01:37, 3029.61it/s]198794it [01:37, 115.69it/s] 204023it [01:37, 2954.04it/s]211660it [01:37, 3199.26it/s]213800it [01:37, 3094.73it/s]199172it [01:37, 167.67it/s]204391it [01:37, 3147.51it/s]212028it [01:37, 3334.28it/s]214140it [01:37, 3152.18it/s]199485it [01:37, 226.50it/s]204729it [01:37, 3127.73it/s]214500it [01:37, 3279.03it/s]212365it [01:38, 3258.50it/s]199835it [01:37, 316.78it/s]205071it [01:38, 3207.34it/s]212700it [01:38, 3283.80it/s]214830it [01:38, 3189.69it/s]200175it [01:38, 433.66it/s]205404it [01:38, 3081.88it/s]215186it [01:38, 3295.44it/s]213031it [01:38, 3073.93it/s]200491it [01:38, 567.80it/s]205751it [01:38, 3189.89it/s]213379it [01:38, 3186.05it/s]215517it [01:38, 3145.26it/s]200833it [01:38, 760.92it/s]206123it [01:38, 3339.18it/s]215883it [01:38, 3291.22it/s]213728it [01:38, 3122.12it/s]201146it [01:38, 964.08it/s]206463it [01:38, 3238.32it/s]216229it [01:38, 3339.83it/s]214102it [01:38, 3293.22it/s]201514it [01:38, 1266.23it/s]206834it [01:38, 3370.19it/s]216565it [01:38, 3258.88it/s]214468it [01:38, 3397.79it/s]201894it [01:38, 1613.64it/s]207175it [01:38, 3306.54it/s]216909it [01:38, 3309.94it/s]214811it [01:38, 3276.03it/s]202236it [01:38, 1885.80it/s]207529it [01:38, 3371.21it/s]217242it [01:38, 3234.01it/s]215174it [01:38, 3374.24it/s]202608it [01:38, 2229.99it/s]207869it [01:38, 3298.10it/s]217593it [01:38, 3301.57it/s]215514it [01:38, 3272.69it/s]202954it [01:38, 2434.44it/s]208250it [01:38, 3444.14it/s]217927it [01:38, 3201.27it/s]215877it [01:39, 3374.49it/s]203308it [01:39, 2684.99it/s]208613it [01:39, 3496.98it/s]218289it [01:39, 3319.43it/s]216236it [01:39, 3435.85it/s]203649it [01:39, 2547.60it/s]208965it [01:39, 3165.02it/s]218625it [01:39, 3330.63it/s]216582it [01:39, 3253.30it/s]209289it [01:39, 3160.46it/s]203956it [01:39, 2541.75it/s]218960it [01:39, 3147.80it/s]216917it [01:39, 3280.64it/s]209610it [01:39, 2989.87it/s]219291it [01:39, 3191.78it/s]204247it [01:39, 2470.41it/s]217248it [01:39, 3123.20it/s]209920it [01:39, 3018.30it/s]219613it [01:39, 3015.58it/s]217589it [01:39, 3201.82it/s]204520it [01:39, 2219.19it/s]210254it [01:39, 3107.60it/s]219991it [01:39, 3227.89it/s]217928it [01:39, 3218.99it/s]204882it [01:39, 2548.07it/s]210568it [01:39, 3014.46it/s]220363it [01:39, 3366.32it/s]218302it [01:39, 3366.93it/s]205244it [01:39, 2817.96it/s]210935it [01:39, 3198.48it/s]220703it [01:39, 3321.55it/s]218690it [01:39, 3515.01it/s]205548it [01:39, 2840.66it/s]211258it [01:39, 3163.38it/s]221061it [01:39, 3394.77it/s]219044it [01:40, 3420.24it/s]205917it [01:39, 3069.41it/s]211626it [01:40, 3310.08it/s]221403it [01:40, 3338.49it/s]219428it [01:40, 3541.62it/s]206237it [01:40, 3073.72it/s]211987it [01:40, 3395.02it/s]221792it [01:40, 3497.03it/s]219784it [01:40, 3339.79it/s]206609it [01:40, 3256.27it/s]212329it [01:40, 3324.66it/s]222144it [01:40, 3346.12it/s]220169it [01:40, 3481.91it/s]206972it [01:40, 3361.70it/s]212663it [01:40, 3288.54it/s]222497it [01:40, 3396.84it/s]220521it [01:40, 3444.35it/s]207314it [01:40, 3286.58it/s]212993it [01:40, 3228.98it/s]222851it [01:40, 3436.85it/s]220902it [01:40, 3547.97it/s]207682it [01:40, 3399.30it/s]213360it [01:40, 3355.80it/s]223197it [01:40, 3313.04it/s]221283it [01:40, 3622.32it/s]208026it [01:40, 3341.52it/s]213726it [01:40, 3276.59it/s]223558it [01:40, 3397.18it/s]221647it [01:40, 3548.51it/s]208408it [01:40, 3479.01it/s]214102it [01:40, 3412.76it/s]223900it [01:40, 3294.44it/s]222006it [01:40, 3559.24it/s]208759it [01:40, 3392.22it/s]214466it [01:40, 3478.11it/s]224267it [01:40, 3400.03it/s]222363it [01:40, 3439.42it/s]209133it [01:40, 3492.54it/s]214816it [01:41, 3353.04it/s]224626it [01:40, 3454.75it/s]222741it [01:41, 3536.48it/s]209495it [01:41, 3528.47it/s]215190it [01:41, 3463.39it/s]224973it [01:41, 3330.76it/s]223096it [01:41, 3395.89it/s]209850it [01:41, 3394.01it/s]215539it [01:41, 3328.65it/s]225337it [01:41, 3417.12it/s]223463it [01:41, 3472.78it/s]210205it [01:41, 3438.77it/s]215912it [01:41, 3440.96it/s]225681it [01:41, 3297.53it/s]223812it [01:41, 3366.84it/s]210551it [01:41, 3319.54it/s]216259it [01:41, 3335.61it/s]226037it [01:41, 3370.74it/s]224193it [01:41, 3490.97it/s]210926it [01:41, 3441.19it/s]216634it [01:41, 3452.16it/s]226376it [01:41, 3252.75it/s]224558it [01:41, 3535.48it/s]211272it [01:41, 3313.79it/s]217013it [01:41, 3549.90it/s]226731it [01:41, 3335.90it/s]224913it [01:41, 3419.33it/s]211643it [01:41, 3424.74it/s]217370it [01:41, 3411.12it/s]227080it [01:41, 3380.15it/s]225277it [01:41, 3482.08it/s]212020it [01:41, 3523.79it/s]217755it [01:41, 3535.89it/s]227420it [01:41, 3247.80it/s]225627it [01:41, 3385.02it/s]212375it [01:41, 3362.45it/s]218111it [01:41, 3398.23it/s]227773it [01:41, 3326.62it/s]225978it [01:42, 3415.02it/s]212743it [01:41, 3450.97it/s]218459it [01:42, 3421.00it/s]228108it [01:42, 3098.68it/s]226328it [01:42, 3217.69it/s]213091it [01:42, 3206.23it/s]218803it [01:42, 3262.70it/s]228432it [01:42, 3136.10it/s]226664it [01:42, 3255.34it/s]213417it [01:42, 3158.64it/s]219132it [01:42, 3170.41it/s]228750it [01:42, 3147.56it/s]226992it [01:42, 3197.34it/s]213736it [01:42, 2926.20it/s]219466it [01:42, 3208.30it/s]229067it [01:42, 2974.81it/s]227314it [01:42, 3013.49it/s]214058it [01:42, 3004.55it/s]219789it [01:42, 3098.87it/s]229392it [01:42, 3051.54it/s]227629it [01:42, 3050.99it/s]214408it [01:42, 3141.90it/s]220165it [01:42, 3283.65it/s]229700it [01:42, 3031.97it/s]227986it [01:42, 3195.81it/s]214727it [01:42, 3061.40it/s]220496it [01:42, 3226.24it/s]230050it [01:42, 3166.51it/s]228308it [01:42, 3131.31it/s]215094it [01:42, 3232.65it/s]220871it [01:42, 3375.83it/s]230403it [01:42, 3269.93it/s]228667it [01:42, 3261.74it/s]215421it [01:42, 3192.16it/s]221238it [01:42, 3460.44it/s]230732it [01:42, 3173.11it/s]228995it [01:43, 3177.34it/s]215771it [01:42, 3279.24it/s]221586it [01:43, 3321.61it/s]231089it [01:42, 3285.50it/s]229358it [01:43, 3305.65it/s]216129it [01:43, 3364.54it/s]221958it [01:43, 3433.86it/s]231420it [01:43, 3205.32it/s]229691it [01:43, 3220.69it/s]216467it [01:43, 3228.28it/s]222304it [01:43, 3334.42it/s]231774it [01:43, 3301.00it/s]230052it [01:43, 3332.09it/s]216828it [01:43, 3337.41it/s]222659it [01:43, 3394.18it/s]232124it [01:43, 3357.20it/s]230394it [01:43, 3357.24it/s]217164it [01:43, 3247.61it/s]223000it [01:43, 3303.32it/s]232461it [01:43, 3248.84it/s]230731it [01:43, 3264.75it/s]217536it [01:43, 3381.87it/s]223355it [01:43, 3372.52it/s]232811it [01:43, 3320.84it/s]231098it [01:43, 3380.94it/s]217906it [01:43, 3472.34it/s]223719it [01:43, 3449.86it/s]233145it [01:43, 3229.30it/s]231438it [01:43, 3283.15it/s]218255it [01:43, 3381.03it/s]224066it [01:43, 3333.01it/s]233505it [01:43, 3335.55it/s]231791it [01:43, 3352.14it/s]218606it [01:43, 3416.88it/s]224436it [01:43, 3428.06it/s]233862it [01:43, 3402.76it/s]232145it [01:43, 3406.18it/s]218949it [01:43, 3279.03it/s]224781it [01:43, 3198.86it/s]234204it [01:43, 3220.69it/s]232487it [01:44, 3199.57it/s]219285it [01:44, 3294.91it/s]225108it [01:44, 3218.00it/s]234530it [01:44, 3230.81it/s]232824it [01:44, 3246.36it/s]219616it [01:44, 3135.35it/s]225466it [01:44, 3321.12it/s]234855it [01:44, 3082.01it/s]233152it [01:44, 3107.36it/s]219997it [01:44, 3325.42it/s]225801it [01:44, 3139.08it/s]235210it [01:44, 3211.94it/s]233514it [01:44, 3251.44it/s]220360it [01:44, 3411.51it/s]226156it [01:44, 3254.26it/s]235558it [01:44, 3286.96it/s]233865it [01:44, 3324.83it/s]220704it [01:44, 3356.57it/s]226485it [01:44, 3195.40it/s]235889it [01:44, 3183.99it/s]234200it [01:44, 3251.55it/s]221055it [01:44, 3400.88it/s]226834it [01:44, 3278.66it/s]236244it [01:44, 3286.51it/s]234531it [01:44, 3268.14it/s]221397it [01:44, 3296.35it/s]227164it [01:44, 3243.06it/s]236575it [01:44, 3079.28it/s]234860it [01:44, 3122.24it/s]221735it [01:44, 3320.03it/s]227490it [01:44, 3070.13it/s]236897it [01:44, 3113.41it/s]235196it [01:44, 3188.54it/s]222072it [01:44, 3333.68it/s]227800it [01:44, 3050.93it/s]237233it [01:44, 3182.51it/s]235517it [01:45, 3155.50it/s]222407it [01:44, 3082.32it/s]237554it [01:45, 2982.01it/s]228107it [01:45, 2814.75it/s]235834it [01:45, 2962.95it/s]222721it [01:45, 3097.44it/s]237866it [01:45, 3019.08it/s]228420it [01:45, 2898.93it/s]236148it [01:45, 3011.14it/s]223034it [01:45, 2874.01it/s]228731it [01:45, 2956.98it/s]238171it [01:45, 2878.06it/s]236452it [01:45, 2825.50it/s]223327it [01:45, 2762.37it/s]238475it [01:45, 2889.13it/s]229030it [01:45, 2754.76it/s]236765it [01:45, 2908.24it/s]223659it [01:45, 2913.72it/s]238792it [01:45, 2968.17it/s]229387it [01:45, 2978.33it/s]237116it [01:45, 3077.03it/s]223963it [01:45, 2948.79it/s]239091it [01:45, 2935.19it/s]229690it [01:45, 2957.22it/s]237427it [01:45, 3006.92it/s]224306it [01:45, 3085.72it/s]239439it [01:45, 3091.72it/s]230046it [01:45, 3126.60it/s]237780it [01:45, 3155.99it/s]224647it [01:45, 3078.88it/s]230380it [01:45, 3186.43it/s]239767it [01:45, 3042.79it/s]238098it [01:45, 3107.46it/s]224978it [01:45, 3143.59it/s]240117it [01:45, 3174.01it/s]230702it [01:45, 3101.12it/s]238452it [01:45, 3230.37it/s]225340it [01:45, 3281.26it/s]240469it [01:45, 3273.22it/s]231066it [01:46, 3256.07it/s]238790it [01:46, 3271.69it/s]225670it [01:46, 3211.39it/s]240798it [01:46, 3188.42it/s]231394it [01:46, 3167.59it/s]239119it [01:46, 3187.26it/s]226001it [01:46, 3238.86it/s]241152it [01:46, 3289.55it/s]231743it [01:46, 3258.45it/s]239473it [01:46, 3289.42it/s]226327it [01:46, 3165.94it/s]232088it [01:46, 3312.36it/s]241483it [01:46, 3202.24it/s]239804it [01:46, 3181.84it/s]226663it [01:46, 3219.69it/s]241848it [01:46, 3330.90it/s]240159it [01:46, 3285.30it/s]232421it [01:46, 3141.63it/s]227012it [01:46, 3296.75it/s]242203it [01:46, 3392.85it/s]240498it [01:46, 3315.20it/s]232747it [01:46, 3173.90it/s]227343it [01:46, 3213.76it/s]242544it [01:46, 3285.48it/s]240831it [01:46, 3212.21it/s]233067it [01:46, 3041.33it/s]227700it [01:46, 3316.76it/s]242898it [01:46, 3357.31it/s]241189it [01:46, 3317.12it/s]233418it [01:46, 3172.64it/s]228033it [01:46, 3230.26it/s]243235it [01:46, 3253.62it/s]233749it [01:46, 3210.80it/s]241523it [01:46, 3207.00it/s]228395it [01:46, 3341.27it/s]243597it [01:46, 3358.35it/s]241887it [01:47, 3330.61it/s]234072it [01:46, 3072.27it/s]228761it [01:46, 3432.12it/s]243952it [01:47, 3411.47it/s]242249it [01:47, 3413.52it/s]234423it [01:47, 3195.09it/s]229106it [01:47, 3288.11it/s]244295it [01:47, 3294.99it/s]242592it [01:47, 3262.83it/s]234745it [01:47, 2915.60it/s]229477it [01:47, 3399.43it/s]244626it [01:47, 3247.15it/s]242949it [01:47, 3349.72it/s]235099it [01:47, 3085.98it/s]229819it [01:47, 3290.05it/s]244952it [01:47, 3170.02it/s]243286it [01:47, 3274.68it/s]235446it [01:47, 3191.69it/s]230177it [01:47, 3371.98it/s]245310it [01:47, 3285.42it/s]243647it [01:47, 3368.69it/s]235770it [01:47, 2998.22it/s]230527it [01:47, 3257.88it/s]245647it [01:47, 3209.22it/s]243986it [01:47, 3238.55it/s]236117it [01:47, 3126.78it/s]230882it [01:47, 3339.92it/s]246005it [01:47, 3312.95it/s]244345it [01:47, 3336.65it/s]231247it [01:47, 3427.34it/s]236435it [01:47, 2930.67it/s]246352it [01:47, 3357.66it/s]244696it [01:47, 3385.56it/s]236756it [01:47, 3004.46it/s]231592it [01:47, 3245.01it/s]245036it [01:47, 3144.73it/s]246689it [01:47, 2966.36it/s]237065it [01:47, 3026.73it/s]231927it [01:47, 3263.97it/s]245357it [01:48, 3160.45it/s]247010it [01:47, 3030.20it/s]237371it [01:48, 2841.83it/s]232256it [01:48, 3081.74it/s]247327it [01:48, 2901.40it/s]237674it [01:48, 2893.55it/s]232574it [01:48, 3108.80it/s]245676it [01:48, 2637.13it/s]247650it [01:48, 2988.90it/s]237972it [01:48, 2915.99it/s]232888it [01:48, 2999.16it/s]245983it [01:48, 2746.01it/s]247995it [01:48, 3116.87it/s]238266it [01:48, 2919.01it/s]246333it [01:48, 2944.96it/s]233190it [01:48, 2945.75it/s]248311it [01:48, 3070.07it/s]238619it [01:48, 3094.86it/s]233541it [01:48, 3103.79it/s]246640it [01:48, 2791.87it/s]248653it [01:48, 3168.25it/s]238931it [01:48, 2995.14it/s]246971it [01:48, 2930.43it/s]233887it [01:48, 2962.61it/s]249007it [01:48, 3127.33it/s]239277it [01:48, 3127.27it/s]247310it [01:48, 3055.60it/s]234234it [01:48, 3100.97it/s]249362it [01:48, 3247.08it/s]239621it [01:48, 3217.22it/s]234567it [01:48, 3165.22it/s]247623it [01:48, 2975.53it/s]249708it [01:48, 3307.31it/s]239945it [01:48, 3140.14it/s]247965it [01:49, 3098.95it/s]234887it [01:48, 2972.66it/s]250041it [01:48, 3220.98it/s]240284it [01:49, 3210.62it/s]248280it [01:49, 3072.13it/s]235226it [01:49, 3086.09it/s]250392it [01:49, 3304.14it/s]240607it [01:49, 3120.67it/s]248616it [01:49, 3153.80it/s]235549it [01:49, 3124.70it/s]250724it [01:49, 3205.90it/s]240947it [01:49, 3199.01it/s]248976it [01:49, 3283.13it/s]235865it [01:49, 2942.87it/s]251073it [01:49, 3285.21it/s]241304it [01:49, 3305.02it/s]249307it [01:49, 3202.59it/s]236196it [01:49, 3043.06it/s]251426it [01:49, 3354.59it/s]241636it [01:49, 3153.62it/s]249646it [01:49, 3254.84it/s]236504it [01:49, 2886.57it/s]251763it [01:49, 3215.46it/s]241994it [01:49, 3273.22it/s]249973it [01:49, 3194.85it/s]236838it [01:49, 3010.62it/s]252125it [01:49, 3328.99it/s]242324it [01:49, 3181.84it/s]250312it [01:49, 3251.08it/s]237171it [01:49, 3100.30it/s]252460it [01:49, 3194.95it/s]242659it [01:49, 3229.51it/s]250674it [01:49, 3356.95it/s]237484it [01:49, 2935.95it/s]252815it [01:49, 3295.06it/s]243013it [01:49, 3318.37it/s]251011it [01:49, 3202.18it/s]237807it [01:49, 3017.80it/s]253165it [01:49, 3352.52it/s]243347it [01:49, 3216.08it/s]251367it [01:50, 3303.72it/s]253502it [01:50, 3239.74it/s]238112it [01:50, 2874.87it/s]243683it [01:50, 3255.39it/s]251700it [01:50, 3165.17it/s]253848it [01:50, 3301.30it/s]238437it [01:50, 2978.25it/s]244010it [01:50, 3160.67it/s]252057it [01:50, 3278.23it/s]238770it [01:50, 3076.50it/s]254180it [01:50, 3208.07it/s]244362it [01:50, 3264.01it/s]252387it [01:50, 3151.94it/s]254527it [01:50, 3282.30it/s]239081it [01:50, 2913.49it/s]244709it [01:50, 3271.29it/s]252741it [01:50, 3261.34it/s]254879it [01:50, 3349.94it/s]239412it [01:50, 3023.51it/s]245038it [01:50, 3173.88it/s]253099it [01:50, 3352.50it/s]239746it [01:50, 3112.43it/s]255216it [01:50, 3212.33it/s]245388it [01:50, 3266.95it/s]253437it [01:50, 3178.61it/s]255559it [01:50, 3274.21it/s]240060it [01:50, 2889.83it/s]245716it [01:50, 3018.02it/s]253783it [01:50, 3256.46it/s]240378it [01:50, 2968.21it/s]246051it [01:50, 3108.15it/s]254112it [01:50, 2989.43it/s]246366it [01:50, 3103.79it/s]240679it [01:50, 2692.14it/s]254420it [01:51, 3013.56it/s]240974it [01:51, 2760.12it/s]246679it [01:51, 2934.89it/s]254739it [01:51, 3061.59it/s]246976it [01:51, 2906.98it/s]241256it [01:51, 2555.56it/s]255049it [01:51, 2869.39it/s]247319it [01:51, 3053.88it/s]241518it [01:51, 2547.51it/s]255403it [01:51, 3053.08it/s]247627it [01:51, 3018.15it/s]241849it [01:51, 2754.19it/s]255728it [01:51, 3002.19it/s]247960it [01:51, 3107.79it/s]242172it [01:51, 2886.79it/s]248273it [01:51, 3055.70it/s]242465it [01:51, 2830.77it/s]248623it [01:51, 3184.32it/s]242798it [01:51, 2973.01it/s]248953it [01:51, 3216.52it/s]243127it [01:51, 2939.80it/s]249276it [01:51, 3149.04it/s]243473it [01:51, 3086.15it/s]249634it [01:51, 3273.29it/s]243802it [01:51, 3142.73it/s]249963it [01:52, 3073.79it/s]244119it [01:52, 3000.80it/s]250299it [01:52, 3153.56it/s]244457it [01:52, 3103.55it/s]250641it [01:52, 3228.85it/s]244803it [01:52, 3206.24it/s]250966it [01:52, 2975.17it/s]245126it [01:52, 3086.09it/s]251308it [01:52, 3097.34it/s]245468it [01:52, 3180.00it/s]245788it [01:52, 3067.03it/s]251623it [01:52, 2927.44it/s]246130it [01:52, 3167.54it/s]251941it [01:52, 2995.52it/s]246470it [01:52, 3233.59it/s]252289it [01:52, 3130.76it/s]246795it [01:52, 3118.63it/s]252606it [01:52, 2970.97it/s]247133it [01:53, 3193.38it/s]252948it [01:53, 3082.18it/s]247454it [01:53, 3073.49it/s]253260it [01:53, 3016.44it/s]247808it [01:53, 3204.50it/s]253601it [01:53, 3124.02it/s]248160it [01:53, 3294.88it/s]253957it [01:53, 3247.25it/s]248492it [01:53, 3173.90it/s]254284it [01:53, 3130.76it/s]248846it [01:53, 3277.64it/s]254623it [01:53, 3202.99it/s]254946it [01:53, 3007.92it/s]249176it [01:53, 2867.20it/s]255251it [01:53, 2918.86it/s]249473it [01:53, 2734.41it/s]255546it [01:53, 2708.59it/s]249754it [01:53, 2636.70it/s]250023it [01:54, 2537.90it/s]250384it [01:54, 2821.67it/s]250687it [01:54, 2863.63it/s]251047it [01:54, 3070.72it/s]251388it [01:54, 3167.22it/s]251709it [01:54, 3045.23it/s]252017it [01:54, 2944.72it/s]252364it [01:54, 3091.36it/s]252676it [01:54, 2934.89it/s]253020it [01:55, 3075.81it/s]253331it [01:55, 2936.24it/s]253671it [01:55, 3065.15it/s]254009it [01:55, 3154.27it/s]254328it [01:55, 2979.23it/s]254648it [01:55, 3039.56it/s]254955it [01:55, 2901.83it/s]255295it [01:55, 3039.03it/s]255632it [01:55, 3132.23it/s]255559it [02:02, 3274.21it/s]255797it [02:02, 88.06it/s]  256078it [02:02, 120.64it/s]255728it [02:02, 3002.19it/s]256393it [02:02, 171.24it/s]255798it [02:02, 74.35it/s]  256072it [02:02, 107.67it/s]256699it [02:02, 236.94it/s]256367it [02:02, 157.52it/s]257010it [02:02, 329.18it/s]256662it [02:02, 225.25it/s]257348it [02:02, 462.94it/s]256989it [02:02, 327.73it/s]257653it [02:02, 613.21it/s]257313it [02:03, 461.34it/s]257988it [02:02, 825.05it/s]257620it [02:03, 613.45it/s]258310it [02:03, 1063.39it/s]257939it [02:03, 818.87it/s]258624it [02:03, 1290.11it/s]258272it [02:03, 1075.73it/s]258960it [02:03, 1596.24it/s]258581it [02:03, 1291.76it/s]259270it [02:03, 1836.16it/s]258923it [02:03, 1610.81it/s]259617it [02:03, 2158.61it/s]259966it [02:03, 2451.16it/s]259229it [02:03, 1811.59it/s]259556it [02:03, 2097.42it/s]260293it [02:03, 2570.69it/s]259896it [02:03, 2382.51it/s]260637it [02:03, 2786.09it/s]260961it [02:03, 2809.27it/s]260210it [02:03, 2424.07it/s]261297it [02:03, 2955.05it/s]260552it [02:04, 2665.33it/s]261635it [02:04, 3071.08it/s]260862it [02:04, 2620.15it/s]261961it [02:04, 2926.70it/s]261208it [02:04, 2835.58it/s]262297it [02:04, 3043.42it/s]261535it [02:04, 2951.09it/s]262613it [02:04, 2897.40it/s]261849it [02:04, 2862.32it/s]262968it [02:04, 3075.10it/s]262187it [02:04, 3002.26it/s]263322it [02:04, 3203.74it/s]262517it [02:04, 2998.00it/s]263649it [02:04, 3142.06it/s]262879it [02:04, 3171.59it/s]264004it [02:04, 3257.24it/s]263218it [02:04, 3233.24it/s]264334it [02:04, 3101.71it/s]263547it [02:05, 3064.82it/s]264668it [02:05, 3168.13it/s]263876it [02:05, 3126.11it/s]264988it [02:05, 3139.62it/s]264194it [02:05, 3140.03it/s]255546it [02:05, 2708.59it/s]255796it [02:05, 84.71it/s]  265305it [02:05, 2912.92it/s]264511it [02:05, 2858.40it/s]256098it [02:05, 120.31it/s]265601it [02:05, 2917.78it/s]264804it [02:05, 2846.86it/s]256398it [02:05, 169.35it/s]265896it [02:05, 2767.38it/s]265094it [02:05, 2768.37it/s]256686it [02:05, 233.17it/s]266248it [02:05, 2973.48it/s]265435it [02:05, 2945.70it/s]257050it [02:05, 344.69it/s]266614it [02:05, 3167.33it/s]265798it [02:05, 3139.49it/s]257385it [02:05, 478.94it/s]266935it [02:05, 3128.40it/s]266116it [02:05, 3052.35it/s]257699it [02:05, 636.91it/s]267299it [02:05, 3275.49it/s]266476it [02:06, 3207.05it/s]258042it [02:06, 856.81it/s]267630it [02:06, 3212.27it/s]266800it [02:06, 3144.06it/s]258362it [02:06, 1089.79it/s]267996it [02:06, 3339.68it/s]267153it [02:06, 3253.07it/s]258715it [02:06, 1396.67it/s]268362it [02:06, 3432.10it/s]267515it [02:06, 3359.83it/s]259043it [02:06, 1681.28it/s]268707it [02:06, 3335.04it/s]267853it [02:06, 3233.21it/s]259370it [02:06, 1913.62it/s]269073it [02:06, 3427.79it/s]268224it [02:06, 3367.56it/s]259722it [02:06, 2232.87it/s]269418it [02:06, 3326.87it/s]268563it [02:06, 3213.18it/s]260048it [02:06, 2370.77it/s]269782it [02:06, 3415.73it/s]268926it [02:06, 3328.92it/s]260385it [02:06, 2603.18it/s]270125it [02:06, 3335.21it/s]269262it [02:06, 3183.61it/s]260704it [02:06, 2739.19it/s]270488it [02:06, 3419.64it/s]269626it [02:06, 3310.84it/s]261022it [02:06, 2790.50it/s]270840it [02:06, 3448.39it/s]269991it [02:07, 3406.20it/s]261347it [02:07, 2913.49it/s]271186it [02:07, 3337.02it/s]270334it [02:07, 3254.91it/s]261675it [02:07, 2889.92it/s]271548it [02:07, 3418.62it/s]270702it [02:07, 3372.96it/s]262012it [02:07, 3020.69it/s]271892it [02:07, 3343.73it/s]271042it [02:07, 3207.85it/s]262366it [02:07, 3165.48it/s]272254it [02:07, 3422.26it/s]271407it [02:07, 3328.12it/s]262693it [02:07, 3032.10it/s]272598it [02:07, 3302.53it/s]271748it [02:07, 3349.48it/s]263045it [02:07, 3166.47it/s]272966it [02:07, 3410.09it/s]272086it [02:07, 3268.76it/s]263368it [02:07, 3048.62it/s]273312it [02:07, 3341.29it/s]272428it [02:07, 3311.67it/s]263708it [02:07, 3146.50it/s]273648it [02:07, 3160.23it/s]272761it [02:07, 3135.41it/s]264065it [02:07, 3267.14it/s]274004it [02:07, 3270.93it/s]273087it [02:08, 3164.24it/s]264396it [02:08, 3110.19it/s]273406it [02:08, 3170.98it/s]274334it [02:08, 2775.09it/s]264711it [02:08, 3071.67it/s]273725it [02:08, 2942.56it/s]274626it [02:08, 2766.89it/s]265021it [02:08, 3002.88it/s]274924it [02:08, 2822.39it/s]274024it [02:08, 2853.00it/s]265324it [02:08, 2820.32it/s]275214it [02:08, 2717.34it/s]274313it [02:08, 2562.47it/s]265655it [02:08, 2953.41it/s]275551it [02:08, 2894.97it/s]274627it [02:08, 2708.00it/s]265954it [02:08, 2806.19it/s]275882it [02:08, 3010.56it/s]274930it [02:08, 2793.81it/s]266284it [02:08, 2941.02it/s]276188it [02:08, 2872.81it/s]275215it [02:08, 2676.51it/s]266600it [02:08, 3002.71it/s]276513it [02:08, 2976.02it/s]275530it [02:08, 2805.09it/s]266903it [02:08, 2885.17it/s]276815it [02:08, 2855.74it/s]275869it [02:09, 2969.85it/s]255632it [02:08, 3132.23it/s]255797it [02:08, 69.93it/s]  267241it [02:09, 3024.45it/s]277179it [02:09, 3073.21it/s]256149it [02:09, 106.46it/s]276170it [02:09, 2889.24it/s]267555it [02:09, 3016.55it/s]277532it [02:09, 3201.82it/s]256506it [02:09, 158.07it/s]276496it [02:09, 2993.24it/s]267919it [02:09, 3195.67it/s]277856it [02:09, 3135.44it/s]256842it [02:09, 224.31it/s]276798it [02:09, 2939.92it/s]268270it [02:09, 3287.23it/s]278209it [02:09, 3248.12it/s]257195it [02:09, 319.69it/s]277143it [02:09, 3084.71it/s]268601it [02:09, 3211.92it/s]278536it [02:09, 3186.08it/s]277498it [02:09, 3219.39it/s]257529it [02:09, 437.39it/s]268956it [02:09, 3308.95it/s]278897it [02:09, 3307.52it/s]257887it [02:09, 604.84it/s]277822it [02:09, 3151.60it/s]269289it [02:09, 3224.90it/s]279254it [02:09, 3383.14it/s]258246it [02:09, 815.57it/s]278161it [02:09, 3219.18it/s]269643it [02:09, 3315.70it/s]279594it [02:09, 3305.37it/s]258587it [02:09, 1043.62it/s]278485it [02:09, 3117.13it/s]269994it [02:09, 3371.86it/s]279948it [02:09, 3371.64it/s]258954it [02:09, 1345.20it/s]278828it [02:09, 3205.54it/s]270333it [02:09, 3294.33it/s]280287it [02:09, 3271.92it/s]279172it [02:10, 3273.81it/s]259296it [02:10, 1606.76it/s]270678it [02:10, 3338.53it/s]280648it [02:10, 3367.05it/s]259665it [02:10, 1949.69it/s]279501it [02:10, 3153.54it/s]271013it [02:10, 3267.74it/s]280996it [02:10, 3276.25it/s]279840it [02:10, 3219.29it/s]260007it [02:10, 2175.71it/s]271362it [02:10, 3330.47it/s]281357it [02:10, 3371.84it/s]260371it [02:10, 2482.32it/s]271725it [02:10, 3417.34it/s]280164it [02:10, 3035.07it/s]281717it [02:10, 3437.44it/s]260730it [02:10, 2736.88it/s]280506it [02:10, 3141.76it/s]272068it [02:10, 3321.35it/s]282062it [02:10, 3312.54it/s]280844it [02:10, 3208.91it/s]261076it [02:10, 2827.10it/s]272405it [02:10, 3334.05it/s]282414it [02:10, 3370.76it/s]261438it [02:10, 3028.14it/s]272740it [02:10, 3247.49it/s]281168it [02:10, 2996.60it/s]282753it [02:10, 3187.50it/s]273068it [02:10, 3255.33it/s]261781it [02:10, 2950.26it/s]281490it [02:10, 3056.93it/s]283096it [02:10, 3253.62it/s]273418it [02:10, 3323.97it/s]262118it [02:10, 3060.55it/s]281799it [02:10, 3034.05it/s]283424it [02:10, 3246.98it/s]262445it [02:10, 3085.55it/s]273751it [02:11, 3074.58it/s]282105it [02:11, 2866.65it/s]283751it [02:11, 2977.70it/s]274076it [02:11, 3123.52it/s]282406it [02:11, 2906.14it/s]262769it [02:11, 2919.70it/s]284065it [02:11, 3020.96it/s]263073it [02:11, 2930.82it/s]274392it [02:11, 2924.35it/s]282699it [02:11, 2798.79it/s]284371it [02:11, 2932.36it/s]274731it [02:11, 3051.34it/s]263375it [02:11, 2886.71it/s]283027it [02:11, 2926.02it/s]284728it [02:11, 3110.92it/s]263731it [02:11, 3074.71it/s]275090it [02:11, 3202.09it/s]283367it [02:11, 3059.53it/s]285075it [02:11, 3212.19it/s]264084it [02:11, 3204.18it/s]283676it [02:11, 3006.33it/s]275414it [02:11, 3098.00it/s]285399it [02:11, 3160.77it/s]264409it [02:11, 3149.24it/s]284016it [02:11, 3117.79it/s]275772it [02:11, 3233.75it/s]285744it [02:11, 3242.10it/s]264761it [02:11, 3255.61it/s]284357it [02:11, 3055.21it/s]276099it [02:11, 3110.42it/s]286070it [02:11, 3177.24it/s]265090it [02:11, 3181.10it/s]284701it [02:11, 3162.53it/s]276457it [02:11, 3242.31it/s]286414it [02:11, 3251.99it/s]265440it [02:11, 3272.88it/s]285042it [02:12, 3232.60it/s]276795it [02:11, 3119.89it/s]286776it [02:11, 3359.67it/s]265806it [02:12, 3380.68it/s]285367it [02:12, 3142.43it/s]277163it [02:12, 3276.98it/s]287112it [02:12, 2173.57it/s]
266146it [02:12, 3316.20it/s]285719it [02:12, 3250.94it/s]277512it [02:12, 3337.27it/s]266517it [02:12, 3429.01it/s]286046it [02:12, 3183.39it/s]277849it [02:12, 3264.85it/s]266862it [02:12, 3334.10it/s]286390it [02:12, 3256.12it/s]278187it [02:12, 3296.30it/s]267225it [02:12, 3419.07it/s]286738it [02:12, 3320.61it/s]278519it [02:12, 3199.75it/s]267569it [02:12, 3301.81it/s]287071it [02:12, 3213.19it/s]278878it [02:12, 3311.76it/s]287112it [02:12, 2164.23it/s]
267935it [02:12, 3403.36it/s]279221it [02:12, 3346.05it/s]268293it [02:12, 3453.18it/s]279557it [02:12, 3249.50it/s]268640it [02:12, 3353.42it/s]279898it [02:12, 3295.57it/s]269002it [02:12, 3429.21it/s]280229it [02:13, 3201.45it/s]269347it [02:13, 3335.00it/s]280575it [02:13, 3270.13it/s]269693it [02:13, 3369.46it/s]280924it [02:13, 3332.18it/s]270039it [02:13, 3394.91it/s]281259it [02:13, 3101.87it/s]270380it [02:13, 3129.11it/s]281584it [02:13, 3140.99it/s]270721it [02:13, 3206.07it/s]281901it [02:13, 2977.32it/s]282241it [02:13, 3092.90it/s]271046it [02:13, 2879.77it/s]282564it [02:13, 3130.33it/s]271391it [02:13, 3029.93it/s]271721it [02:13, 3102.11it/s]282880it [02:13, 2614.44it/s]272038it [02:13, 2939.46it/s]283157it [02:14, 2640.59it/s]272349it [02:14, 2986.05it/s]283462it [02:14, 2747.88it/s]272652it [02:14, 2901.08it/s]283746it [02:14, 2760.17it/s]273004it [02:14, 3072.91it/s]284085it [02:14, 2937.15it/s]273330it [02:14, 3126.40it/s]284385it [02:14, 2930.61it/s]273646it [02:14, 2946.17it/s]284722it [02:14, 3057.53it/s]273991it [02:14, 3085.14it/s]285054it [02:14, 3133.01it/s]274303it [02:14, 2947.05it/s]285370it [02:14, 2946.57it/s]274627it [02:14, 3028.89it/s]285669it [02:14, 2930.74it/s]274943it [02:14, 3065.76it/s]285991it [02:14, 3013.37it/s]275252it [02:15, 2888.18it/s]286295it [02:15, 2926.96it/s]275600it [02:15, 3053.88it/s]286629it [02:15, 3044.48it/s]275956it [02:15, 3047.30it/s]286936it [02:15, 2965.93it/s]287112it [02:15, 2121.39it/s]
276319it [02:15, 3209.73it/s]276680it [02:15, 3322.39it/s]277015it [02:15, 3238.78it/s]277379it [02:15, 3351.56it/s]277717it [02:15, 3254.86it/s]278076it [02:15, 3350.50it/s]278414it [02:15, 3352.18it/s]278751it [02:16, 3165.10it/s]279081it [02:16, 3202.73it/s]279404it [02:16, 2973.32it/s]279746it [02:16, 3096.20it/s]280085it [02:16, 3176.89it/s]280406it [02:16, 2997.64it/s]280745it [02:16, 3105.05it/s]281060it [02:16, 2827.07it/s]281360it [02:16, 2873.47it/s]281683it [02:17, 2971.31it/s]281985it [02:17, 2848.92it/s]282350it [02:17, 3070.48it/s]282676it [02:17, 3058.85it/s]283019it [02:17, 3162.49it/s]283379it [02:17, 3287.20it/s]283710it [02:17, 3065.94it/s]284054it [02:17, 3169.09it/s]284375it [02:17, 3024.87it/s]284695it [02:18, 3073.07it/s]285035it [02:18, 3164.44it/s]285354it [02:18, 2933.87it/s]285692it [02:18, 3055.22it/s]286035it [02:18, 3160.75it/s]286355it [02:18, 2951.09it/s]286697it [02:18, 3080.66it/s]287010it [02:18, 2906.42it/s]287112it [02:18, 2067.61it/s]
2022-07-07 15:24:29 | INFO | root | success load 287112 data
2022-07-07 15:24:29 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-cased' is a path or url to a directory containing tokenizer files.
2022-07-07 15:24:29 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/added_tokens.json. We won't load it.
2022-07-07 15:24:29 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-cased/special_tokens_map.json. We won't load it.
2022-07-07 15:24:29 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/vocab.txt
2022-07-07 15:24:29 | INFO | transformer.tokenization_utils | loading file None
2022-07-07 15:24:29 | INFO | transformer.tokenization_utils | loading file None
2022-07-07 15:24:29 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-cased/tokenizer_config.json
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-07 15:30:04 | INFO | train_inner | epoch 001:    100 / 1122 loss=nan, nll_loss=12.316, mask_ins=7.469, word_ins_ml=12.781, word_reposition=5.794, kpe=nan, ppl=nan, wps=6221.1, ups=0.3, wpb=20527, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=23.47, clip=20, loss_scale=128, train_wall=289, wall=473
2022-07-07 15:35:30 | INFO | train_inner | epoch 001:    200 / 1122 loss=22.365, nll_loss=11.17, mask_ins=4.431, word_ins_ml=11.757, word_reposition=4.791, kpe=1.386, ppl=5.40151e+06, wps=6304.4, ups=0.31, wpb=20583.2, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=18.897, clip=0, loss_scale=128, train_wall=285, wall=800
2022-07-07 15:40:56 | INFO | train_inner | epoch 001:    300 / 1122 loss=17.874, nll_loss=11.305, mask_ins=2.325, word_ins_ml=11.864, word_reposition=2.43, kpe=1.255, ppl=240199, wps=6310.8, ups=0.31, wpb=20561.3, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=5.751, clip=0, loss_scale=128, train_wall=283, wall=1126
2022-07-07 15:46:21 | INFO | train_inner | epoch 001:    400 / 1122 loss=16.274, nll_loss=10.934, mask_ins=1.936, word_ins_ml=11.539, word_reposition=1.601, kpe=1.199, ppl=79244.2, wps=6328.1, ups=0.31, wpb=20576.5, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=3.411, clip=0, loss_scale=128, train_wall=284, wall=1451
2022-07-07 15:51:46 | INFO | train_inner | epoch 001:    500 / 1122 loss=15.767, nll_loss=10.608, mask_ins=1.849, word_ins_ml=11.26, word_reposition=1.5, kpe=1.158, ppl=55780.9, wps=6317.2, ups=0.31, wpb=20523.5, bsz=256, num_updates=500, lr=5.009e-05, gnorm=3.3, clip=0, loss_scale=128, train_wall=283, wall=1776
2022-07-07 15:57:27 | INFO | train_inner | epoch 001:    600 / 1122 loss=15.433, nll_loss=10.244, mask_ins=1.847, word_ins_ml=10.951, word_reposition=1.51, kpe=1.126, ppl=44250.2, wps=6013.4, ups=0.29, wpb=20491.4, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=2.979, clip=0, loss_scale=242, train_wall=299, wall=2117
2022-07-07 16:02:53 | INFO | train_inner | epoch 001:    700 / 1122 loss=15.127, nll_loss=9.887, mask_ins=1.854, word_ins_ml=10.647, word_reposition=1.522, kpe=1.104, ppl=35788.2, wps=6286.7, ups=0.31, wpb=20542.5, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=3.016, clip=0, loss_scale=256, train_wall=285, wall=2443
2022-07-07 16:08:20 | INFO | train_inner | epoch 001:    800 / 1122 loss=14.831, nll_loss=9.609, mask_ins=1.829, word_ins_ml=10.41, word_reposition=1.502, kpe=1.091, ppl=29150.6, wps=6308.9, ups=0.31, wpb=20579, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=2.851, clip=0, loss_scale=256, train_wall=284, wall=2770
2022-07-07 16:13:46 | INFO | train_inner | epoch 001:    900 / 1122 loss=14.558, nll_loss=9.381, mask_ins=1.779, word_ins_ml=10.214, word_reposition=1.496, kpe=1.069, ppl=24114.5, wps=6279.2, ups=0.31, wpb=20464, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=2.803, clip=0, loss_scale=256, train_wall=284, wall=3095
2022-07-07 16:19:15 | INFO | train_inner | epoch 001:   1000 / 1122 loss=14.321, nll_loss=9.206, mask_ins=1.717, word_ins_ml=10.064, word_reposition=1.479, kpe=1.061, ppl=20462.7, wps=6260.4, ups=0.3, wpb=20597.8, bsz=256, num_updates=1000, lr=0.00010008, gnorm=2.496, clip=0, loss_scale=256, train_wall=287, wall=3425
2022-07-07 16:24:41 | INFO | train_inner | epoch 001:   1100 / 1122 loss=14.125, nll_loss=9.033, mask_ins=1.696, word_ins_ml=9.915, word_reposition=1.467, kpe=1.047, ppl=17861.5, wps=6270.9, ups=0.31, wpb=20473.7, bsz=256, num_updates=1100, lr=0.000110078, gnorm=2.362, clip=0, loss_scale=453, train_wall=284, wall=3751
2022-07-07 16:25:52 | INFO | train | epoch 001 | loss nan | nll_loss 10.309 | mask_ins 2.594 | word_ins_ml 11.014 | word_reposition 2.265 | kpe nan | ppl nan | wps 6259.7 | ups 0.31 | wpb 20520.3 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 6.408 | clip 1.8 | loss_scale 220 | train_wall 3209 | wall 3822
2022-07-07 16:27:16 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.921 | nll_loss 9.06 | mask_ins 1.764 | word_ins_ml 9.997 | word_reposition 1.739 | kpe 1.421 | ppl 31024.5 | wps 11742.8 | wpb 2367.6 | bsz 32 | num_updates 1122
2022-07-07 16:27:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 14.921) (writing took 6.375280007719994 seconds)
2022-07-07 16:31:37 | INFO | train_inner | epoch 002:     78 / 1122 loss=13.967, nll_loss=8.874, mask_ins=1.683, word_ins_ml=9.776, word_reposition=1.47, kpe=1.037, ppl=16010.1, wps=4893.5, ups=0.24, wpb=20333.3, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=2.315, clip=0, loss_scale=512, train_wall=283, wall=4167
2022-07-07 16:37:02 | INFO | train_inner | epoch 002:    178 / 1122 loss=13.765, nll_loss=8.692, mask_ins=1.668, word_ins_ml=9.619, word_reposition=1.453, kpe=1.025, ppl=13923.7, wps=6337.3, ups=0.31, wpb=20587.3, bsz=256, num_updates=1300, lr=0.000130074, gnorm=2.253, clip=0, loss_scale=512, train_wall=283, wall=4491
2022-07-07 16:42:27 | INFO | train_inner | epoch 002:    278 / 1122 loss=13.628, nll_loss=8.531, mask_ins=1.673, word_ins_ml=9.478, word_reposition=1.46, kpe=1.017, ppl=12658.3, wps=6320.5, ups=0.31, wpb=20599.8, bsz=256, num_updates=1400, lr=0.000140072, gnorm=2.277, clip=0, loss_scale=512, train_wall=284, wall=4817
2022-07-07 16:47:54 | INFO | train_inner | epoch 002:    378 / 1122 loss=13.412, nll_loss=8.341, mask_ins=1.659, word_ins_ml=9.312, word_reposition=1.429, kpe=1.012, ppl=10898.3, wps=6234, ups=0.31, wpb=20347.3, bsz=256, num_updates=1500, lr=0.00015007, gnorm=2.251, clip=0, loss_scale=512, train_wall=285, wall=5144
2022-07-07 16:53:20 | INFO | train_inner | epoch 002:    478 / 1122 loss=13.207, nll_loss=8.126, mask_ins=1.647, word_ins_ml=9.124, word_reposition=1.43, kpe=1.007, ppl=9455.8, wps=6315.6, ups=0.31, wpb=20567.7, bsz=256, num_updates=1600, lr=0.000160068, gnorm=2.207, clip=0, loss_scale=845, train_wall=284, wall=5469
2022-07-07 16:58:44 | INFO | train_inner | epoch 002:    578 / 1122 loss=12.952, nll_loss=7.813, mask_ins=1.661, word_ins_ml=8.848, word_reposition=1.43, kpe=1.013, ppl=7923.64, wps=6323.6, ups=0.31, wpb=20536.9, bsz=256, num_updates=1700, lr=0.000170066, gnorm=2.424, clip=0, loss_scale=1024, train_wall=284, wall=5794
2022-07-07 17:04:09 | INFO | train_inner | epoch 002:    678 / 1122 loss=12.568, nll_loss=7.398, mask_ins=1.647, word_ins_ml=8.486, word_reposition=1.417, kpe=1.019, ppl=6073.76, wps=6298.9, ups=0.31, wpb=20477.4, bsz=256, num_updates=1800, lr=0.000180064, gnorm=2.578, clip=0, loss_scale=1024, train_wall=283, wall=6119
2022-07-07 17:09:49 | INFO | train_inner | epoch 002:    778 / 1122 loss=nan, nll_loss=6.967, mask_ins=1.634, word_ins_ml=8.112, word_reposition=1.4, kpe=nan, ppl=nan, wps=6049.9, ups=0.29, wpb=20576, bsz=256, num_updates=1900, lr=0.000190062, gnorm=2.791, clip=0, loss_scale=1024, train_wall=298, wall=6459
2022-07-07 17:15:18 | INFO | train_inner | epoch 002:    878 / 1122 loss=11.784, nll_loss=6.573, mask_ins=1.63, word_ins_ml=7.771, word_reposition=1.362, kpe=1.021, ppl=3526.89, wps=6215.8, ups=0.3, wpb=20447.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=2.887, clip=0, loss_scale=1024, train_wall=287, wall=6788
2022-07-07 17:20:45 | INFO | train_inner | epoch 002:    978 / 1122 loss=11.496, nll_loss=6.29, mask_ins=1.62, word_ins_ml=7.529, word_reposition=1.331, kpe=1.016, ppl=2888.58, wps=6276.2, ups=0.31, wpb=20513.5, bsz=256, num_updates=2100, lr=0.000210058, gnorm=2.803, clip=0, loss_scale=1567, train_wall=285, wall=7115
2022-07-07 17:26:12 | INFO | train_inner | epoch 002:   1078 / 1122 loss=nan, nll_loss=6.041, mask_ins=1.628, word_ins_ml=7.315, word_reposition=1.316, kpe=nan, ppl=nan, wps=6346.5, ups=0.31, wpb=20708.1, bsz=256, num_updates=2200, lr=0.000220056, gnorm=2.778, clip=0, loss_scale=2048, train_wall=283, wall=7442
2022-07-07 17:28:34 | INFO | train | epoch 002 | loss nan | nll_loss 7.511 | mask_ins 1.649 | word_ins_ml 8.589 | word_reposition 1.404 | kpe nan | ppl nan | wps 6120.4 | ups 0.3 | wpb 20521 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 2.519 | clip 0 | loss_scale 1015 | train_wall 3202 | wall 7584
2022-07-07 17:29:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.54 | nll_loss 7.481 | mask_ins 1.654 | word_ins_ml 8.675 | word_reposition 1.775 | kpe 1.437 | ppl 11910.5 | wps 11797.8 | wpb 2367.6 | bsz 32 | num_updates 2244 | best_loss 13.54
2022-07-07 17:30:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 13.54) (writing took 8.785793817602098 seconds)
2022-07-07 17:33:09 | INFO | train_inner | epoch 003:     56 / 1122 loss=nan, nll_loss=5.847, mask_ins=1.625, word_ins_ml=7.147, word_reposition=1.297, kpe=nan, ppl=nan, wps=4882.3, ups=0.24, wpb=20387.7, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=2.785, clip=0, loss_scale=2048, train_wall=284, wall=7859
2022-07-07 17:38:35 | INFO | train_inner | epoch 003:    156 / 1122 loss=10.793, nll_loss=5.593, mask_ins=1.606, word_ins_ml=6.928, word_reposition=1.255, kpe=1.003, ppl=1773.8, wps=6284.3, ups=0.31, wpb=20466.9, bsz=256, num_updates=2400, lr=0.000240052, gnorm=2.627, clip=0, loss_scale=2048, train_wall=284, wall=8185
2022-07-07 17:44:01 | INFO | train_inner | epoch 003:    256 / 1122 loss=10.627, nll_loss=5.442, mask_ins=1.582, word_ins_ml=6.798, word_reposition=1.242, kpe=1.005, ppl=1581.75, wps=6318.1, ups=0.31, wpb=20590.4, bsz=256, num_updates=2500, lr=0.00025005, gnorm=2.54, clip=0, loss_scale=2048, train_wall=285, wall=8511
2022-07-07 17:49:27 | INFO | train_inner | epoch 003:    356 / 1122 loss=10.461, nll_loss=5.265, mask_ins=1.594, word_ins_ml=6.643, word_reposition=1.223, kpe=1.001, ppl=1409.21, wps=6300.6, ups=0.31, wpb=20552.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=2.561, clip=0, loss_scale=2888, train_wall=285, wall=8837
2022-07-07 17:54:52 | INFO | train_inner | epoch 003:    456 / 1122 loss=10.288, nll_loss=5.11, mask_ins=1.583, word_ins_ml=6.508, word_reposition=1.194, kpe=1.004, ppl=1250.52, wps=6264.7, ups=0.31, wpb=20384, bsz=256, num_updates=2700, lr=0.000270046, gnorm=2.494, clip=0, loss_scale=4096, train_wall=284, wall=9162
2022-07-07 18:00:18 | INFO | train_inner | epoch 003:    556 / 1122 loss=10.054, nll_loss=4.888, mask_ins=1.57, word_ins_ml=6.313, word_reposition=1.169, kpe=1.002, ppl=1063.02, wps=6292.2, ups=0.31, wpb=20480.9, bsz=256, num_updates=2800, lr=0.000280044, gnorm=2.506, clip=0, loss_scale=4096, train_wall=284, wall=9488
2022-07-07 18:05:42 | INFO | train_inner | epoch 003:    656 / 1122 loss=9.864, nll_loss=4.709, mask_ins=1.562, word_ins_ml=6.156, word_reposition=1.144, kpe=1.002, ppl=932.16, wps=6350.8, ups=0.31, wpb=20612.3, bsz=256, num_updates=2900, lr=0.000290042, gnorm=2.537, clip=0, loss_scale=4096, train_wall=284, wall=9812
2022-07-07 18:11:07 | INFO | train_inner | epoch 003:    756 / 1122 loss=9.489, nll_loss=4.365, mask_ins=1.526, word_ins_ml=5.854, word_reposition=1.105, kpe=1.004, ppl=718.33, wps=6347.8, ups=0.31, wpb=20597.8, bsz=256, num_updates=3000, lr=0.00030004, gnorm=2.671, clip=0, loss_scale=4096, train_wall=283, wall=10137
2022-07-07 18:16:37 | INFO | train_inner | epoch 003:    856 / 1122 loss=9.161, nll_loss=4.135, mask_ins=1.436, word_ins_ml=5.649, word_reposition=1.074, kpe=1.002, ppl=572.33, wps=6254.1, ups=0.3, wpb=20609.8, bsz=256, num_updates=3100, lr=0.000310038, gnorm=2.709, clip=0, loss_scale=5284, train_wall=287, wall=10466
2022-07-07 18:22:42 | INFO | train_inner | epoch 003:    956 / 1122 loss=nan, nll_loss=3.936, mask_ins=1.361, word_ins_ml=5.473, word_reposition=1.031, kpe=nan, ppl=nan, wps=5630.3, ups=0.27, wpb=20572.9, bsz=256, num_updates=3200, lr=0.000320036, gnorm=2.564, clip=0, loss_scale=8192, train_wall=323, wall=10832
2022-07-07 18:28:08 | INFO | train_inner | epoch 003:   1056 / 1122 loss=8.77, nll_loss=3.861, mask_ins=1.331, word_ins_ml=5.404, word_reposition=1.03, kpe=1.004, ppl=436.41, wps=6281.2, ups=0.31, wpb=20512.4, bsz=256, num_updates=3300, lr=0.000330034, gnorm=2.457, clip=0, loss_scale=8192, train_wall=284, wall=11158
2022-07-07 18:31:42 | INFO | train | epoch 003 | loss nan | nll_loss 4.728 | mask_ins 1.508 | word_ins_ml 6.17 | word_reposition 1.145 | kpe nan | ppl nan | wps 6078.5 | ups 0.3 | wpb 20521.3 | bsz 255.8 | num_updates 3366 | lr 0.000336633 | gnorm 2.575 | clip 0 | loss_scale 4598 | train_wall 3228 | wall 11372
2022-07-07 18:33:05 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.816 | nll_loss 6.884 | mask_ins 1.725 | word_ins_ml 8.22 | word_reposition 1.483 | kpe 1.387 | ppl 7212.95 | wps 11870.7 | wpb 2367.6 | bsz 32 | num_updates 3366 | best_loss 12.816
2022-07-07 18:33:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 3 @ 3366 updates, score 12.816) (writing took 9.448672360740602 seconds)
2022-07-07 18:35:05 | INFO | train_inner | epoch 004:     34 / 1122 loss=8.613, nll_loss=3.753, mask_ins=1.304, word_ins_ml=5.308, word_reposition=1, kpe=1.001, ppl=391.55, wps=4891.4, ups=0.24, wpb=20354.1, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=2.552, clip=0, loss_scale=8192, train_wall=282, wall=11575
2022-07-07 18:40:30 | INFO | train_inner | epoch 004:    134 / 1122 loss=8.517, nll_loss=3.688, mask_ins=1.282, word_ins_ml=5.25, word_reposition=0.991, kpe=0.994, ppl=366.25, wps=6284.3, ups=0.31, wpb=20472.6, bsz=256, num_updates=3500, lr=0.00035003, gnorm=2.302, clip=0, loss_scale=8192, train_wall=285, wall=11900
2022-07-07 18:45:57 | INFO | train_inner | epoch 004:    234 / 1122 loss=8.416, nll_loss=3.619, mask_ins=1.255, word_ins_ml=5.187, word_reposition=0.977, kpe=0.997, ppl=341.45, wps=6318.8, ups=0.31, wpb=20634.1, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.292, clip=0, loss_scale=9585, train_wall=284, wall=12227
2022-07-07 18:46:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-07 18:51:26 | INFO | train_inner | epoch 004:    335 / 1122 loss=8.359, nll_loss=3.57, mask_ins=1.244, word_ins_ml=5.142, word_reposition=0.978, kpe=0.995, ppl=328.25, wps=6204.9, ups=0.3, wpb=20429.3, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.251, clip=0, loss_scale=8922, train_wall=287, wall=12556
2022-07-07 18:56:52 | INFO | train_inner | epoch 004:    435 / 1122 loss=8.262, nll_loss=3.512, mask_ins=1.226, word_ins_ml=5.09, word_reposition=0.958, kpe=0.988, ppl=306.92, wps=6333.3, ups=0.31, wpb=20656.5, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.188, clip=0, loss_scale=8192, train_wall=284, wall=12882
2022-07-07 19:02:18 | INFO | train_inner | epoch 004:    535 / 1122 loss=8.276, nll_loss=3.527, mask_ins=1.219, word_ins_ml=5.101, word_reposition=0.958, kpe=0.998, ppl=310.04, wps=6290.8, ups=0.31, wpb=20481.9, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.214, clip=0, loss_scale=8192, train_wall=283, wall=13208
2022-07-07 19:07:43 | INFO | train_inner | epoch 004:    635 / 1122 loss=nan, nll_loss=3.476, mask_ins=1.214, word_ins_ml=5.055, word_reposition=0.952, kpe=nan, ppl=nan, wps=6318.8, ups=0.31, wpb=20519.3, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.148, clip=0, loss_scale=8192, train_wall=283, wall=13533
2022-07-07 19:13:12 | INFO | train_inner | epoch 004:    735 / 1122 loss=8.182, nll_loss=3.466, mask_ins=1.195, word_ins_ml=5.045, word_reposition=0.946, kpe=0.996, ppl=290.36, wps=6256.7, ups=0.3, wpb=20609.6, bsz=256, num_updates=4100, lr=0.000410018, gnorm=2.129, clip=0, loss_scale=8192, train_wall=288, wall=13862
2022-07-07 19:15:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-07 19:18:40 | INFO | train_inner | epoch 004:    836 / 1122 loss=nan, nll_loss=3.377, mask_ins=1.187, word_ins_ml=4.966, word_reposition=0.934, kpe=nan, ppl=nan, wps=6237.2, ups=0.3, wpb=20454.8, bsz=256, num_updates=4200, lr=0.000420016, gnorm=2.148, clip=0, loss_scale=10463, train_wall=286, wall=14190
2022-07-07 19:24:07 | INFO | train_inner | epoch 004:    936 / 1122 loss=8.082, nll_loss=3.385, mask_ins=1.184, word_ins_ml=4.971, word_reposition=0.936, kpe=0.991, ppl=270.98, wps=6323.1, ups=0.31, wpb=20641.5, bsz=256, num_updates=4300, lr=0.000430014, gnorm=2.044, clip=0, loss_scale=8192, train_wall=284, wall=14516
2022-07-07 19:29:33 | INFO | train_inner | epoch 004:   1036 / 1122 loss=8.073, nll_loss=3.394, mask_ins=1.168, word_ins_ml=4.978, word_reposition=0.934, kpe=0.993, ppl=269.3, wps=6272.5, ups=0.31, wpb=20455.6, bsz=256, num_updates=4400, lr=0.000440012, gnorm=2.044, clip=0, loss_scale=8192, train_wall=284, wall=14843
2022-07-07 19:32:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-07 19:34:50 | INFO | train | epoch 004 | loss nan | nll_loss 3.496 | mask_ins 1.216 | word_ins_ml 5.073 | word_reposition 0.956 | kpe nan | ppl nan | wps 6061.8 | ups 0.3 | wpb 20520.2 | bsz 255.8 | num_updates 4485 | lr 0.00044851 | gnorm 2.18 | clip 0 | loss_scale 8458 | train_wall 3225 | wall 15160
2022-07-07 19:36:13 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.889 | nll_loss 6.802 | mask_ins 1.594 | word_ins_ml 8.163 | word_reposition 1.641 | kpe 1.49 | ppl 7583.83 | wps 11809 | wpb 2367.6 | bsz 32 | num_updates 4485 | best_loss 12.816
2022-07-07 19:36:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_last.pt (epoch 4 @ 4485 updates, score 12.889) (writing took 5.551948370411992 seconds)
2022-07-07 19:37:08 | INFO | train_inner | epoch 005:     15 / 1122 loss=8.026, nll_loss=3.346, mask_ins=1.16, word_ins_ml=4.935, word_reposition=0.93, kpe=1, ppl=260.59, wps=4472.6, ups=0.22, wpb=20368.5, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=2.131, clip=0, loss_scale=6164, train_wall=324, wall=15298
2022-07-07 19:42:33 | INFO | train_inner | epoch 005:    115 / 1122 loss=7.987, nll_loss=3.319, mask_ins=1.163, word_ins_ml=4.91, word_reposition=0.929, kpe=0.985, ppl=253.76, wps=6355, ups=0.31, wpb=20646.2, bsz=256, num_updates=4600, lr=0.000460008, gnorm=1.985, clip=0, loss_scale=4096, train_wall=284, wall=15623
2022-07-07 19:47:59 | INFO | train_inner | epoch 005:    215 / 1122 loss=7.917, nll_loss=3.258, mask_ins=1.153, word_ins_ml=4.855, word_reposition=0.921, kpe=0.989, ppl=241.72, wps=6295.7, ups=0.31, wpb=20517.8, bsz=256, num_updates=4700, lr=0.000470006, gnorm=2.018, clip=0, loss_scale=4096, train_wall=284, wall=15949
2022-07-07 19:53:24 | INFO | train_inner | epoch 005:    315 / 1122 loss=7.976, nll_loss=3.308, mask_ins=1.163, word_ins_ml=4.898, word_reposition=0.922, kpe=0.993, ppl=251.73, wps=6316.6, ups=0.31, wpb=20565.4, bsz=256, num_updates=4800, lr=0.000480004, gnorm=2.026, clip=0, loss_scale=4096, train_wall=285, wall=16274
2022-07-07 19:58:51 | INFO | train_inner | epoch 005:    415 / 1122 loss=nan, nll_loss=3.294, mask_ins=1.137, word_ins_ml=4.885, word_reposition=0.932, kpe=nan, ppl=nan, wps=6283.7, ups=0.31, wpb=20494.5, bsz=256, num_updates=4900, lr=0.000490002, gnorm=2, clip=0, loss_scale=4096, train_wall=285, wall=16600
2022-07-07 20:04:16 | INFO | train_inner | epoch 005:    515 / 1122 loss=7.883, nll_loss=3.257, mask_ins=1.141, word_ins_ml=4.851, word_reposition=0.907, kpe=0.984, ppl=236.08, wps=6249.5, ups=0.31, wpb=20366.2, bsz=256, num_updates=5000, lr=0.0005, gnorm=1.923, clip=0, loss_scale=5652, train_wall=284, wall=16926
2022-07-07 20:06:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-07 20:09:49 | INFO | train_inner | epoch 005:    616 / 1122 loss=7.917, nll_loss=3.301, mask_ins=1.126, word_ins_ml=4.889, word_reposition=0.913, kpe=0.989, ppl=241.67, wps=6172.5, ups=0.3, wpb=20507.2, bsz=256, num_updates=5100, lr=0.000495074, gnorm=1.93, clip=0, loss_scale=5394, train_wall=290, wall=17259
2022-07-07 20:15:14 | INFO | train_inner | epoch 005:    716 / 1122 loss=7.849, nll_loss=3.224, mask_ins=1.141, word_ins_ml=4.82, word_reposition=0.901, kpe=0.987, ppl=230.6, wps=6308.8, ups=0.31, wpb=20528.7, bsz=256, num_updates=5200, lr=0.00049029, gnorm=1.899, clip=0, loss_scale=4096, train_wall=283, wall=17584
2022-07-07 20:20:40 | INFO | train_inner | epoch 005:    816 / 1122 loss=7.838, nll_loss=3.228, mask_ins=1.123, word_ins_ml=4.823, word_reposition=0.906, kpe=0.986, ppl=228.87, wps=6330.6, ups=0.31, wpb=20655.4, bsz=256, num_updates=5300, lr=0.000485643, gnorm=1.863, clip=0, loss_scale=4096, train_wall=285, wall=17910
2022-07-07 20:26:05 | INFO | train_inner | epoch 005:    916 / 1122 loss=7.786, nll_loss=3.176, mask_ins=1.119, word_ins_ml=4.776, word_reposition=0.905, kpe=0.987, ppl=220.75, wps=6340.1, ups=0.31, wpb=20560, bsz=256, num_updates=5400, lr=0.000481125, gnorm=1.867, clip=0, loss_scale=4096, train_wall=283, wall=18235
2022-07-07 20:31:31 | INFO | train_inner | epoch 005:   1016 / 1122 loss=nan, nll_loss=3.154, mask_ins=1.113, word_ins_ml=4.755, word_reposition=0.895, kpe=nan, ppl=nan, wps=6270.7, ups=0.31, wpb=20463.8, bsz=256, num_updates=5500, lr=0.000476731, gnorm=1.767, clip=0, loss_scale=4096, train_wall=284, wall=18561
2022-07-07 20:35:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-07 20:37:01 | INFO | train_inner | epoch 005:   1117 / 1122 loss=7.769, nll_loss=3.16, mask_ins=1.121, word_ins_ml=4.759, word_reposition=0.909, kpe=0.98, ppl=218.18, wps=6244.5, ups=0.3, wpb=20573.4, bsz=256, num_updates=5600, lr=0.000472456, gnorm=1.809, clip=0, loss_scale=5110, train_wall=287, wall=18890
2022-07-07 20:37:16 | INFO | train | epoch 005 | loss nan | nll_loss 3.246 | mask_ins 1.137 | word_ins_ml 4.841 | word_reposition 0.913 | kpe nan | ppl nan | wps 6135.5 | ups 0.3 | wpb 20520.7 | bsz 255.8 | num_updates 5605 | lr 0.000472245 | gnorm 1.93 | clip 0 | loss_scale 4443 | train_wall 3188 | wall 18905
2022-07-07 20:38:39 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 12.101 | nll_loss 6.276 | mask_ins 1.696 | word_ins_ml 7.661 | word_reposition 1.345 | kpe 1.4 | ppl 4393.79 | wps 11831.8 | wpb 2367.6 | bsz 32 | num_updates 5605 | best_loss 12.101
2022-07-07 20:38:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 5 @ 5605 updates, score 12.101) (writing took 9.703559354878962 seconds)
2022-07-07 20:43:58 | INFO | train_inner | epoch 006:     95 / 1122 loss=7.691, nll_loss=3.12, mask_ins=1.106, word_ins_ml=4.724, word_reposition=0.889, kpe=0.972, ppl=206.6, wps=4868.6, ups=0.24, wpb=20334.7, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=1.9, clip=0, loss_scale=4096, train_wall=283, wall=19308
2022-07-07 20:50:04 | INFO | train_inner | epoch 006:    195 / 1122 loss=7.677, nll_loss=3.117, mask_ins=1.097, word_ins_ml=4.72, word_reposition=0.887, kpe=0.973, ppl=204.66, wps=5625, ups=0.27, wpb=20602.5, bsz=256, num_updates=5800, lr=0.000464238, gnorm=1.777, clip=0, loss_scale=4096, train_wall=324, wall=19674
2022-07-07 20:55:31 | INFO | train_inner | epoch 006:    295 / 1122 loss=nan, nll_loss=3.058, mask_ins=1.089, word_ins_ml=4.667, word_reposition=0.884, kpe=nan, ppl=nan, wps=6313.2, ups=0.31, wpb=20599, bsz=256, num_updates=5900, lr=0.000460287, gnorm=1.773, clip=0, loss_scale=4096, train_wall=285, wall=20001
2022-07-07 21:00:56 | INFO | train_inner | epoch 006:    395 / 1122 loss=7.587, nll_loss=3.047, mask_ins=1.085, word_ins_ml=4.656, word_reposition=0.875, kpe=0.971, ppl=192.32, wps=6326.5, ups=0.31, wpb=20569.9, bsz=256, num_updates=6000, lr=0.000456435, gnorm=1.73, clip=0, loss_scale=4096, train_wall=283, wall=20326
2022-07-07 21:06:25 | INFO | train_inner | epoch 006:    495 / 1122 loss=7.598, nll_loss=3.08, mask_ins=1.079, word_ins_ml=4.684, word_reposition=0.869, kpe=0.965, ppl=193.72, wps=6226.5, ups=0.3, wpb=20499.6, bsz=256, num_updates=6100, lr=0.000452679, gnorm=1.673, clip=0, loss_scale=4956, train_wall=287, wall=20655
2022-07-07 21:07:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-07 21:11:54 | INFO | train_inner | epoch 006:    596 / 1122 loss=7.568, nll_loss=3.042, mask_ins=1.079, word_ins_ml=4.65, word_reposition=0.874, kpe=0.964, ppl=189.78, wps=6191.2, ups=0.3, wpb=20371.8, bsz=256, num_updates=6200, lr=0.000449013, gnorm=1.71, clip=0, loss_scale=4907, train_wall=287, wall=20984
2022-07-07 21:17:20 | INFO | train_inner | epoch 006:    696 / 1122 loss=7.559, nll_loss=3.043, mask_ins=1.07, word_ins_ml=4.65, word_reposition=0.873, kpe=0.965, ppl=188.55, wps=6338.2, ups=0.31, wpb=20637, bsz=256, num_updates=6300, lr=0.000445435, gnorm=1.706, clip=0, loss_scale=4096, train_wall=284, wall=21310
2022-07-07 21:22:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 21:22:48 | INFO | train_inner | epoch 006:    797 / 1122 loss=nan, nll_loss=2.939, mask_ins=1.064, word_ins_ml=4.557, word_reposition=0.868, kpe=nan, ppl=nan, wps=6251.4, ups=0.3, wpb=20511, bsz=256, num_updates=6400, lr=0.000441942, gnorm=1.685, clip=0, loss_scale=3893, train_wall=287, wall=21638
2022-07-07 21:22:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-07 21:23:56 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-07 21:28:20 | INFO | train_inner | epoch 006:    899 / 1122 loss=7.498, nll_loss=2.974, mask_ins=1.065, word_ins_ml=4.587, word_reposition=0.878, kpe=0.969, ppl=180.8, wps=6213.8, ups=0.3, wpb=20616.4, bsz=256, num_updates=6500, lr=0.000438529, gnorm=1.732, clip=0, loss_scale=632, train_wall=289, wall=21970
2022-07-07 21:33:45 | INFO | train_inner | epoch 006:    999 / 1122 loss=7.481, nll_loss=2.974, mask_ins=1.063, word_ins_ml=4.587, word_reposition=0.868, kpe=0.964, ppl=178.69, wps=6310.5, ups=0.31, wpb=20509.1, bsz=256, num_updates=6600, lr=0.000435194, gnorm=1.652, clip=0, loss_scale=512, train_wall=284, wall=22295
2022-07-07 21:39:09 | INFO | train_inner | epoch 006:   1099 / 1122 loss=7.461, nll_loss=2.958, mask_ins=1.06, word_ins_ml=4.572, word_reposition=0.865, kpe=0.963, ppl=176.14, wps=6316.4, ups=0.31, wpb=20497.3, bsz=256, num_updates=6700, lr=0.000431934, gnorm=1.648, clip=0, loss_scale=512, train_wall=283, wall=22619
2022-07-07 21:40:23 | INFO | train | epoch 006 | loss nan | nll_loss 3.031 | mask_ins 1.077 | word_ins_ml 4.64 | word_reposition 0.875 | kpe nan | ppl nan | wps 6058.3 | ups 0.3 | wpb 20521.2 | bsz 255.8 | num_updates 6723 | lr 0.000431195 | gnorm 1.722 | clip 0 | loss_scale 3200 | train_wall 3226 | wall 22692
2022-07-07 21:41:46 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.837 | nll_loss 6.034 | mask_ins 1.628 | word_ins_ml 7.437 | word_reposition 1.347 | kpe 1.425 | ppl 3659.09 | wps 11830.2 | wpb 2367.6 | bsz 32 | num_updates 6723 | best_loss 11.837
2022-07-07 21:41:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 6 @ 6723 updates, score 11.837) (writing took 9.598437875509262 seconds)
2022-07-07 21:46:06 | INFO | train_inner | epoch 007:     77 / 1122 loss=7.429, nll_loss=2.939, mask_ins=1.058, word_ins_ml=4.555, word_reposition=0.86, kpe=0.956, ppl=172.33, wps=4880.3, ups=0.24, wpb=20316.4, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=1.767, clip=0, loss_scale=512, train_wall=283, wall=23035
2022-07-07 21:51:31 | INFO | train_inner | epoch 007:    177 / 1122 loss=7.408, nll_loss=2.905, mask_ins=1.068, word_ins_ml=4.524, word_reposition=0.862, kpe=0.954, ppl=169.79, wps=6330.5, ups=0.31, wpb=20599.3, bsz=256, num_updates=6900, lr=0.000425628, gnorm=1.641, clip=0, loss_scale=512, train_wall=283, wall=23361
2022-07-07 21:56:55 | INFO | train_inner | epoch 007:    277 / 1122 loss=7.455, nll_loss=2.982, mask_ins=1.043, word_ins_ml=4.59, word_reposition=0.865, kpe=0.957, ppl=175.49, wps=6366, ups=0.31, wpb=20645.8, bsz=256, num_updates=7000, lr=0.000422577, gnorm=1.646, clip=0, loss_scale=870, train_wall=284, wall=23685
2022-07-07 22:03:04 | INFO | train_inner | epoch 007:    377 / 1122 loss=7.333, nll_loss=2.876, mask_ins=1.04, word_ins_ml=4.496, word_reposition=0.85, kpe=0.947, ppl=161.27, wps=5564.2, ups=0.27, wpb=20515.5, bsz=256, num_updates=7100, lr=0.000419591, gnorm=1.612, clip=0, loss_scale=1024, train_wall=326, wall=24054
2022-07-07 22:08:29 | INFO | train_inner | epoch 007:    477 / 1122 loss=nan, nll_loss=2.883, mask_ins=1.05, word_ins_ml=4.503, word_reposition=0.848, kpe=nan, ppl=nan, wps=6319.2, ups=0.31, wpb=20524.7, bsz=256, num_updates=7200, lr=0.000416667, gnorm=1.682, clip=0, loss_scale=1024, train_wall=283, wall=24379
2022-07-07 22:13:53 | INFO | train_inner | epoch 007:    577 / 1122 loss=7.355, nll_loss=2.897, mask_ins=1.046, word_ins_ml=4.514, word_reposition=0.84, kpe=0.955, ppl=163.74, wps=6299.3, ups=0.31, wpb=20453.1, bsz=256, num_updates=7300, lr=0.000413803, gnorm=1.608, clip=0, loss_scale=1024, train_wall=283, wall=24703
2022-07-07 22:19:18 | INFO | train_inner | epoch 007:    677 / 1122 loss=7.315, nll_loss=2.858, mask_ins=1.035, word_ins_ml=4.479, word_reposition=0.849, kpe=0.951, ppl=159.2, wps=6321.7, ups=0.31, wpb=20519.5, bsz=256, num_updates=7400, lr=0.000410997, gnorm=1.616, clip=0, loss_scale=1024, train_wall=283, wall=25028
2022-07-07 22:24:43 | INFO | train_inner | epoch 007:    777 / 1122 loss=7.303, nll_loss=2.847, mask_ins=1.039, word_ins_ml=4.469, word_reposition=0.847, kpe=0.948, ppl=157.9, wps=6332.4, ups=0.31, wpb=20558.8, bsz=256, num_updates=7500, lr=0.000408248, gnorm=1.631, clip=0, loss_scale=1618, train_wall=282, wall=25353
2022-07-07 22:30:07 | INFO | train_inner | epoch 007:    877 / 1122 loss=7.307, nll_loss=2.861, mask_ins=1.032, word_ins_ml=4.481, word_reposition=0.844, kpe=0.951, ppl=158.36, wps=6338.8, ups=0.31, wpb=20536.3, bsz=256, num_updates=7600, lr=0.000405554, gnorm=1.616, clip=0, loss_scale=2048, train_wall=282, wall=25677
2022-07-07 22:35:32 | INFO | train_inner | epoch 007:    977 / 1122 loss=7.249, nll_loss=2.799, mask_ins=1.036, word_ins_ml=4.426, word_reposition=0.839, kpe=0.949, ppl=152.15, wps=6340.5, ups=0.31, wpb=20602.1, bsz=256, num_updates=7700, lr=0.000402911, gnorm=1.591, clip=0, loss_scale=2048, train_wall=283, wall=26002
2022-07-07 22:40:56 | INFO | train_inner | epoch 007:   1077 / 1122 loss=7.261, nll_loss=2.837, mask_ins=1.012, word_ins_ml=4.459, word_reposition=0.842, kpe=0.948, ppl=153.39, wps=6324.7, ups=0.31, wpb=20515.4, bsz=256, num_updates=7800, lr=0.00040032, gnorm=1.592, clip=0, loss_scale=2048, train_wall=283, wall=26326
2022-07-07 22:43:21 | INFO | train | epoch 007 | loss nan | nll_loss 2.877 | mask_ins 1.041 | word_ins_ml 4.496 | word_reposition 0.849 | kpe nan | ppl nan | wps 6093.3 | ups 0.3 | wpb 20520.3 | bsz 255.8 | num_updates 7845 | lr 0.000399171 | gnorm 1.632 | clip 0 | loss_scale 1297 | train_wall 3219 | wall 26471
2022-07-07 22:44:44 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.676 | nll_loss 5.951 | mask_ins 1.546 | word_ins_ml 7.361 | word_reposition 1.348 | kpe 1.42 | ppl 3271.07 | wps 11941.4 | wpb 2367.6 | bsz 32 | num_updates 7845 | best_loss 11.676
2022-07-07 22:44:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 7 @ 7845 updates, score 11.676) (writing took 9.594273045659065 seconds)
2022-07-07 22:47:53 | INFO | train_inner | epoch 008:     55 / 1122 loss=7.265, nll_loss=2.827, mask_ins=1.033, word_ins_ml=4.449, word_reposition=0.84, kpe=0.943, ppl=153.77, wps=4891.5, ups=0.24, wpb=20388.8, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=1.599, clip=0, loss_scale=2048, train_wall=283, wall=26743
2022-07-07 22:53:18 | INFO | train_inner | epoch 008:    155 / 1122 loss=nan, nll_loss=2.79, mask_ins=1.019, word_ins_ml=4.416, word_reposition=0.836, kpe=nan, ppl=nan, wps=6304.6, ups=0.31, wpb=20508.8, bsz=256, num_updates=8000, lr=0.000395285, gnorm=1.564, clip=0, loss_scale=2990, train_wall=284, wall=27068
2022-07-07 22:58:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 22:58:51 | INFO | train_inner | epoch 008:    256 / 1122 loss=7.209, nll_loss=2.804, mask_ins=1.014, word_ins_ml=4.428, word_reposition=0.83, kpe=0.937, ppl=147.97, wps=6163.2, ups=0.3, wpb=20494.5, bsz=256, num_updates=8100, lr=0.000392837, gnorm=1.57, clip=0, loss_scale=3873, train_wall=289, wall=27401
2022-07-07 23:04:16 | INFO | train_inner | epoch 008:    356 / 1122 loss=7.167, nll_loss=2.757, mask_ins=1.008, word_ins_ml=4.386, word_reposition=0.832, kpe=0.942, ppl=143.74, wps=6322.8, ups=0.31, wpb=20554.5, bsz=256, num_updates=8200, lr=0.000390434, gnorm=1.561, clip=0, loss_scale=2048, train_wall=283, wall=27726
2022-07-07 23:09:40 | INFO | train_inner | epoch 008:    456 / 1122 loss=7.18, nll_loss=2.785, mask_ins=1.006, word_ins_ml=4.41, word_reposition=0.828, kpe=0.936, ppl=145, wps=6342.2, ups=0.31, wpb=20536.2, bsz=256, num_updates=8300, lr=0.000388075, gnorm=1.596, clip=0, loss_scale=2048, train_wall=283, wall=28050
2022-07-07 23:15:43 | INFO | train_inner | epoch 008:    556 / 1122 loss=7.193, nll_loss=2.786, mask_ins=1.008, word_ins_ml=4.411, word_reposition=0.835, kpe=0.939, ppl=146.28, wps=5663.1, ups=0.28, wpb=20568.8, bsz=256, num_updates=8400, lr=0.000385758, gnorm=1.6, clip=0, loss_scale=2048, train_wall=321, wall=28413
2022-07-07 23:21:08 | INFO | train_inner | epoch 008:    656 / 1122 loss=7.218, nll_loss=2.812, mask_ins=1.013, word_ins_ml=4.434, word_reposition=0.833, kpe=0.939, ppl=148.93, wps=6334, ups=0.31, wpb=20608.5, bsz=256, num_updates=8500, lr=0.000383482, gnorm=1.603, clip=0, loss_scale=2048, train_wall=284, wall=28738
2022-07-07 23:26:33 | INFO | train_inner | epoch 008:    756 / 1122 loss=7.137, nll_loss=2.736, mask_ins=1.007, word_ins_ml=4.366, word_reposition=0.829, kpe=0.936, ppl=140.79, wps=6316.2, ups=0.31, wpb=20489.7, bsz=256, num_updates=8600, lr=0.000381246, gnorm=1.538, clip=0, loss_scale=2048, train_wall=283, wall=29063
2022-07-07 23:31:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-07 23:32:00 | INFO | train_inner | epoch 008:    857 / 1122 loss=nan, nll_loss=2.746, mask_ins=1.007, word_ins_ml=4.373, word_reposition=0.825, kpe=nan, ppl=nan, wps=6286, ups=0.31, wpb=20606, bsz=256, num_updates=8700, lr=0.000379049, gnorm=1.576, clip=0, loss_scale=3772, train_wall=286, wall=29390
2022-07-07 23:37:26 | INFO | train_inner | epoch 008:    957 / 1122 loss=7.148, nll_loss=2.746, mask_ins=1.006, word_ins_ml=4.374, word_reposition=0.831, kpe=0.937, ppl=141.82, wps=6316.4, ups=0.31, wpb=20544.7, bsz=256, num_updates=8800, lr=0.000376889, gnorm=1.531, clip=0, loss_scale=2048, train_wall=284, wall=29716
2022-07-07 23:42:51 | INFO | train_inner | epoch 008:   1057 / 1122 loss=7.12, nll_loss=2.728, mask_ins=1.006, word_ins_ml=4.357, word_reposition=0.817, kpe=0.939, ppl=139.11, wps=6262.5, ups=0.31, wpb=20353.3, bsz=256, num_updates=8900, lr=0.000374766, gnorm=1.548, clip=0, loss_scale=2048, train_wall=283, wall=30041
2022-07-07 23:46:21 | INFO | train | epoch 008 | loss nan | nll_loss 2.77 | mask_ins 1.011 | word_ins_ml 4.397 | word_reposition 0.83 | kpe nan | ppl nan | wps 6081.2 | ups 0.3 | wpb 20520.9 | bsz 255.8 | num_updates 8965 | lr 0.000373405 | gnorm 1.572 | clip 0 | loss_scale 2451 | train_wall 3220 | wall 30250
2022-07-07 23:47:44 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.623 | nll_loss 5.975 | mask_ins 1.526 | word_ins_ml 7.38 | word_reposition 1.345 | kpe 1.372 | ppl 3155.14 | wps 11861.2 | wpb 2367.6 | bsz 32 | num_updates 8965 | best_loss 11.623
2022-07-07 23:47:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 8 @ 8965 updates, score 11.623) (writing took 9.588275560177863 seconds)
2022-07-07 23:49:48 | INFO | train_inner | epoch 009:     35 / 1122 loss=7.136, nll_loss=2.753, mask_ins=1.003, word_ins_ml=4.379, word_reposition=0.822, kpe=0.932, ppl=140.66, wps=4869.3, ups=0.24, wpb=20295.8, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=1.603, clip=0, loss_scale=2048, train_wall=282, wall=30457
2022-07-07 23:55:16 | INFO | train_inner | epoch 009:    135 / 1122 loss=7.071, nll_loss=2.699, mask_ins=0.992, word_ins_ml=4.331, word_reposition=0.822, kpe=0.926, ppl=134.45, wps=6296.9, ups=0.3, wpb=20664.1, bsz=256, num_updates=9100, lr=0.000370625, gnorm=1.478, clip=0, loss_scale=2048, train_wall=285, wall=30786
2022-07-08 00:00:40 | INFO | train_inner | epoch 009:    235 / 1122 loss=7.099, nll_loss=2.72, mask_ins=1.003, word_ins_ml=4.35, word_reposition=0.823, kpe=0.923, ppl=137.05, wps=6313.6, ups=0.31, wpb=20477.3, bsz=256, num_updates=9200, lr=0.000368605, gnorm=1.453, clip=0, loss_scale=2109, train_wall=283, wall=31110
2022-07-08 00:06:04 | INFO | train_inner | epoch 009:    335 / 1122 loss=7.054, nll_loss=2.696, mask_ins=0.996, word_ins_ml=4.327, word_reposition=0.806, kpe=0.925, ppl=132.85, wps=6308, ups=0.31, wpb=20452.7, bsz=256, num_updates=9300, lr=0.000366618, gnorm=1.523, clip=0, loss_scale=4096, train_wall=283, wall=31434
2022-07-08 00:07:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-08 00:11:32 | INFO | train_inner | epoch 009:    436 / 1122 loss=nan, nll_loss=2.666, mask_ins=0.993, word_ins_ml=4.301, word_reposition=0.821, kpe=nan, ppl=nan, wps=6244.4, ups=0.31, wpb=20462.1, bsz=256, num_updates=9400, lr=0.000364662, gnorm=1.542, clip=0, loss_scale=2656, train_wall=286, wall=31762
2022-07-08 00:16:56 | INFO | train_inner | epoch 009:    536 / 1122 loss=7.027, nll_loss=2.684, mask_ins=0.975, word_ins_ml=4.317, word_reposition=0.811, kpe=0.924, ppl=130.42, wps=6340, ups=0.31, wpb=20567.8, bsz=256, num_updates=9500, lr=0.000362738, gnorm=1.498, clip=0, loss_scale=2048, train_wall=283, wall=32086
2022-07-08 00:22:35 | INFO | train_inner | epoch 009:    636 / 1122 loss=7.058, nll_loss=2.703, mask_ins=0.989, word_ins_ml=4.333, word_reposition=0.814, kpe=0.922, ppl=133.28, wps=6072.1, ups=0.3, wpb=20553.7, bsz=256, num_updates=9600, lr=0.000360844, gnorm=1.527, clip=0, loss_scale=2048, train_wall=297, wall=32425
2022-07-08 00:28:27 | INFO | train_inner | epoch 009:    736 / 1122 loss=7.019, nll_loss=2.657, mask_ins=0.983, word_ins_ml=4.293, word_reposition=0.816, kpe=0.926, ppl=129.66, wps=5842.4, ups=0.28, wpb=20547.1, bsz=256, num_updates=9700, lr=0.000358979, gnorm=1.534, clip=0, loss_scale=2048, train_wall=310, wall=32776
2022-07-08 00:33:51 | INFO | train_inner | epoch 009:    836 / 1122 loss=7.067, nll_loss=2.701, mask_ins=0.988, word_ins_ml=4.331, word_reposition=0.822, kpe=0.926, ppl=134.09, wps=6327.4, ups=0.31, wpb=20500.3, bsz=256, num_updates=9800, lr=0.000357143, gnorm=1.518, clip=0, loss_scale=2048, train_wall=283, wall=33100
2022-07-08 00:38:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-08 00:39:18 | INFO | train_inner | epoch 009:    937 / 1122 loss=7.007, nll_loss=2.658, mask_ins=0.987, word_ins_ml=4.292, word_reposition=0.807, kpe=0.921, ppl=128.59, wps=6301.8, ups=0.31, wpb=20609, bsz=256, num_updates=9900, lr=0.000355335, gnorm=1.48, clip=0, loss_scale=3123, train_wall=286, wall=33428
2022-07-08 00:44:43 | INFO | train_inner | epoch 009:   1037 / 1122 loss=nan, nll_loss=2.691, mask_ins=0.979, word_ins_ml=4.321, word_reposition=0.814, kpe=nan, ppl=nan, wps=6299.2, ups=0.31, wpb=20500.4, bsz=256, num_updates=10000, lr=0.000353553, gnorm=1.534, clip=0, loss_scale=2048, train_wall=284, wall=33753
2022-07-08 00:48:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 00:49:19 | INFO | train | epoch 009 | loss nan | nll_loss 2.69 | mask_ins 0.988 | word_ins_ml 4.322 | word_reposition 0.815 | kpe nan | ppl nan | wps 6078.5 | ups 0.3 | wpb 20523.1 | bsz 255.8 | num_updates 10084 | lr 0.000352078 | gnorm 1.518 | clip 0 | loss_scale 2376 | train_wall 3219 | wall 34029
2022-07-08 00:50:44 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.546 | nll_loss 5.905 | mask_ins 1.493 | word_ins_ml 7.32 | word_reposition 1.325 | kpe 1.407 | ppl 2989.74 | wps 11667.8 | wpb 2367.6 | bsz 32 | num_updates 10084 | best_loss 11.546
2022-07-08 00:50:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 9 @ 10084 updates, score 11.546) (writing took 9.984263881109655 seconds)
2022-07-08 00:51:48 | INFO | train_inner | epoch 010:     16 / 1122 loss=7.044, nll_loss=2.693, mask_ins=0.985, word_ins_ml=4.323, word_reposition=0.814, kpe=0.922, ppl=132, wps=4824.9, ups=0.24, wpb=20497.4, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=1.671, clip=0, loss_scale=1754, train_wall=288, wall=34178
2022-07-08 00:57:13 | INFO | train_inner | epoch 010:    116 / 1122 loss=6.963, nll_loss=2.621, mask_ins=0.971, word_ins_ml=4.259, word_reposition=0.812, kpe=0.922, ppl=124.74, wps=6339.5, ups=0.31, wpb=20579.8, bsz=256, num_updates=10200, lr=0.00035007, gnorm=1.664, clip=0, loss_scale=1024, train_wall=284, wall=34502
2022-07-08 01:02:37 | INFO | train_inner | epoch 010:    216 / 1122 loss=6.993, nll_loss=2.669, mask_ins=0.97, word_ins_ml=4.302, word_reposition=0.807, kpe=0.914, ppl=127.34, wps=6321.9, ups=0.31, wpb=20526, bsz=256, num_updates=10300, lr=0.000348367, gnorm=1.608, clip=0, loss_scale=1024, train_wall=284, wall=34827
2022-07-08 01:08:01 | INFO | train_inner | epoch 010:    316 / 1122 loss=6.963, nll_loss=2.634, mask_ins=0.968, word_ins_ml=4.27, word_reposition=0.81, kpe=0.915, ppl=124.76, wps=6348.3, ups=0.31, wpb=20577.8, bsz=256, num_updates=10400, lr=0.000346688, gnorm=1.519, clip=0, loss_scale=1024, train_wall=283, wall=35151
2022-07-08 01:13:27 | INFO | train_inner | epoch 010:    416 / 1122 loss=7.005, nll_loss=2.651, mask_ins=0.987, word_ins_ml=4.285, word_reposition=0.814, kpe=0.919, ppl=128.46, wps=6309.1, ups=0.31, wpb=20553, bsz=256, num_updates=10500, lr=0.000345033, gnorm=1.616, clip=0, loss_scale=1024, train_wall=284, wall=35477
2022-07-08 01:18:52 | INFO | train_inner | epoch 010:    516 / 1122 loss=6.966, nll_loss=2.629, mask_ins=0.976, word_ins_ml=4.264, word_reposition=0.811, kpe=0.915, ppl=125.05, wps=6310.1, ups=0.31, wpb=20520.6, bsz=256, num_updates=10600, lr=0.000343401, gnorm=1.561, clip=0, loss_scale=1198, train_wall=284, wall=35802
2022-07-08 01:19:35 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-08 01:20:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-08 01:24:24 | INFO | train_inner | epoch 010:    618 / 1122 loss=nan, nll_loss=2.612, mask_ins=0.969, word_ins_ml=4.25, word_reposition=0.803, kpe=nan, ppl=nan, wps=6235.8, ups=0.3, wpb=20671, bsz=256, num_updates=10700, lr=0.000341793, gnorm=1.639, clip=0, loss_scale=773, train_wall=290, wall=36134
2022-07-08 01:29:49 | INFO | train_inner | epoch 010:    718 / 1122 loss=6.941, nll_loss=2.61, mask_ins=0.976, word_ins_ml=4.248, word_reposition=0.797, kpe=0.92, ppl=122.83, wps=6273.6, ups=0.31, wpb=20401.7, bsz=256, num_updates=10800, lr=0.000340207, gnorm=1.614, clip=0, loss_scale=512, train_wall=283, wall=36459
2022-07-08 01:35:39 | INFO | train_inner | epoch 010:    818 / 1122 loss=nan, nll_loss=2.63, mask_ins=0.958, word_ins_ml=4.266, word_reposition=0.802, kpe=nan, ppl=nan, wps=5870, ups=0.29, wpb=20566.7, bsz=256, num_updates=10900, lr=0.000338643, gnorm=1.651, clip=0, loss_scale=512, train_wall=309, wall=36809
2022-07-08 01:41:17 | INFO | train_inner | epoch 010:    918 / 1122 loss=6.924, nll_loss=2.603, mask_ins=0.962, word_ins_ml=4.241, word_reposition=0.798, kpe=0.922, ppl=121.46, wps=6086.8, ups=0.3, wpb=20525.6, bsz=256, num_updates=11000, lr=0.0003371, gnorm=1.854, clip=0, loss_scale=512, train_wall=296, wall=37147
2022-07-08 01:46:39 | INFO | train_inner | epoch 010:   1018 / 1122 loss=6.928, nll_loss=2.599, mask_ins=0.966, word_ins_ml=4.238, word_reposition=0.802, kpe=0.922, ppl=121.81, wps=6336.8, ups=0.31, wpb=20454.5, bsz=256, num_updates=11100, lr=0.000335578, gnorm=1.504, clip=0, loss_scale=512, train_wall=282, wall=37469
2022-07-08 01:52:08 | INFO | train_inner | epoch 010:   1118 / 1122 loss=6.934, nll_loss=2.605, mask_ins=0.97, word_ins_ml=4.242, word_reposition=0.802, kpe=0.919, ppl=122.25, wps=6239.5, ups=0.3, wpb=20469.9, bsz=256, num_updates=11200, lr=0.000334077, gnorm=1.61, clip=0, loss_scale=829, train_wall=286, wall=37797
2022-07-08 01:52:20 | INFO | train | epoch 010 | loss nan | nll_loss 2.625 | mask_ins 0.97 | word_ins_ml 4.261 | word_reposition 0.806 | kpe nan | ppl nan | wps 6078.3 | ups 0.3 | wpb 20519.8 | bsz 255.8 | num_updates 11204 | lr 0.000334017 | gnorm 1.632 | clip 0 | loss_scale 817 | train_wall 3221 | wall 37810
2022-07-08 01:53:44 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.493 | nll_loss 5.838 | mask_ins 1.481 | word_ins_ml 7.26 | word_reposition 1.343 | kpe 1.408 | ppl 2882.06 | wps 11764.5 | wpb 2367.6 | bsz 32 | num_updates 11204 | best_loss 11.493
2022-07-08 01:53:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 10 @ 11204 updates, score 11.493) (writing took 9.714191910810769 seconds)
2022-07-08 01:59:05 | INFO | train_inner | epoch 011:     96 / 1122 loss=6.896, nll_loss=2.582, mask_ins=0.955, word_ins_ml=4.222, word_reposition=0.81, kpe=0.908, ppl=119.11, wps=4885.9, ups=0.24, wpb=20418.2, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=1.663, clip=0, loss_scale=1024, train_wall=283, wall=38215
2022-07-08 01:59:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-08 02:00:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 256.0
2022-07-08 02:04:37 | INFO | train_inner | epoch 011:    198 / 1122 loss=nan, nll_loss=2.544, mask_ins=0.961, word_ins_ml=4.188, word_reposition=0.794, kpe=nan, ppl=nan, wps=6183.3, ups=0.3, wpb=20504.5, bsz=256, num_updates=11400, lr=0.000331133, gnorm=1.57, clip=0, loss_scale=351, train_wall=289, wall=38547
2022-07-08 02:10:01 | INFO | train_inner | epoch 011:    298 / 1122 loss=nan, nll_loss=2.578, mask_ins=0.957, word_ins_ml=4.218, word_reposition=0.808, kpe=nan, ppl=nan, wps=6314.1, ups=0.31, wpb=20476.2, bsz=256, num_updates=11500, lr=0.00032969, gnorm=1.549, clip=0, loss_scale=256, train_wall=283, wall=38871
2022-07-08 02:15:27 | INFO | train_inner | epoch 011:    398 / 1122 loss=6.87, nll_loss=2.563, mask_ins=0.963, word_ins_ml=4.204, word_reposition=0.795, kpe=0.908, ppl=116.97, wps=6323.2, ups=0.31, wpb=20590.3, bsz=256, num_updates=11600, lr=0.000328266, gnorm=1.507, clip=0, loss_scale=256, train_wall=284, wall=39197
2022-07-08 02:20:52 | INFO | train_inner | epoch 011:    498 / 1122 loss=6.926, nll_loss=2.623, mask_ins=0.957, word_ins_ml=4.258, word_reposition=0.802, kpe=0.91, ppl=121.64, wps=6350.2, ups=0.31, wpb=20620.6, bsz=256, num_updates=11700, lr=0.00032686, gnorm=1.484, clip=0, loss_scale=256, train_wall=284, wall=39522
2022-07-08 02:26:17 | INFO | train_inner | epoch 011:    598 / 1122 loss=6.886, nll_loss=2.592, mask_ins=0.957, word_ins_ml=4.229, word_reposition=0.794, kpe=0.906, ppl=118.25, wps=6330, ups=0.31, wpb=20575.7, bsz=256, num_updates=11800, lr=0.000325472, gnorm=1.678, clip=0, loss_scale=256, train_wall=284, wall=39847
2022-07-08 02:31:42 | INFO | train_inner | epoch 011:    698 / 1122 loss=6.872, nll_loss=2.576, mask_ins=0.951, word_ins_ml=4.215, word_reposition=0.796, kpe=0.91, ppl=117.15, wps=6303, ups=0.31, wpb=20510.4, bsz=256, num_updates=11900, lr=0.000324102, gnorm=1.547, clip=0, loss_scale=440, train_wall=284, wall=40172
2022-07-08 02:37:07 | INFO | train_inner | epoch 011:    798 / 1122 loss=6.889, nll_loss=2.577, mask_ins=0.961, word_ins_ml=4.216, word_reposition=0.797, kpe=0.914, ppl=118.53, wps=6260.9, ups=0.31, wpb=20341.8, bsz=256, num_updates=12000, lr=0.000322749, gnorm=1.584, clip=0, loss_scale=512, train_wall=283, wall=40497
2022-07-08 02:42:32 | INFO | train_inner | epoch 011:    898 / 1122 loss=6.856, nll_loss=2.567, mask_ins=0.95, word_ins_ml=4.208, word_reposition=0.789, kpe=0.909, ppl=115.88, wps=6349.9, ups=0.31, wpb=20607.2, bsz=256, num_updates=12100, lr=0.000321412, gnorm=1.688, clip=0, loss_scale=512, train_wall=283, wall=40822
2022-07-08 02:48:26 | INFO | train_inner | epoch 011:    998 / 1122 loss=6.872, nll_loss=2.58, mask_ins=0.951, word_ins_ml=4.219, word_reposition=0.79, kpe=0.912, ppl=117.12, wps=5782, ups=0.28, wpb=20502.1, bsz=256, num_updates=12200, lr=0.000320092, gnorm=1.525, clip=0, loss_scale=512, train_wall=312, wall=41176
2022-07-08 02:54:03 | INFO | train_inner | epoch 011:   1098 / 1122 loss=6.838, nll_loss=2.556, mask_ins=0.943, word_ins_ml=4.197, word_reposition=0.789, kpe=0.909, ppl=114.44, wps=6103.7, ups=0.3, wpb=20567.2, bsz=256, num_updates=12300, lr=0.000318788, gnorm=1.584, clip=0, loss_scale=512, train_wall=295, wall=41513
2022-07-08 02:55:20 | INFO | train | epoch 011 | loss nan | nll_loss 2.576 | mask_ins 0.955 | word_ins_ml 4.216 | word_reposition 0.797 | kpe nan | ppl nan | wps 6080.3 | ups 0.3 | wpb 20522.3 | bsz 255.8 | num_updates 12324 | lr 0.000318478 | gnorm 1.578 | clip 0 | loss_scale 444 | train_wall 3221 | wall 41590
2022-07-08 02:56:43 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.431 | nll_loss 5.846 | mask_ins 1.476 | word_ins_ml 7.271 | word_reposition 1.284 | kpe 1.4 | ppl 2760.18 | wps 11855.3 | wpb 2367.6 | bsz 32 | num_updates 12324 | best_loss 11.431
2022-07-08 02:56:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_bert12_adaptor_kpe_3decoder_cased/checkpoint_best.pt (epoch 11 @ 12324 updates, score 11.431) (writing took 9.533674528822303 seconds)
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
