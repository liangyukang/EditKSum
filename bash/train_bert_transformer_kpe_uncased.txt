nohup: ignoring input
2022-07-05 16:39:34 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15633
2022-07-05 16:39:34 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15633
2022-07-05 16:39:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-07-05 16:39:34 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15633
2022-07-05 16:39:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-07-05 16:39:34 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:15633
2022-07-05 16:39:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-07-05 16:39:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-07-05 16:39:34 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-05 16:39:34 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0
2022-07-05 16:39:34 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-05 16:39:34 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1
2022-07-05 16:39:34 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-05 16:39:34 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3
2022-07-05 16:39:34 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-07-05 16:39:34 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2
2022-07-05 16:39:38 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, criterion='nat_loss', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', task='translation_lev', num_workers=0, skip_invalid_size_inputs_valid_test=True, max_tokens=None, max_sentences=8, required_batch_size_multiple=8, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=7, disable_validation=False, max_tokens_valid=None, max_sentences_valid=8, curriculum=0, distributed_world_size=4, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:15633', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, arch='kpe_editor_transformer_with_adapter', max_epoch=0, max_update=100000, clip_norm=25, sentence_avg=False, update_freq=[8], lr=[0.0005], min_lr=1e-09, use_bmuf=False, save_dir='../checkpoints_bert_transformer_kpe_uncased', restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, keep_best_checkpoints=3, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, early_exit='6,6,6', layers_num='6,6,6', finetune_embeddings=False, finetune_whole_encoder=False, decoder_adapter_dimention=2048, finetune_position_embeddings=False, use_adapter_bert=False, keywords_num=40, label_smoothing=0.1, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, warmup_updates=5000, warmup_init_lr=1e-07, data='/data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510', source_lang=None, target_lang=None, load_alignments=False, left_pad_source='False', left_pad_target='False', max_source_positions=512, max_target_positions=512, upsample_primary=1, truncate_source=False, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, noise='random_delete_shuffle', random_seed=1, cached_features_dir='/data/yukangliang/实验/BertKpeEditorWithAdaptor/cached_examples_bert_uncased_510', tokenizer_dir='/data/yukangliang/预训练模型/bert-base-uncased', encoder_adapter_dimention=2048, decoder_input='target', kpe=True, share_all_embeddings=True, no_share_discriminator=True, dropout=0.3, decoder_learned_pos=True, encoder_learned_pos=True, apply_bert_init=True, cache_dir='/data/yukangliang/预训练模型/bert-base-uncased', decoder_cache_dir='/data/yukangliang/预训练模型/bert-base-uncased-decoder', share_decoder_input_output_embed=False, encoder='bert', decoder='transformer', encoder_embed_path=None, encoder_embed_dim=768, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, decoder_embed_path=None, decoder_embed_dim=768, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_input=False, decoder_output_dim=768, decoder_input_dim=768, no_share_maskpredictor=False, share_discriminator_maskpredictor=False, no_share_last_layer=False, cached_dir='/data/yukangliang/预训练模型/bert-base-uncased')
2022-07-05 16:39:38 | INFO | fairseq.tasks.translation | [source] dictionary: 30522 types
2022-07-05 16:39:38 | INFO | fairseq.tasks.translation | [target] dictionary: 30522 types
2022-07-05 16:39:38 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/valid.source-target.source
start load cached examples valid ...
0it [00:00, ?it/s]2022-07-05 16:39:38 | INFO | fairseq.data.data_utils | loaded 13368 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/valid.source-target.target
2022-07-05 16:39:38 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510 valid source-target 13368 examples
start load cached examples valid ...
start load cached examples valid ...
0it [00:00, ?it/s]0it [00:00, ?it/s]start load cached examples valid ...
0it [00:00, ?it/s]394it [00:00, 3939.36it/s]394it [00:00, 3935.33it/s]399it [00:00, 3966.05it/s]397it [00:00, 3964.31it/s]788it [00:00, 3576.83it/s]796it [00:00, 3602.76it/s]788it [00:00, 3468.29it/s]794it [00:00, 3584.48it/s]1148it [00:00, 3571.39it/s]1165it [00:00, 3639.89it/s]1156it [00:00, 3556.21it/s]1155it [00:00, 3569.26it/s]1507it [00:00, 3467.98it/s]1531it [00:00, 3477.59it/s]1515it [00:00, 3413.44it/s]1514it [00:00, 3446.47it/s]1887it [00:00, 3580.40it/s]1913it [00:00, 3592.29it/s]1868it [00:00, 3452.07it/s]1892it [00:00, 3560.59it/s]2247it [00:00, 3489.45it/s]2274it [00:00, 3517.13it/s]2250it [00:00, 3494.59it/s]2234it [00:00, 3408.72it/s]2635it [00:00, 3612.11it/s]2672it [00:00, 3662.46it/s]2636it [00:00, 3609.61it/s]2636it [00:00, 3598.44it/s]3036it [00:00, 3734.37it/s]3035it [00:00, 3727.06it/s]3033it [00:00, 3712.32it/s]3059it [00:00, 3592.64it/s]3411it [00:00, 3631.71it/s]3454it [00:00, 3697.85it/s]3409it [00:00, 3620.79it/s]3406it [00:00, 3555.86it/s]3788it [00:01, 3672.05it/s]3846it [00:01, 3761.36it/s]3801it [00:01, 3709.95it/s]3798it [00:01, 3660.88it/s]4157it [00:01, 3586.45it/s]4224it [00:01, 3639.20it/s]4174it [00:01, 3566.01it/s]4167it [00:01, 3566.16it/s]4527it [00:01, 3618.22it/s]4604it [00:01, 3685.26it/s]4559it [00:01, 3647.29it/s]4537it [00:01, 3603.08it/s]4890it [00:01, 3526.45it/s]4974it [00:01, 3570.16it/s]4926it [00:01, 3499.24it/s]4899it [00:01, 3494.85it/s]5269it [00:01, 3603.17it/s]5354it [00:01, 3636.39it/s]5306it [00:01, 3584.52it/s]5277it [00:01, 3574.62it/s]5631it [00:01, 3387.97it/s]5719it [00:01, 3427.37it/s]5636it [00:01, 3413.12it/s]5667it [00:01, 3306.62it/s]5979it [00:01, 3412.14it/s]6065it [00:01, 3431.93it/s]5980it [00:01, 3415.55it/s]6025it [00:01, 3381.33it/s]6323it [00:01, 3419.77it/s]6338it [00:01, 3460.66it/s]6382it [00:01, 3433.14it/s]6419it [00:01, 3300.37it/s]6752it [00:02, 2120.86it/s]6686it [00:02, 2026.81it/s]6667it [00:02, 1927.71it/s]6729it [00:02, 1916.96it/s]7105it [00:02, 2408.74it/s]7041it [00:02, 2324.17it/s]7011it [00:02, 2214.48it/s]7083it [00:02, 2219.08it/s]7398it [00:02, 2523.92it/s]7344it [00:02, 2458.90it/s]7344it [00:02, 2401.22it/s]7379it [00:02, 2372.18it/s]7739it [00:02, 2739.45it/s]7683it [00:02, 2679.15it/s]7700it [00:02, 2665.75it/s]7726it [00:02, 2624.89it/s]8089it [00:02, 2935.43it/s]8026it [00:02, 2868.37it/s]8055it [00:02, 2883.13it/s]8064it [00:02, 2811.99it/s]8409it [00:02, 2814.72it/s]8346it [00:02, 2867.70it/s]8381it [00:02, 2912.72it/s]8383it [00:02, 2857.69it/s]8763it [00:02, 3006.19it/s]8703it [00:02, 3054.26it/s]8724it [00:02, 3050.33it/s]8739it [00:02, 3045.03it/s]9080it [00:02, 2999.95it/s]9027it [00:02, 2992.36it/s]9050it [00:02, 3032.44it/s]9065it [00:02, 3033.03it/s]9431it [00:02, 3141.64it/s]9380it [00:02, 3139.19it/s]9400it [00:02, 3160.65it/s]9409it [00:02, 3146.37it/s]9774it [00:03, 3221.70it/s]9738it [00:03, 3262.87it/s]9747it [00:03, 3248.00it/s]9767it [00:03, 3268.59it/s]10103it [00:03, 3148.96it/s]10073it [00:03, 3140.48it/s]10080it [00:03, 3170.96it/s]10103it [00:03, 3190.22it/s]10456it [00:03, 3257.19it/s]10425it [00:03, 3240.35it/s]10431it [00:03, 3267.51it/s]10448it [00:03, 3263.71it/s]10786it [00:03, 3171.04it/s]10754it [00:03, 3152.10it/s]10763it [00:03, 3163.36it/s]10780it [00:03, 3180.98it/s]11120it [00:03, 3218.71it/s]11103it [00:03, 3247.91it/s]11112it [00:03, 3255.78it/s]11131it [00:03, 3274.71it/s]11472it [00:03, 3306.00it/s]11437it [00:03, 3273.70it/s]11466it [00:03, 3335.34it/s]11471it [00:03, 3309.95it/s]11805it [00:03, 3119.39it/s]11802it [00:03, 3209.12it/s]11767it [00:03, 3038.04it/s]11805it [00:03, 3120.69it/s]12141it [00:03, 3187.38it/s]12156it [00:03, 3302.76it/s]12113it [00:03, 3154.07it/s]12159it [00:03, 3238.42it/s]12463it [00:03, 3114.65it/s]12489it [00:03, 3212.22it/s]12433it [00:03, 3092.08it/s]12487it [00:03, 3133.26it/s]12813it [00:03, 3224.38it/s]12834it [00:04, 3278.22it/s]12786it [00:04, 3214.84it/s]12841it [00:04, 3248.25it/s]13163it [00:04, 3302.46it/s]13185it [00:04, 3345.30it/s]13139it [00:04, 3305.59it/s]13194it [00:04, 3327.14it/s]13368it [00:04, 3202.12it/s]
2022-07-05 16:39:42 | INFO | root | success load 13368 data
2022-07-05 16:39:42 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-uncased' is a path or url to a directory containing tokenizer files.
2022-07-05 16:39:42 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/added_tokens.json. We won't load it.
2022-07-05 16:39:42 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/special_tokens_map.json. We won't load it.
2022-07-05 16:39:42 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/vocab.txt
2022-07-05 16:39:42 | INFO | transformer.tokenization_utils | loading file None
2022-07-05 16:39:42 | INFO | transformer.tokenization_utils | loading file None
2022-07-05 16:39:42 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/tokenizer_config.json
13368it [00:04, 3184.59it/s]
13368it [00:04, 3167.53it/s]
13368it [00:04, 3163.96it/s]
2022-07-05 16:39:43 | INFO | transformer.configuration_utils | loading configuration file /data/yukangliang/预训练模型/bert-base-uncased/config.json
2022-07-05 16:39:43 | INFO | transformer.configuration_utils | Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pruned_heads": {},
  "torchscript": false,
  "transformers_version": "4.6.0.dev0",
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "use_cache": true,
  "vocab_size": 30522
}

2022-07-05 16:39:43 | INFO | transformer.modeling_utils | loading weights file /data/yukangliang/预训练模型/bert-base-uncased/pytorch_model.bin
2022-07-05 16:39:44 | INFO | transformer.modeling_utils | Weights of BertEncoder not initialized from pretrained model: ['kpe.cnn2gram.cnn_list.0.weight', 'kpe.cnn2gram.cnn_list.0.bias', 'kpe.classifier.weight', 'kpe.classifier.bias', 'kpe.chunk_classifier.weight', 'kpe.chunk_classifier.bias']
2022-07-05 16:39:44 | INFO | transformer.modeling_utils | Weights from pretrained model not used in BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
Trained parameters: len 520
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 400
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']Trained parameters: len 520
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 400
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
start load cached examples train ...
0it [00:00, ?it/s]Trained parameters: len 520
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 400
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
2022-07-05 16:39:46 | INFO | fairseq_cli.train | KPEEDITORTransformerModel(
  (encoder): BertEncoder(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (kpe): Kpe(
      (cnn2gram): NGramers(
        (cnn_list): ModuleList(
          (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))
        )
        (relu): ReLU()
        (dropout): Dropout(p=0.05, inplace=False)
      )
      (classifier): Linear(in_features=512, out_features=1, bias=True)
      (chunk_classifier): Linear(in_features=512, out_features=2, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (decoder): EditorTransformerDecoder(
    (embed_tokens): Embedding(30522, 768, padding_idx=0)
    (embed_positions): LearnedPositionalEmbedding(513, 768, padding_idx=0)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=30522, bias=False)
    (embed_mask_ins): Embedding(256, 1536)
    (layers_reposition): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
2022-07-05 16:39:46 | INFO | fairseq_cli.train | model kpe_editor_transformer_with_adapter, criterion LabelSmoothedDualImitationCriterion
2022-07-05 16:39:46 | INFO | fairseq_cli.train | num. model params: 228640259 (num. trained: 228640259)
2022-07-05 16:39:46 | INFO | fairseq_cli.train | num. Encoder model params: 109877507 (Encoder num. trained: 109877507)
2022-07-05 16:39:46 | INFO | fairseq_cli.train | num. Decoder model params: 118762752 (Decoder num. trained: 118762752)
357it [00:00, 3454.24it/s]Trained parameters: len 520
Trained parameters: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.encoder_attn.k_proj.weight', 'decoder.layers_reposition.0.encoder_attn.k_proj.bias', 'decoder.layers_reposition.0.encoder_attn.v_proj.weight', 'decoder.layers_reposition.0.encoder_attn.v_proj.bias', 'decoder.layers_reposition.0.encoder_attn.q_proj.weight', 'decoder.layers_reposition.0.encoder_attn.q_proj.bias', 'decoder.layers_reposition.0.encoder_attn.out_proj.weight', 'decoder.layers_reposition.0.encoder_attn.out_proj.bias', 'decoder.layers_reposition.0.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.0.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.encoder_attn.k_proj.weight', 'decoder.layers_reposition.1.encoder_attn.k_proj.bias', 'decoder.layers_reposition.1.encoder_attn.v_proj.weight', 'decoder.layers_reposition.1.encoder_attn.v_proj.bias', 'decoder.layers_reposition.1.encoder_attn.q_proj.weight', 'decoder.layers_reposition.1.encoder_attn.q_proj.bias', 'decoder.layers_reposition.1.encoder_attn.out_proj.weight', 'decoder.layers_reposition.1.encoder_attn.out_proj.bias', 'decoder.layers_reposition.1.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.1.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.encoder_attn.k_proj.weight', 'decoder.layers_reposition.2.encoder_attn.k_proj.bias', 'decoder.layers_reposition.2.encoder_attn.v_proj.weight', 'decoder.layers_reposition.2.encoder_attn.v_proj.bias', 'decoder.layers_reposition.2.encoder_attn.q_proj.weight', 'decoder.layers_reposition.2.encoder_attn.q_proj.bias', 'decoder.layers_reposition.2.encoder_attn.out_proj.weight', 'decoder.layers_reposition.2.encoder_attn.out_proj.bias', 'decoder.layers_reposition.2.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.2.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.encoder_attn.k_proj.weight', 'decoder.layers_reposition.3.encoder_attn.k_proj.bias', 'decoder.layers_reposition.3.encoder_attn.v_proj.weight', 'decoder.layers_reposition.3.encoder_attn.v_proj.bias', 'decoder.layers_reposition.3.encoder_attn.q_proj.weight', 'decoder.layers_reposition.3.encoder_attn.q_proj.bias', 'decoder.layers_reposition.3.encoder_attn.out_proj.weight', 'decoder.layers_reposition.3.encoder_attn.out_proj.bias', 'decoder.layers_reposition.3.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.3.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.encoder_attn.k_proj.weight', 'decoder.layers_reposition.4.encoder_attn.k_proj.bias', 'decoder.layers_reposition.4.encoder_attn.v_proj.weight', 'decoder.layers_reposition.4.encoder_attn.v_proj.bias', 'decoder.layers_reposition.4.encoder_attn.q_proj.weight', 'decoder.layers_reposition.4.encoder_attn.q_proj.bias', 'decoder.layers_reposition.4.encoder_attn.out_proj.weight', 'decoder.layers_reposition.4.encoder_attn.out_proj.bias', 'decoder.layers_reposition.4.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.4.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.encoder_attn.k_proj.weight', 'decoder.layers_reposition.5.encoder_attn.k_proj.bias', 'decoder.layers_reposition.5.encoder_attn.v_proj.weight', 'decoder.layers_reposition.5.encoder_attn.v_proj.bias', 'decoder.layers_reposition.5.encoder_attn.q_proj.weight', 'decoder.layers_reposition.5.encoder_attn.q_proj.bias', 'decoder.layers_reposition.5.encoder_attn.out_proj.weight', 'decoder.layers_reposition.5.encoder_attn.out_proj.bias', 'decoder.layers_reposition.5.encoder_attn_layer_norm.weight', 'decoder.layers_reposition.5.encoder_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']
Trained parameters not adapter: len 400
Trained parameters not adapter: ['encoder.bert.embeddings.word_embeddings.weight', 'encoder.bert.embeddings.position_embeddings.weight', 'encoder.bert.embeddings.token_type_embeddings.weight', 'encoder.bert.embeddings.LayerNorm.weight', 'encoder.bert.embeddings.LayerNorm.bias', 'encoder.bert.encoder.layer.0.attention.self.query.weight', 'encoder.bert.encoder.layer.0.attention.self.query.bias', 'encoder.bert.encoder.layer.0.attention.self.key.weight', 'encoder.bert.encoder.layer.0.attention.self.key.bias', 'encoder.bert.encoder.layer.0.attention.self.value.weight', 'encoder.bert.encoder.layer.0.attention.self.value.bias', 'encoder.bert.encoder.layer.0.attention.output.dense.weight', 'encoder.bert.encoder.layer.0.attention.output.dense.bias', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.0.intermediate.dense.weight', 'encoder.bert.encoder.layer.0.intermediate.dense.bias', 'encoder.bert.encoder.layer.0.output.dense.weight', 'encoder.bert.encoder.layer.0.output.dense.bias', 'encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.attention.self.query.weight', 'encoder.bert.encoder.layer.1.attention.self.query.bias', 'encoder.bert.encoder.layer.1.attention.self.key.weight', 'encoder.bert.encoder.layer.1.attention.self.key.bias', 'encoder.bert.encoder.layer.1.attention.self.value.weight', 'encoder.bert.encoder.layer.1.attention.self.value.bias', 'encoder.bert.encoder.layer.1.attention.output.dense.weight', 'encoder.bert.encoder.layer.1.attention.output.dense.bias', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.1.intermediate.dense.weight', 'encoder.bert.encoder.layer.1.intermediate.dense.bias', 'encoder.bert.encoder.layer.1.output.dense.weight', 'encoder.bert.encoder.layer.1.output.dense.bias', 'encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.attention.self.query.weight', 'encoder.bert.encoder.layer.2.attention.self.query.bias', 'encoder.bert.encoder.layer.2.attention.self.key.weight', 'encoder.bert.encoder.layer.2.attention.self.key.bias', 'encoder.bert.encoder.layer.2.attention.self.value.weight', 'encoder.bert.encoder.layer.2.attention.self.value.bias', 'encoder.bert.encoder.layer.2.attention.output.dense.weight', 'encoder.bert.encoder.layer.2.attention.output.dense.bias', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.2.intermediate.dense.weight', 'encoder.bert.encoder.layer.2.intermediate.dense.bias', 'encoder.bert.encoder.layer.2.output.dense.weight', 'encoder.bert.encoder.layer.2.output.dense.bias', 'encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.attention.self.query.weight', 'encoder.bert.encoder.layer.3.attention.self.query.bias', 'encoder.bert.encoder.layer.3.attention.self.key.weight', 'encoder.bert.encoder.layer.3.attention.self.key.bias', 'encoder.bert.encoder.layer.3.attention.self.value.weight', 'encoder.bert.encoder.layer.3.attention.self.value.bias', 'encoder.bert.encoder.layer.3.attention.output.dense.weight', 'encoder.bert.encoder.layer.3.attention.output.dense.bias', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.3.intermediate.dense.weight', 'encoder.bert.encoder.layer.3.intermediate.dense.bias', 'encoder.bert.encoder.layer.3.output.dense.weight', 'encoder.bert.encoder.layer.3.output.dense.bias', 'encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.attention.self.query.weight', 'encoder.bert.encoder.layer.4.attention.self.query.bias', 'encoder.bert.encoder.layer.4.attention.self.key.weight', 'encoder.bert.encoder.layer.4.attention.self.key.bias', 'encoder.bert.encoder.layer.4.attention.self.value.weight', 'encoder.bert.encoder.layer.4.attention.self.value.bias', 'encoder.bert.encoder.layer.4.attention.output.dense.weight', 'encoder.bert.encoder.layer.4.attention.output.dense.bias', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.4.intermediate.dense.weight', 'encoder.bert.encoder.layer.4.intermediate.dense.bias', 'encoder.bert.encoder.layer.4.output.dense.weight', 'encoder.bert.encoder.layer.4.output.dense.bias', 'encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.attention.self.query.weight', 'encoder.bert.encoder.layer.5.attention.self.query.bias', 'encoder.bert.encoder.layer.5.attention.self.key.weight', 'encoder.bert.encoder.layer.5.attention.self.key.bias', 'encoder.bert.encoder.layer.5.attention.self.value.weight', 'encoder.bert.encoder.layer.5.attention.self.value.bias', 'encoder.bert.encoder.layer.5.attention.output.dense.weight', 'encoder.bert.encoder.layer.5.attention.output.dense.bias', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.5.intermediate.dense.weight', 'encoder.bert.encoder.layer.5.intermediate.dense.bias', 'encoder.bert.encoder.layer.5.output.dense.weight', 'encoder.bert.encoder.layer.5.output.dense.bias', 'encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.attention.self.query.weight', 'encoder.bert.encoder.layer.6.attention.self.query.bias', 'encoder.bert.encoder.layer.6.attention.self.key.weight', 'encoder.bert.encoder.layer.6.attention.self.key.bias', 'encoder.bert.encoder.layer.6.attention.self.value.weight', 'encoder.bert.encoder.layer.6.attention.self.value.bias', 'encoder.bert.encoder.layer.6.attention.output.dense.weight', 'encoder.bert.encoder.layer.6.attention.output.dense.bias', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.6.intermediate.dense.weight', 'encoder.bert.encoder.layer.6.intermediate.dense.bias', 'encoder.bert.encoder.layer.6.output.dense.weight', 'encoder.bert.encoder.layer.6.output.dense.bias', 'encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.attention.self.query.weight', 'encoder.bert.encoder.layer.7.attention.self.query.bias', 'encoder.bert.encoder.layer.7.attention.self.key.weight', 'encoder.bert.encoder.layer.7.attention.self.key.bias', 'encoder.bert.encoder.layer.7.attention.self.value.weight', 'encoder.bert.encoder.layer.7.attention.self.value.bias', 'encoder.bert.encoder.layer.7.attention.output.dense.weight', 'encoder.bert.encoder.layer.7.attention.output.dense.bias', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.7.intermediate.dense.weight', 'encoder.bert.encoder.layer.7.intermediate.dense.bias', 'encoder.bert.encoder.layer.7.output.dense.weight', 'encoder.bert.encoder.layer.7.output.dense.bias', 'encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.attention.self.query.weight', 'encoder.bert.encoder.layer.8.attention.self.query.bias', 'encoder.bert.encoder.layer.8.attention.self.key.weight', 'encoder.bert.encoder.layer.8.attention.self.key.bias', 'encoder.bert.encoder.layer.8.attention.self.value.weight', 'encoder.bert.encoder.layer.8.attention.self.value.bias', 'encoder.bert.encoder.layer.8.attention.output.dense.weight', 'encoder.bert.encoder.layer.8.attention.output.dense.bias', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.8.intermediate.dense.weight', 'encoder.bert.encoder.layer.8.intermediate.dense.bias', 'encoder.bert.encoder.layer.8.output.dense.weight', 'encoder.bert.encoder.layer.8.output.dense.bias', 'encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.attention.self.query.weight', 'encoder.bert.encoder.layer.9.attention.self.query.bias', 'encoder.bert.encoder.layer.9.attention.self.key.weight', 'encoder.bert.encoder.layer.9.attention.self.key.bias', 'encoder.bert.encoder.layer.9.attention.self.value.weight', 'encoder.bert.encoder.layer.9.attention.self.value.bias', 'encoder.bert.encoder.layer.9.attention.output.dense.weight', 'encoder.bert.encoder.layer.9.attention.output.dense.bias', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.9.intermediate.dense.weight', 'encoder.bert.encoder.layer.9.intermediate.dense.bias', 'encoder.bert.encoder.layer.9.output.dense.weight', 'encoder.bert.encoder.layer.9.output.dense.bias', 'encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.attention.self.query.weight', 'encoder.bert.encoder.layer.10.attention.self.query.bias', 'encoder.bert.encoder.layer.10.attention.self.key.weight', 'encoder.bert.encoder.layer.10.attention.self.key.bias', 'encoder.bert.encoder.layer.10.attention.self.value.weight', 'encoder.bert.encoder.layer.10.attention.self.value.bias', 'encoder.bert.encoder.layer.10.attention.output.dense.weight', 'encoder.bert.encoder.layer.10.attention.output.dense.bias', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.10.intermediate.dense.weight', 'encoder.bert.encoder.layer.10.intermediate.dense.bias', 'encoder.bert.encoder.layer.10.output.dense.weight', 'encoder.bert.encoder.layer.10.output.dense.bias', 'encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.attention.self.query.weight', 'encoder.bert.encoder.layer.11.attention.self.query.bias', 'encoder.bert.encoder.layer.11.attention.self.key.weight', 'encoder.bert.encoder.layer.11.attention.self.key.bias', 'encoder.bert.encoder.layer.11.attention.self.value.weight', 'encoder.bert.encoder.layer.11.attention.self.value.bias', 'encoder.bert.encoder.layer.11.attention.output.dense.weight', 'encoder.bert.encoder.layer.11.attention.output.dense.bias', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.bert.encoder.layer.11.intermediate.dense.weight', 'encoder.bert.encoder.layer.11.intermediate.dense.bias', 'encoder.bert.encoder.layer.11.output.dense.weight', 'encoder.bert.encoder.layer.11.output.dense.bias', 'encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'encoder.bert.pooler.dense.weight', 'encoder.bert.pooler.dense.bias', 'encoder.kpe.cnn2gram.cnn_list.0.weight', 'encoder.kpe.cnn2gram.cnn_list.0.bias', 'encoder.kpe.classifier.weight', 'encoder.kpe.classifier.bias', 'encoder.kpe.chunk_classifier.weight', 'encoder.kpe.chunk_classifier.bias', 'decoder.embed_tokens.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.0.fc1.weight', 'decoder.layers.0.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.2.fc1.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.3.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.fc2.weight', 'decoder.layers.4.fc2.bias', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.embed_mask_ins.weight', 'decoder.layers_reposition.0.self_attn.k_proj.weight', 'decoder.layers_reposition.0.self_attn.k_proj.bias', 'decoder.layers_reposition.0.self_attn.v_proj.weight', 'decoder.layers_reposition.0.self_attn.v_proj.bias', 'decoder.layers_reposition.0.self_attn.q_proj.weight', 'decoder.layers_reposition.0.self_attn.q_proj.bias', 'decoder.layers_reposition.0.self_attn.out_proj.weight', 'decoder.layers_reposition.0.self_attn.out_proj.bias', 'decoder.layers_reposition.0.self_attn_layer_norm.weight', 'decoder.layers_reposition.0.self_attn_layer_norm.bias', 'decoder.layers_reposition.0.fc1.weight', 'decoder.layers_reposition.0.fc1.bias', 'decoder.layers_reposition.0.fc2.weight', 'decoder.layers_reposition.0.fc2.bias', 'decoder.layers_reposition.0.final_layer_norm.weight', 'decoder.layers_reposition.0.final_layer_norm.bias', 'decoder.layers_reposition.1.self_attn.k_proj.weight', 'decoder.layers_reposition.1.self_attn.k_proj.bias', 'decoder.layers_reposition.1.self_attn.v_proj.weight', 'decoder.layers_reposition.1.self_attn.v_proj.bias', 'decoder.layers_reposition.1.self_attn.q_proj.weight', 'decoder.layers_reposition.1.self_attn.q_proj.bias', 'decoder.layers_reposition.1.self_attn.out_proj.weight', 'decoder.layers_reposition.1.self_attn.out_proj.bias', 'decoder.layers_reposition.1.self_attn_layer_norm.weight', 'decoder.layers_reposition.1.self_attn_layer_norm.bias', 'decoder.layers_reposition.1.fc1.weight', 'decoder.layers_reposition.1.fc1.bias', 'decoder.layers_reposition.1.fc2.weight', 'decoder.layers_reposition.1.fc2.bias', 'decoder.layers_reposition.1.final_layer_norm.weight', 'decoder.layers_reposition.1.final_layer_norm.bias', 'decoder.layers_reposition.2.self_attn.k_proj.weight', 'decoder.layers_reposition.2.self_attn.k_proj.bias', 'decoder.layers_reposition.2.self_attn.v_proj.weight', 'decoder.layers_reposition.2.self_attn.v_proj.bias', 'decoder.layers_reposition.2.self_attn.q_proj.weight', 'decoder.layers_reposition.2.self_attn.q_proj.bias', 'decoder.layers_reposition.2.self_attn.out_proj.weight', 'decoder.layers_reposition.2.self_attn.out_proj.bias', 'decoder.layers_reposition.2.self_attn_layer_norm.weight', 'decoder.layers_reposition.2.self_attn_layer_norm.bias', 'decoder.layers_reposition.2.fc1.weight', 'decoder.layers_reposition.2.fc1.bias', 'decoder.layers_reposition.2.fc2.weight', 'decoder.layers_reposition.2.fc2.bias', 'decoder.layers_reposition.2.final_layer_norm.weight', 'decoder.layers_reposition.2.final_layer_norm.bias', 'decoder.layers_reposition.3.self_attn.k_proj.weight', 'decoder.layers_reposition.3.self_attn.k_proj.bias', 'decoder.layers_reposition.3.self_attn.v_proj.weight', 'decoder.layers_reposition.3.self_attn.v_proj.bias', 'decoder.layers_reposition.3.self_attn.q_proj.weight', 'decoder.layers_reposition.3.self_attn.q_proj.bias', 'decoder.layers_reposition.3.self_attn.out_proj.weight', 'decoder.layers_reposition.3.self_attn.out_proj.bias', 'decoder.layers_reposition.3.self_attn_layer_norm.weight', 'decoder.layers_reposition.3.self_attn_layer_norm.bias', 'decoder.layers_reposition.3.fc1.weight', 'decoder.layers_reposition.3.fc1.bias', 'decoder.layers_reposition.3.fc2.weight', 'decoder.layers_reposition.3.fc2.bias', 'decoder.layers_reposition.3.final_layer_norm.weight', 'decoder.layers_reposition.3.final_layer_norm.bias', 'decoder.layers_reposition.4.self_attn.k_proj.weight', 'decoder.layers_reposition.4.self_attn.k_proj.bias', 'decoder.layers_reposition.4.self_attn.v_proj.weight', 'decoder.layers_reposition.4.self_attn.v_proj.bias', 'decoder.layers_reposition.4.self_attn.q_proj.weight', 'decoder.layers_reposition.4.self_attn.q_proj.bias', 'decoder.layers_reposition.4.self_attn.out_proj.weight', 'decoder.layers_reposition.4.self_attn.out_proj.bias', 'decoder.layers_reposition.4.self_attn_layer_norm.weight', 'decoder.layers_reposition.4.self_attn_layer_norm.bias', 'decoder.layers_reposition.4.fc1.weight', 'decoder.layers_reposition.4.fc1.bias', 'decoder.layers_reposition.4.fc2.weight', 'decoder.layers_reposition.4.fc2.bias', 'decoder.layers_reposition.4.final_layer_norm.weight', 'decoder.layers_reposition.4.final_layer_norm.bias', 'decoder.layers_reposition.5.self_attn.k_proj.weight', 'decoder.layers_reposition.5.self_attn.k_proj.bias', 'decoder.layers_reposition.5.self_attn.v_proj.weight', 'decoder.layers_reposition.5.self_attn.v_proj.bias', 'decoder.layers_reposition.5.self_attn.q_proj.weight', 'decoder.layers_reposition.5.self_attn.q_proj.bias', 'decoder.layers_reposition.5.self_attn.out_proj.weight', 'decoder.layers_reposition.5.self_attn.out_proj.bias', 'decoder.layers_reposition.5.self_attn_layer_norm.weight', 'decoder.layers_reposition.5.self_attn_layer_norm.bias', 'decoder.layers_reposition.5.fc1.weight', 'decoder.layers_reposition.5.fc1.bias', 'decoder.layers_reposition.5.fc2.weight', 'decoder.layers_reposition.5.fc2.bias', 'decoder.layers_reposition.5.final_layer_norm.weight', 'decoder.layers_reposition.5.final_layer_norm.bias']716it [00:00, 3528.77it/s]2022-07-05 16:39:46 | INFO | fairseq_cli.train | training on 4 GPUs
2022-07-05 16:39:46 | INFO | fairseq_cli.train | max tokens per GPU = None and max sentences per GPU = 8
2022-07-05 16:39:46 | INFO | fairseq.trainer | no existing checkpoint found ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt
2022-07-05 16:39:46 | INFO | fairseq.trainer | loading train data for epoch 1

start load cached examples train ...
0it [00:00, ?it/s]1099it [00:00, 3661.82it/s]2022-07-05 16:39:46 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/train.source-target.source
2022-07-05 16:39:46 | INFO | fairseq.data.data_utils | loaded 287112 examples from: /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510/train.source-target.target
2022-07-05 16:39:46 | INFO | fairseq.tasks.translation | /data/yukangliang/实验/BertKpeEditorWithAdaptor/data-bin-bert-uncased-510 train source-target 287112 examples
start load cached examples train ...
0it [00:00, ?it/s]357it [00:00, 3310.12it/s]1466it [00:00, 3482.52it/s]356it [00:00, 3404.72it/s]
start load cached examples train ...
0it [00:00, ?it/s]733it [00:00, 3561.30it/s]1841it [00:00, 3573.39it/s]732it [00:00, 3605.39it/s]1091it [00:00, 3569.18it/s]357it [00:00, 3254.84it/s]2200it [00:00, 3473.71it/s]1094it [00:00, 3609.76it/s]716it [00:00, 3442.46it/s]1449it [00:00, 3194.65it/s]2570it [00:00, 3542.35it/s]1456it [00:00, 3396.48it/s]1099it [00:00, 3611.69it/s]1816it [00:00, 3352.68it/s]2926it [00:00, 3454.99it/s]1839it [00:00, 3545.74it/s]1461it [00:00, 3391.57it/s]2156it [00:00, 3261.97it/s]3307it [00:00, 3559.63it/s]2196it [00:00, 3445.51it/s]1845it [00:00, 3543.63it/s]2523it [00:00, 3386.11it/s]3672it [00:01, 3585.96it/s]2566it [00:00, 3524.74it/s]2202it [00:00, 3432.73it/s]4032it [00:01, 3421.72it/s]2877it [00:00, 3222.23it/s]2920it [00:00, 3440.37it/s]2573it [00:00, 3516.56it/s]4414it [00:01, 3536.23it/s]3258it [00:00, 3391.19it/s]3294it [00:00, 3528.76it/s]2927it [00:00, 3431.27it/s]3630it [00:01, 3487.23it/s]4770it [00:01, 3427.99it/s]3669it [00:01, 3594.00it/s]3297it [00:00, 3504.00it/s]5147it [00:01, 3524.70it/s]3982it [00:01, 3336.87it/s]4030it [00:01, 3475.24it/s]3676it [00:01, 3587.05it/s]4360it [00:01, 3462.61it/s]5502it [00:01, 3437.36it/s]4405it [00:01, 3554.86it/s]4036it [00:01, 3467.04it/s]5878it [00:01, 3530.27it/s]4710it [00:01, 3303.73it/s]4762it [00:01, 3434.52it/s]4385it [00:01, 3451.65it/s]6237it [00:01, 3429.94it/s]5061it [00:01, 3361.82it/s]5127it [00:01, 3496.23it/s]4732it [00:01, 3372.52it/s]6613it [00:01, 3523.17it/s]5400it [00:01, 3281.85it/s]5478it [00:01, 3376.89it/s]5107it [00:01, 3481.65it/s]6974it [00:01, 3547.12it/s]5755it [00:01, 3356.58it/s]5853it [00:01, 3483.53it/s]5457it [00:01, 3389.66it/s]7330it [00:02, 3443.28it/s]6135it [00:01, 3482.45it/s]6235it [00:01, 3580.85it/s]5817it [00:01, 3448.21it/s]7696it [00:02, 3491.08it/s]6485it [00:01, 3337.41it/s]6595it [00:01, 3439.26it/s]6200it [00:01, 3559.38it/s]6863it [00:02, 3461.22it/s]6977it [00:01, 3546.17it/s]6557it [00:01, 3457.17it/s]7212it [00:02, 3333.87it/s]7334it [00:02, 3450.80it/s]6937it [00:01, 3546.40it/s]7588it [00:02, 3454.84it/s]7706it [00:02, 3521.35it/s]7293it [00:02, 3420.34it/s]7936it [00:02, 3356.14it/s]7675it [00:02, 3533.49it/s]8047it [00:03, 1131.35it/s]8426it [00:03, 1446.52it/s]8792it [00:03, 1766.66it/s]9116it [00:03, 2018.88it/s]8060it [00:03, 1144.92it/s]9504it [00:03, 2384.27it/s]8442it [00:03, 1462.32it/s]9843it [00:03, 2582.49it/s]8030it [00:03, 1136.92it/s]8813it [00:03, 1787.68it/s]8274it [00:03, 983.59it/s] 10225it [00:03, 2875.57it/s]8405it [00:03, 1444.35it/s]9132it [00:03, 2026.86it/s]8639it [00:03, 1268.02it/s]8775it [00:03, 1768.61it/s]10574it [00:03, 2970.79it/s]9517it [00:03, 2384.51it/s]8936it [00:03, 1493.07it/s]10950it [00:03, 3176.93it/s]9093it [00:03, 1980.43it/s]9296it [00:03, 1827.45it/s]9856it [00:03, 2487.98it/s]11331it [00:03, 3347.90it/s]9461it [00:03, 2309.40it/s]9666it [00:03, 2173.11it/s]10237it [00:03, 2792.13it/s]9789it [00:03, 2499.08it/s]11692it [00:04, 3317.47it/s]9993it [00:03, 2370.73it/s]10576it [00:03, 2889.53it/s]10162it [00:03, 2787.91it/s]12074it [00:04, 3457.71it/s]10365it [00:03, 2677.27it/s]10957it [00:03, 3124.97it/s]10508it [00:03, 2902.53it/s]12434it [00:04, 3399.48it/s]10702it [00:04, 2789.01it/s]11345it [00:03, 3184.83it/s]10883it [00:03, 3121.53it/s]12820it [00:04, 3529.28it/s]11079it [00:04, 3038.88it/s]11710it [00:04, 3308.15it/s]11261it [00:03, 3298.76it/s]13181it [00:04, 3504.16it/s]11422it [00:04, 3055.45it/s]12099it [00:04, 3467.81it/s]11617it [00:04, 3268.70it/s]13574it [00:04, 3626.02it/s]11800it [00:04, 3249.63it/s]12460it [00:04, 3427.50it/s]11978it [00:04, 3356.64it/s]13941it [00:04, 3550.19it/s]12170it [00:04, 3373.45it/s]12835it [00:04, 3517.95it/s]12327it [00:04, 3334.92it/s]14338it [00:04, 3669.35it/s]12524it [00:04, 3310.47it/s]13195it [00:04, 3482.87it/s]12702it [00:04, 3452.14it/s]14708it [00:04, 3546.42it/s]12895it [00:04, 3423.10it/s]13579it [00:04, 3584.70it/s]13055it [00:04, 3391.13it/s]15104it [00:04, 3664.93it/s]13246it [00:04, 3358.43it/s]13942it [00:04, 3516.61it/s]13447it [00:04, 3541.60it/s]15479it [00:05, 3688.22it/s]13622it [00:04, 3472.30it/s]14326it [00:04, 3607.02it/s]13819it [00:04, 3593.05it/s]15850it [00:05, 3565.76it/s]13975it [00:04, 3328.69it/s]14706it [00:04, 3529.10it/s]14182it [00:04, 3507.94it/s]16223it [00:05, 3612.42it/s]14356it [00:05, 3464.22it/s]15107it [00:05, 3664.63it/s]14569it [00:04, 3612.35it/s]16586it [00:05, 3518.88it/s]15476it [00:05, 3656.12it/s]14708it [00:05, 3356.59it/s]14933it [00:04, 3517.73it/s]16976it [00:05, 3627.70it/s]15102it [00:05, 3520.81it/s]15844it [00:05, 3535.00it/s]15302it [00:05, 3566.30it/s]17341it [00:05, 3532.86it/s]15460it [00:05, 3537.46it/s]16217it [00:05, 3589.43it/s]15661it [00:05, 3446.26it/s]17730it [00:05, 3634.75it/s]15816it [00:05, 3446.33it/s]16578it [00:05, 3449.11it/s]16038it [00:05, 3536.21it/s]18095it [00:05, 3544.73it/s]16172it [00:05, 3478.68it/s]16971it [00:05, 3584.26it/s]16394it [00:05, 3457.33it/s]16522it [00:05, 3303.29it/s]17332it [00:05, 3323.35it/s]16749it [00:05, 3483.84it/s]16887it [00:05, 3398.85it/s]17722it [00:05, 3482.17it/s]17141it [00:05, 3609.97it/s]17230it [00:05, 3363.02it/s]18075it [00:05, 3440.62it/s]17504it [00:05, 3510.18it/s]17620it [00:06, 3516.66it/s]17892it [00:05, 3614.63it/s]17987it [00:06, 3560.58it/s]18451it [00:06, 1122.75it/s]18837it [00:06, 1437.54it/s]19167it [00:06, 1700.48it/s]19553it [00:06, 2063.01it/s]19884it [00:07, 2298.09it/s]20256it [00:07, 2604.58it/s]20636it [00:07, 2886.16it/s]18423it [00:06, 890.62it/s] 20990it [00:07, 2985.12it/s]18803it [00:07, 1166.76it/s]21370it [00:07, 3196.96it/s]19124it [00:07, 1410.76it/s]18255it [00:07, 840.50it/s] 21726it [00:07, 3227.62it/s]19497it [00:07, 1748.46it/s]18642it [00:07, 1108.52it/s]22105it [00:07, 3381.66it/s]19819it [00:07, 1998.92it/s]18345it [00:07, 766.73it/s] 18977it [00:07, 1351.30it/s]22463it [00:07, 3339.59it/s]20205it [00:07, 2365.83it/s]18669it [00:07, 971.70it/s]19355it [00:07, 1684.84it/s]22847it [00:07, 3478.56it/s]20566it [00:07, 2637.91it/s]18983it [00:07, 1201.19it/s]19738it [00:07, 2036.14it/s]23206it [00:08, 3416.60it/s]20913it [00:07, 2787.96it/s]19370it [00:07, 1551.72it/s]20082it [00:07, 2272.99it/s]23582it [00:08, 3511.81it/s]21278it [00:07, 3003.50it/s]19736it [00:07, 1884.10it/s]20443it [00:07, 2554.06it/s]23960it [00:08, 3588.70it/s]21625it [00:07, 3078.30it/s]20069it [00:07, 2134.24it/s]20787it [00:07, 2698.58it/s]24324it [00:08, 3487.98it/s]21985it [00:07, 3217.81it/s]20429it [00:08, 2437.22it/s]21165it [00:07, 2962.98it/s]24710it [00:08, 3592.84it/s]22335it [00:08, 3226.03it/s]20767it [00:08, 2610.36it/s]21513it [00:07, 3026.66it/s]25073it [00:08, 3501.07it/s]22723it [00:08, 3406.91it/s]21147it [00:08, 2898.70it/s]21886it [00:08, 3212.65it/s]25458it [00:08, 3600.17it/s]23101it [00:08, 3512.24it/s]21497it [00:08, 2962.43it/s]22265it [00:08, 3369.73it/s]25821it [00:08, 3521.91it/s]21867it [00:08, 3155.05it/s]23463it [00:08, 3363.27it/s]22623it [00:08, 3342.73it/s]26193it [00:08, 3578.76it/s]22240it [00:08, 3311.19it/s]23843it [00:08, 3484.52it/s]23011it [00:08, 3491.90it/s]26553it [00:08, 3446.01it/s]22594it [00:08, 3304.34it/s]24198it [00:08, 3417.50it/s]23372it [00:08, 3369.40it/s]26925it [00:09, 3523.67it/s]22962it [00:08, 3409.12it/s]24575it [00:08, 3508.93it/s]23743it [00:08, 3463.33it/s]27307it [00:09, 3601.80it/s]23315it [00:08, 3165.28it/s]24930it [00:08, 3213.69it/s]24096it [00:08, 3318.01it/s]27669it [00:09, 3493.30it/s]23688it [00:09, 3318.30it/s]25304it [00:08, 3357.78it/s]24470it [00:08, 3435.51it/s]28050it [00:09, 3582.37it/s]24030it [00:09, 3280.57it/s]25677it [00:09, 3461.83it/s]24840it [00:08, 3510.32it/s]28410it [00:09, 3467.35it/s]24396it [00:09, 3387.63it/s]26029it [00:09, 3368.41it/s]25195it [00:09, 3395.58it/s]28775it [00:09, 3518.92it/s]24758it [00:09, 3452.53it/s]26393it [00:09, 3443.95it/s]25555it [00:09, 3451.22it/s]29129it [00:09, 3413.83it/s]25108it [00:09, 3356.06it/s]26741it [00:09, 3353.14it/s]25903it [00:09, 3368.04it/s]29499it [00:09, 3495.50it/s]25468it [00:09, 3423.48it/s]27115it [00:09, 3463.39it/s]26269it [00:09, 3450.06it/s]29877it [00:09, 3576.15it/s]25813it [00:09, 3315.35it/s]27464it [00:09, 3360.78it/s]26616it [00:09, 3330.72it/s]30236it [00:09, 3458.88it/s]26183it [00:09, 3424.38it/s]27843it [00:09, 3481.54it/s]26986it [00:09, 3435.73it/s]30598it [00:10, 3504.68it/s]26538it [00:09, 3293.51it/s]28216it [00:09, 3390.77it/s]27370it [00:09, 3551.72it/s]26895it [00:09, 3370.48it/s]28570it [00:09, 3432.09it/s]27727it [00:09, 3339.39it/s]27259it [00:10, 3446.97it/s]28935it [00:10, 3492.38it/s]28078it [00:09, 3379.15it/s]27606it [00:10, 3378.07it/s]29286it [00:10, 3396.98it/s]28419it [00:10, 3313.85it/s]27988it [00:10, 3505.22it/s]29658it [00:10, 3487.80it/s]28789it [00:10, 3421.77it/s]28340it [00:10, 3350.16it/s]30009it [00:10, 3285.21it/s]29134it [00:10, 3331.04it/s]28711it [00:10, 3450.54it/s]30382it [00:10, 3408.38it/s]29486it [00:10, 3382.58it/s]29059it [00:10, 3359.15it/s]30736it [00:10, 3333.16it/s]29864it [00:10, 3495.63it/s]29417it [00:10, 3420.42it/s]30215it [00:10, 3389.44it/s]29792it [00:10, 3515.36it/s]30587it [00:10, 3482.14it/s]30145it [00:10, 3362.14it/s]30950it [00:11, 866.23it/s] 30521it [00:11, 3474.76it/s]31324it [00:11, 1133.94it/s]31647it [00:11, 1376.08it/s]32019it [00:11, 1711.83it/s]32399it [00:11, 2066.08it/s]32737it [00:11, 2286.08it/s]33092it [00:11, 2558.14it/s]33430it [00:11, 2707.47it/s]33807it [00:12, 2971.16it/s]34167it [00:12, 3061.85it/s]31072it [00:11, 731.45it/s] 34539it [00:12, 3236.51it/s]31443it [00:12, 974.06it/s]34907it [00:12, 3358.68it/s]31741it [00:12, 1183.05it/s]35263it [00:12, 3277.52it/s]30937it [00:12, 725.74it/s] 32106it [00:12, 1502.66it/s]35636it [00:12, 3402.25it/s]31296it [00:12, 954.03it/s]32476it [00:12, 1845.37it/s]35987it [00:12, 3325.32it/s]31646it [00:12, 1201.56it/s]32804it [00:12, 2080.55it/s]36352it [00:12, 3415.26it/s]32006it [00:12, 1504.53it/s]33159it [00:12, 2380.70it/s]36700it [00:12, 3343.82it/s]32385it [00:12, 1853.72it/s]33491it [00:12, 2552.63it/s]37067it [00:13, 3436.48it/s]32719it [00:12, 2079.19it/s]33869it [00:12, 2847.27it/s]30871it [00:12, 588.87it/s] 37443it [00:13, 3528.86it/s]33091it [00:12, 2407.87it/s]34210it [00:12, 2935.29it/s]31245it [00:12, 794.99it/s]37799it [00:13, 3422.66it/s]33429it [00:12, 2558.75it/s]34582it [00:12, 3142.34it/s]31626it [00:13, 1052.60it/s]38184it [00:13, 3543.64it/s]33806it [00:12, 2844.69it/s]34947it [00:13, 3279.10it/s]31942it [00:13, 1273.46it/s]38541it [00:13, 3403.34it/s]34166it [00:12, 2956.78it/s]35298it [00:13, 3241.20it/s]32325it [00:13, 1615.05it/s]38922it [00:13, 3519.13it/s]34516it [00:13, 3092.18it/s]35645it [00:13, 3303.95it/s]32657it [00:13, 1823.66it/s]39277it [00:13, 3416.40it/s]34873it [00:13, 3218.99it/s]35988it [00:13, 3257.29it/s]33011it [00:13, 2134.13it/s]39646it [00:13, 3493.50it/s]35218it [00:13, 3195.63it/s]36358it [00:13, 3381.89it/s]33337it [00:13, 2342.43it/s]40024it [00:13, 3574.71it/s]35594it [00:13, 3350.66it/s]36703it [00:13, 3252.74it/s]33715it [00:13, 2665.27it/s]40383it [00:13, 3472.89it/s]35942it [00:13, 3265.16it/s]37071it [00:13, 3371.57it/s]34082it [00:13, 2909.65it/s]40754it [00:14, 3539.72it/s]36309it [00:13, 3379.08it/s]37437it [00:13, 3452.08it/s]34429it [00:13, 2987.83it/s]41110it [00:14, 3417.97it/s]36675it [00:13, 3459.35it/s]37786it [00:13, 3325.93it/s]34786it [00:13, 3140.63it/s]41494it [00:14, 3536.56it/s]37027it [00:13, 3328.44it/s]38160it [00:13, 3443.58it/s]35130it [00:14, 3113.23it/s]41850it [00:14, 3435.54it/s]37395it [00:13, 3426.77it/s]38508it [00:14, 3324.02it/s]35501it [00:14, 3276.29it/s]42231it [00:14, 3540.59it/s]37742it [00:14, 3319.94it/s]38877it [00:14, 3425.89it/s]35847it [00:14, 3046.40it/s]42587it [00:14, 3461.80it/s]38086it [00:14, 3345.96it/s]39222it [00:14, 3276.93it/s]36216it [00:14, 3219.15it/s]42961it [00:14, 3540.09it/s]38423it [00:14, 3274.82it/s]39599it [00:14, 3414.87it/s]36575it [00:14, 3321.41it/s]43324it [00:14, 3565.63it/s]38790it [00:14, 3387.41it/s]39972it [00:14, 3503.62it/s]36917it [00:14, 3237.13it/s]43682it [00:14, 3397.57it/s]39160it [00:14, 3477.75it/s]40325it [00:14, 3400.37it/s]37288it [00:14, 3368.53it/s]44067it [00:15, 3526.26it/s]39510it [00:14, 3383.03it/s]40680it [00:14, 3440.80it/s]37631it [00:14, 3252.83it/s]44422it [00:15, 3451.08it/s]39868it [00:14, 3437.93it/s]41026it [00:14, 3354.90it/s]38005it [00:14, 3388.38it/s]44805it [00:15, 3558.02it/s]40214it [00:14, 3352.59it/s]41403it [00:14, 3473.78it/s]38365it [00:15, 3447.00it/s]45163it [00:15, 3410.17it/s]40581it [00:14, 3441.69it/s]41752it [00:15, 3386.09it/s]38713it [00:15, 3303.35it/s]45545it [00:15, 3524.81it/s]40927it [00:14, 3316.43it/s]42112it [00:15, 3445.87it/s]39081it [00:15, 3408.78it/s]45921it [00:15, 3591.29it/s]41296it [00:15, 3421.33it/s]42479it [00:15, 3508.84it/s]39425it [00:15, 3177.39it/s]41656it [00:15, 3469.50it/s]42831it [00:15, 3408.94it/s]39799it [00:15, 3331.37it/s]42005it [00:15, 3387.54it/s]43189it [00:15, 3456.26it/s]40137it [00:15, 3270.67it/s]42362it [00:15, 3439.96it/s]43536it [00:15, 3353.81it/s]40500it [00:15, 3371.02it/s]42707it [00:15, 3359.24it/s]43910it [00:15, 3463.10it/s]40873it [00:15, 3473.76it/s]43073it [00:15, 3444.54it/s]44258it [00:15, 3355.60it/s]41223it [00:15, 3335.58it/s]43419it [00:15, 3335.94it/s]44637it [00:15, 3479.94it/s]41595it [00:15, 3445.20it/s]43775it [00:15, 3399.11it/s]45016it [00:15, 3569.74it/s]41942it [00:16, 3366.38it/s]44156it [00:15, 3517.63it/s]45375it [00:16, 3457.28it/s]42303it [00:16, 3436.20it/s]44509it [00:16, 3418.20it/s]45735it [00:16, 3498.20it/s]44873it [00:16, 3480.87it/s]42649it [00:16, 3235.57it/s]45223it [00:16, 3374.91it/s]43016it [00:16, 3328.65it/s]45577it [00:16, 3415.68it/s]43386it [00:16, 3431.82it/s]45927it [00:16, 3323.20it/s]43732it [00:16, 3315.95it/s]44096it [00:16, 3406.03it/s]44439it [00:16, 3346.66it/s]44815it [00:16, 3463.94it/s]45163it [00:17, 3331.87it/s]46282it [00:17, 587.59it/s] 45539it [00:17, 3451.48it/s]46658it [00:17, 790.06it/s]45912it [00:17, 3531.24it/s]46973it [00:17, 987.12it/s]47358it [00:17, 1292.21it/s]47684it [00:17, 1544.57it/s]48067it [00:17, 1905.57it/s]48441it [00:17, 2242.79it/s]48792it [00:18, 2466.52it/s]49171it [00:18, 2765.50it/s]49525it [00:18, 2890.92it/s]49908it [00:18, 3129.67it/s]50265it [00:18, 3131.28it/s]46087it [00:18, 530.24it/s] 50639it [00:18, 3292.76it/s]46414it [00:18, 692.66it/s]51023it [00:18, 3442.96it/s]46785it [00:18, 928.34it/s]51385it [00:18, 3370.12it/s]47099it [00:18, 1150.27it/s]51759it [00:18, 3474.02it/s]47474it [00:18, 1476.16it/s]46261it [00:18, 493.14it/s] 52116it [00:19, 3393.50it/s]47801it [00:18, 1736.85it/s]46627it [00:18, 674.17it/s]52483it [00:19, 3471.74it/s]48152it [00:18, 2051.12it/s]46933it [00:18, 854.34it/s]52836it [00:19, 3373.87it/s]48514it [00:18, 2302.30it/s]47311it [00:18, 1139.22it/s]53212it [00:19, 3482.48it/s]48889it [00:19, 2619.16it/s]47676it [00:18, 1416.73it/s]53564it [00:19, 3385.35it/s]49262it [00:19, 2883.20it/s]48038it [00:19, 1737.78it/s]53943it [00:19, 3500.67it/s]49613it [00:19, 2933.97it/s]48414it [00:19, 2086.30it/s]54326it [00:19, 3596.06it/s]49978it [00:19, 3119.27it/s]48756it [00:19, 2248.33it/s]54688it [00:19, 3423.65it/s]50324it [00:19, 3108.12it/s]46267it [00:19, 476.49it/s] 49134it [00:19, 2572.98it/s]55061it [00:19, 3510.02it/s]50688it [00:19, 3252.38it/s]46633it [00:19, 646.18it/s]49474it [00:19, 2668.66it/s]55415it [00:19, 3388.62it/s]51034it [00:19, 3223.80it/s]46924it [00:19, 806.37it/s]49847it [00:19, 2925.45it/s]55790it [00:20, 3490.35it/s]51405it [00:19, 3358.53it/s]47306it [00:19, 1082.82it/s]50196it [00:19, 2986.18it/s]56142it [00:20, 3406.99it/s]51775it [00:19, 3453.64it/s]47676it [00:19, 1358.52it/s]50566it [00:19, 3172.89it/s]56500it [00:20, 3456.25it/s]52128it [00:19, 3369.22it/s]48051it [00:20, 1691.87it/s]50939it [00:19, 3324.31it/s]56881it [00:20, 3557.43it/s]52488it [00:20, 3432.81it/s]48409it [00:20, 2005.70it/s]51292it [00:20, 3234.80it/s]57239it [00:20, 3464.94it/s]52836it [00:20, 3340.50it/s]48750it [00:20, 2242.18it/s]51664it [00:20, 3367.75it/s]57618it [00:20, 3551.05it/s]53205it [00:20, 3439.63it/s]49128it [00:20, 2567.84it/s]52012it [00:20, 3296.79it/s]57975it [00:20, 3444.29it/s]53554it [00:20, 3320.90it/s]49475it [00:20, 2651.94it/s]52367it [00:20, 3363.14it/s]58344it [00:20, 3514.00it/s]53889it [00:20, 3192.28it/s]49844it [00:20, 2901.67it/s]52717it [00:20, 3288.50it/s]58697it [00:20, 3411.48it/s]54265it [00:20, 3350.91it/s]50197it [00:20, 2943.11it/s]53062it [00:20, 3332.95it/s]59050it [00:21, 3445.49it/s]54603it [00:20, 3288.47it/s]50566it [00:20, 3136.06it/s]53426it [00:20, 3419.19it/s]59423it [00:21, 3527.10it/s]54966it [00:20, 3385.45it/s]50938it [00:20, 3293.80it/s]53771it [00:20, 3316.41it/s]59777it [00:21, 3408.04it/s]55307it [00:20, 3286.91it/s]51289it [00:21, 3202.32it/s]54145it [00:20, 3435.63it/s]60149it [00:21, 3497.45it/s]55668it [00:21, 3378.30it/s]51659it [00:21, 3339.10it/s]54491it [00:20, 3345.72it/s]60501it [00:21, 3391.73it/s]56039it [00:21, 3472.88it/s]52005it [00:21, 3172.78it/s]54842it [00:21, 3391.87it/s]60878it [00:21, 3497.98it/s]56388it [00:21, 3362.54it/s]52373it [00:21, 3310.51it/s]55208it [00:21, 3468.26it/s]61230it [00:21, 3411.63it/s]56757it [00:21, 3455.24it/s]52717it [00:21, 3208.74it/s]55557it [00:21, 3337.03it/s]61583it [00:21, 3444.26it/s]57104it [00:21, 3358.31it/s]53083it [00:21, 3332.46it/s]55912it [00:21, 3396.78it/s]61957it [00:21, 3370.21it/s]57442it [00:21, 3298.70it/s]53429it [00:21, 3366.69it/s]56254it [00:21, 3299.07it/s]62334it [00:21, 3482.90it/s]57773it [00:21, 3246.32it/s]53770it [00:21, 3281.61it/s]56624it [00:21, 3412.61it/s]62712it [00:22, 3566.17it/s]58145it [00:21, 3380.11it/s]54136it [00:21, 3389.08it/s]56967it [00:21, 3293.27it/s]63070it [00:22, 3443.91it/s]58508it [00:21, 3452.28it/s]54478it [00:21, 3315.70it/s]57343it [00:21, 3426.08it/s]63445it [00:22, 3529.07it/s]58855it [00:21, 3351.27it/s]54849it [00:22, 3428.03it/s]57714it [00:21, 3508.24it/s]63800it [00:22, 3366.33it/s]59220it [00:22, 3436.10it/s]55204it [00:22, 3462.46it/s]58067it [00:22, 3390.61it/s]64176it [00:22, 3476.06it/s]59565it [00:22, 3303.04it/s]55552it [00:22, 3338.51it/s]58431it [00:22, 3462.02it/s]64526it [00:22, 3378.37it/s]59932it [00:22, 3406.84it/s]55910it [00:22, 3407.23it/s]58779it [00:22, 3329.61it/s]64902it [00:22, 3485.47it/s]60275it [00:22, 3314.81it/s]56253it [00:22, 3330.01it/s]59145it [00:22, 3422.50it/s]65275it [00:22, 3554.38it/s]60643it [00:22, 3418.03it/s]56603it [00:22, 3377.72it/s]59490it [00:22, 3178.95it/s]60987it [00:22, 3424.00it/s]56942it [00:22, 3282.92it/s]59856it [00:22, 3306.29it/s]61331it [00:22, 3342.38it/s]57307it [00:22, 3386.79it/s]60211it [00:22, 3374.05it/s]61696it [00:22, 3428.97it/s]57677it [00:22, 3477.00it/s]60552it [00:22, 3292.98it/s]62040it [00:22, 3340.66it/s]58026it [00:23, 3367.65it/s]60916it [00:22, 3390.79it/s]62413it [00:23, 3413.72it/s]58377it [00:23, 3408.53it/s]61258it [00:22, 3315.08it/s]62779it [00:23, 3482.83it/s]58719it [00:23, 3306.76it/s]61610it [00:23, 3373.70it/s]63129it [00:23, 3369.03it/s]59074it [00:23, 3374.21it/s]61956it [00:23, 3301.92it/s]63494it [00:23, 3449.20it/s]59436it [00:23, 3277.37it/s]62326it [00:23, 3414.48it/s]63841it [00:23, 3344.37it/s]59788it [00:23, 3345.45it/s]62697it [00:23, 3498.04it/s]64204it [00:23, 3425.65it/s]60149it [00:23, 3420.14it/s]63048it [00:23, 3251.56it/s]64548it [00:23, 3195.63it/s]60493it [00:23, 3221.26it/s]63420it [00:23, 3380.93it/s]64918it [00:23, 3333.77it/s]60863it [00:23, 3355.77it/s]63762it [00:23, 3286.01it/s]65280it [00:23, 3414.18it/s]61202it [00:24, 3248.72it/s]64128it [00:23, 3390.87it/s]61570it [00:24, 3368.56it/s]64476it [00:23, 3289.32it/s]61943it [00:24, 3470.67it/s]64829it [00:24, 3356.28it/s]62293it [00:24, 3316.55it/s]65194it [00:24, 3438.43it/s]62664it [00:24, 3425.99it/s]63010it [00:24, 3318.29it/s]63364it [00:24, 3381.30it/s]63704it [00:24, 3171.46it/s]65632it [00:25, 482.52it/s] 64074it [00:24, 3317.94it/s]66011it [00:25, 659.74it/s]64439it [00:24, 3364.69it/s]66315it [00:25, 829.00it/s]64779it [00:25, 3271.59it/s]66693it [00:25, 1100.93it/s]65147it [00:25, 3384.82it/s]67067it [00:25, 1381.98it/s]67443it [00:25, 1715.44it/s]67823it [00:25, 2063.16it/s]68173it [00:25, 2292.92it/s]68544it [00:25, 2592.96it/s]68893it [00:26, 2738.97it/s]69254it [00:26, 2951.45it/s]69600it [00:26, 3005.96it/s]69979it [00:26, 3212.08it/s]70357it [00:26, 3359.22it/s]65625it [00:26, 458.35it/s] 70714it [00:26, 3308.95it/s]65994it [00:26, 627.64it/s]71082it [00:26, 3412.40it/s]66286it [00:26, 787.24it/s]71435it [00:26, 3340.68it/s]66653it [00:26, 1048.00it/s]71812it [00:26, 3462.04it/s]67024it [00:26, 1352.44it/s]72165it [00:26, 3364.32it/s]67352it [00:26, 1610.03it/s]72535it [00:27, 3459.69it/s]67694it [00:26, 1908.64it/s]65540it [00:26, 422.58it/s] 72924it [00:27, 3582.17it/s]68022it [00:26, 2141.36it/s]65904it [00:26, 578.97it/s]73286it [00:27, 3419.11it/s]68390it [00:27, 2467.41it/s]66226it [00:26, 747.25it/s]73665it [00:27, 3523.31it/s]66594it [00:27, 995.20it/s]68744it [00:27, 2633.63it/s]74021it [00:27, 3439.34it/s]66966it [00:27, 1288.69it/s]69110it [00:27, 2882.78it/s]74398it [00:27, 3531.57it/s]69478it [00:27, 3087.13it/s]67296it [00:27, 1542.38it/s]74754it [00:27, 3417.80it/s]67652it [00:27, 1862.19it/s]69826it [00:27, 3071.13it/s]75121it [00:27, 3487.55it/s]67985it [00:27, 2105.02it/s]70200it [00:27, 3250.50it/s]75472it [00:27, 3380.50it/s]68353it [00:27, 2430.33it/s]70547it [00:27, 3215.14it/s]75847it [00:28, 3483.52it/s]68722it [00:27, 2716.33it/s]70909it [00:27, 3327.42it/s]76212it [00:28, 3531.38it/s]71253it [00:27, 3358.23it/s]69070it [00:27, 2772.23it/s]76567it [00:28, 3402.99it/s]69434it [00:27, 2989.78it/s]71597it [00:27, 3214.80it/s]76937it [00:28, 3487.04it/s]65488it [00:28, 370.01it/s] 69775it [00:27, 3017.65it/s]71965it [00:28, 3345.48it/s]77288it [00:28, 3354.76it/s]65860it [00:28, 514.03it/s]70150it [00:28, 3211.82it/s]72306it [00:28, 3272.81it/s]77663it [00:28, 3465.43it/s]66210it [00:28, 686.82it/s]70494it [00:28, 3154.49it/s]72662it [00:28, 3353.41it/s]66513it [00:28, 865.62it/s]78012it [00:28, 3364.60it/s]70858it [00:28, 3287.00it/s]73001it [00:28, 3311.61it/s]66885it [00:28, 1146.20it/s]78388it [00:28, 3476.82it/s]71224it [00:28, 3390.76it/s]73365it [00:28, 3400.65it/s]78758it [00:28, 3538.77it/s]67208it [00:28, 1395.80it/s]73743it [00:28, 3511.14it/s]71573it [00:28, 3302.99it/s]67557it [00:28, 1707.67it/s]79114it [00:29, 3410.00it/s]71930it [00:28, 3378.44it/s]74096it [00:28, 3396.59it/s]67906it [00:28, 1986.96it/s]79486it [00:29, 3498.51it/s]74463it [00:28, 3473.03it/s]72274it [00:28, 3281.17it/s]68270it [00:28, 2313.68it/s]79838it [00:29, 3394.06it/s]72626it [00:28, 3348.15it/s]74812it [00:28, 3238.55it/s]68625it [00:29, 2585.47it/s]80207it [00:29, 3478.63it/s]72964it [00:28, 3299.03it/s]75174it [00:29, 3343.52it/s]68968it [00:29, 2698.92it/s]80557it [00:29, 3389.02it/s]73320it [00:28, 3371.74it/s]75512it [00:29, 3240.57it/s]69339it [00:29, 2948.79it/s]80928it [00:29, 3480.47it/s]73696it [00:29, 3484.80it/s]75881it [00:29, 3366.97it/s]81287it [00:29, 3511.40it/s]69682it [00:29, 2989.03it/s]74047it [00:29, 3393.78it/s]76239it [00:29, 3427.81it/s]70045it [00:29, 3159.74it/s]81640it [00:29, 3387.38it/s]74416it [00:29, 3478.41it/s]76584it [00:29, 3335.42it/s]70418it [00:29, 3314.50it/s]82014it [00:29, 3487.61it/s]74766it [00:29, 3332.68it/s]76935it [00:29, 3383.00it/s]70769it [00:29, 3229.60it/s]82365it [00:29, 3376.54it/s]75125it [00:29, 3406.16it/s]77275it [00:29, 3311.27it/s]71130it [00:29, 3334.57it/s]82740it [00:30, 3482.04it/s]75468it [00:29, 3306.86it/s]77639it [00:29, 3403.89it/s]71474it [00:29, 3274.01it/s]83090it [00:30, 3376.00it/s]75833it [00:29, 3402.20it/s]77985it [00:29, 3197.96it/s]71819it [00:29, 3323.63it/s]83459it [00:30, 3463.61it/s]76175it [00:29, 3369.54it/s]78335it [00:30, 3277.02it/s]83830it [00:30, 3533.69it/s]72157it [00:30, 3228.27it/s]76514it [00:29, 3276.10it/s]78700it [00:30, 3382.85it/s]72525it [00:30, 3354.82it/s]84185it [00:30, 3410.25it/s]76873it [00:30, 3365.86it/s]79041it [00:30, 3286.69it/s]72880it [00:30, 3409.80it/s]84557it [00:30, 3499.08it/s]77211it [00:30, 3279.88it/s]79406it [00:30, 3390.09it/s]73224it [00:30, 3337.20it/s]84909it [00:30, 3386.84it/s]77577it [00:30, 3388.75it/s]79747it [00:30, 3300.30it/s]73595it [00:30, 3445.07it/s]85250it [00:30, 3322.47it/s]77931it [00:30, 3430.64it/s]80101it [00:30, 3366.66it/s]73942it [00:30, 3373.44it/s]85584it [00:30, 3265.19it/s]78276it [00:30, 3330.17it/s]80473it [00:30, 3468.69it/s]74296it [00:30, 3419.87it/s]85955it [00:30, 3390.90it/s]78645it [00:30, 3433.24it/s]80822it [00:30, 3362.97it/s]74640it [00:30, 3316.41it/s]86332it [00:31, 3499.96it/s]78990it [00:30, 3317.80it/s]81181it [00:30, 3427.50it/s]74994it [00:30, 3378.35it/s]86684it [00:31, 3393.53it/s]79336it [00:30, 3356.50it/s]75347it [00:31, 3420.71it/s]81526it [00:30, 3204.09it/s]87052it [00:31, 3474.78it/s]79673it [00:30, 3239.29it/s]75691it [00:31, 3309.55it/s]81893it [00:31, 3333.50it/s]87401it [00:31, 3377.03it/s]80043it [00:30, 3370.48it/s]76044it [00:31, 3372.43it/s]82230it [00:31, 3263.57it/s]87776it [00:31, 3484.10it/s]80414it [00:31, 3467.75it/s]76383it [00:31, 3276.37it/s]82594it [00:31, 3369.77it/s]88126it [00:31, 3380.62it/s]80763it [00:31, 3317.22it/s]76750it [00:31, 3387.62it/s]82946it [00:31, 3410.44it/s]88502it [00:31, 3488.21it/s]81123it [00:31, 3397.38it/s]77096it [00:31, 3408.03it/s]83289it [00:31, 3304.99it/s]88875it [00:31, 3558.21it/s]81465it [00:31, 3286.26it/s]77438it [00:31, 3314.46it/s]83648it [00:31, 3385.07it/s]89233it [00:31, 3439.28it/s]81832it [00:31, 3394.99it/s]77794it [00:31, 3385.17it/s]83989it [00:31, 3301.97it/s]89604it [00:32, 3517.24it/s]82184it [00:31, 3428.85it/s]78134it [00:31, 3296.59it/s]84341it [00:31, 3363.26it/s]82529it [00:31, 3311.00it/s]78499it [00:31, 3397.89it/s]84699it [00:31, 3424.52it/s]82881it [00:31, 3370.34it/s]78840it [00:32, 3240.89it/s]85043it [00:32, 3207.30it/s]83220it [00:31, 3275.01it/s]79207it [00:32, 3362.30it/s]85404it [00:32, 3316.48it/s]83581it [00:32, 3370.67it/s]79570it [00:32, 3438.33it/s]85739it [00:32, 3221.01it/s]83920it [00:32, 3234.78it/s]79916it [00:32, 3310.07it/s]86104it [00:32, 3342.57it/s]84284it [00:32, 3348.17it/s]80278it [00:32, 3398.59it/s]86441it [00:32, 3272.39it/s]84645it [00:32, 3421.23it/s]80620it [00:32, 3288.04it/s]86804it [00:32, 3368.98it/s]84989it [00:32, 3318.18it/s]80987it [00:32, 3394.49it/s]87156it [00:32, 3411.79it/s]85354it [00:32, 3412.49it/s]87499it [00:32, 3340.24it/s]81347it [00:32, 3296.44it/s]85697it [00:32, 3317.90it/s]87866it [00:32, 3434.42it/s]81699it [00:32, 3358.62it/s]86041it [00:32, 3352.96it/s]82055it [00:33, 3414.52it/s]88211it [00:32, 3205.27it/s]86387it [00:32, 3260.68it/s]82398it [00:33, 3303.75it/s]88568it [00:33, 3307.58it/s]86752it [00:32, 3371.27it/s]82756it [00:33, 3382.54it/s]88904it [00:33, 3250.64it/s]87116it [00:33, 3447.94it/s]83096it [00:33, 3287.45it/s]89269it [00:33, 3363.16it/s]87463it [00:33, 3342.63it/s]83461it [00:33, 3390.44it/s]89631it [00:33, 3436.71it/s]87815it [00:33, 3391.61it/s]83814it [00:33, 3428.96it/s]88156it [00:33, 3290.61it/s]84158it [00:33, 3314.02it/s]88526it [00:33, 3408.20it/s]84511it [00:33, 3374.75it/s]88892it [00:33, 3480.46it/s]84850it [00:33, 3269.49it/s]89242it [00:33, 3357.77it/s]85213it [00:33, 3371.30it/s]89580it [00:33, 3329.72it/s]85552it [00:34, 3198.75it/s]85919it [00:34, 3329.43it/s]86297it [00:34, 3456.28it/s]89958it [00:34, 435.58it/s] 86645it [00:34, 3341.73it/s]90326it [00:34, 594.39it/s]86999it [00:34, 3396.89it/s]90657it [00:34, 767.03it/s]87341it [00:34, 3300.99it/s]91018it [00:34, 1008.37it/s]87701it [00:34, 3386.13it/s]91362it [00:34, 1271.24it/s]91684it [00:35, 1518.43it/s]88066it [00:34, 3301.51it/s]92045it [00:35, 1851.05it/s]88439it [00:34, 3421.27it/s]88787it [00:35, 3437.93it/s]92374it [00:35, 2079.40it/s]92734it [00:35, 2393.05it/s]89133it [00:35, 3261.89it/s]93096it [00:35, 2671.86it/s]89496it [00:35, 3365.80it/s]93437it [00:35, 2767.08it/s]93799it [00:35, 2982.36it/s]94139it [00:35, 2999.23it/s]94504it [00:35, 3172.62it/s]94857it [00:36, 3133.09it/s]95218it [00:36, 3264.30it/s]95564it [00:36, 3317.94it/s]95905it [00:36, 3237.59it/s]96264it [00:36, 3335.37it/s]96603it [00:36, 3253.27it/s]96963it [00:36, 3351.24it/s]97327it [00:36, 3433.16it/s]97673it [00:36, 3314.58it/s]89977it [00:36, 336.74it/s] 98038it [00:36, 3408.29it/s]90335it [00:36, 464.15it/s]98381it [00:37, 3294.65it/s]90654it [00:36, 606.21it/s]98744it [00:37, 3389.85it/s]91006it [00:36, 810.81it/s]99085it [00:37, 3277.32it/s]91356it [00:37, 1055.79it/s]99449it [00:37, 3380.04it/s]99797it [00:37, 3408.15it/s]91674it [00:37, 1249.45it/s]89915it [00:37, 326.49it/s] 100140it [00:37, 3285.33it/s]92029it [00:37, 1563.36it/s]90277it [00:37, 454.46it/s]100502it [00:37, 3381.28it/s]92341it [00:37, 1799.04it/s]90632it [00:37, 617.21it/s]92697it [00:37, 2128.59it/s]100842it [00:37, 3274.10it/s]90931it [00:37, 781.91it/s]93054it [00:37, 2432.52it/s]101204it [00:37, 3372.70it/s]91286it [00:37, 1032.15it/s]101565it [00:38, 3440.29it/s]93386it [00:37, 2574.25it/s]91600it [00:37, 1264.66it/s]93744it [00:37, 2818.59it/s]101911it [00:38, 3316.37it/s]91955it [00:37, 1583.69it/s]102274it [00:38, 3405.83it/s]94077it [00:37, 2869.79it/s]92310it [00:37, 1910.94it/s]94434it [00:38, 3051.03it/s]102617it [00:38, 3286.70it/s]92642it [00:37, 2084.19it/s]94775it [00:38, 3149.13it/s]102978it [00:38, 3372.76it/s]92999it [00:38, 2391.96it/s]95111it [00:38, 3102.57it/s]103317it [00:38, 3274.56it/s]93325it [00:38, 2533.22it/s]95441it [00:38, 3157.12it/s]103668it [00:38, 3340.70it/s]93683it [00:38, 2786.25it/s]104030it [00:38, 3419.57it/s]95768it [00:38, 3104.07it/s]94016it [00:38, 2836.43it/s]96126it [00:38, 3237.90it/s]104374it [00:38, 3304.07it/s]94374it [00:38, 3031.40it/s]96484it [00:38, 3336.27it/s]104734it [00:38, 3387.31it/s]94733it [00:38, 3182.31it/s]96823it [00:38, 3242.44it/s]105075it [00:39, 3275.74it/s]95073it [00:38, 3121.50it/s]97181it [00:38, 3338.13it/s]105431it [00:39, 3356.03it/s]89835it [00:38, 293.19it/s] 95431it [00:38, 3247.41it/s]97518it [00:38, 3223.11it/s]105778it [00:39, 3257.75it/s]90201it [00:39, 409.35it/s]97871it [00:39, 3307.95it/s]95768it [00:38, 3086.04it/s]106138it [00:39, 3354.01it/s]90539it [00:39, 548.85it/s]96115it [00:39, 3191.89it/s]106498it [00:39, 3424.74it/s]98214it [00:39, 3186.05it/s]90835it [00:39, 700.00it/s]96469it [00:39, 3288.92it/s]98570it [00:39, 3290.32it/s]106842it [00:39, 3307.50it/s]91178it [00:39, 924.77it/s]96804it [00:39, 3198.82it/s]98925it [00:39, 3336.29it/s]107205it [00:39, 3398.51it/s]91496it [00:39, 1152.37it/s]97160it [00:39, 3301.15it/s]99261it [00:39, 3241.96it/s]107547it [00:39, 3251.23it/s]91853it [00:39, 1465.70it/s]97494it [00:39, 3210.05it/s]99614it [00:39, 3320.36it/s]107911it [00:39, 3361.49it/s]92186it [00:39, 1757.06it/s]97853it [00:39, 3317.10it/s]108276it [00:40, 3443.01it/s]99948it [00:39, 3232.95it/s]92510it [00:39, 1994.78it/s]98211it [00:39, 3391.21it/s]100304it [00:39, 3324.47it/s]108623it [00:40, 3330.42it/s]92868it [00:39, 2317.99it/s]100647it [00:39, 3355.09it/s]98553it [00:39, 3269.08it/s]108988it [00:40, 3421.47it/s]93197it [00:40, 2480.77it/s]98883it [00:39, 3275.42it/s]100984it [00:40, 3243.91it/s]109332it [00:40, 3306.52it/s]93517it [00:40, 2629.95it/s]99213it [00:39, 3179.60it/s]101340it [00:40, 3334.79it/s]109691it [00:40, 3386.72it/s]93863it [00:40, 2840.14it/s]99554it [00:40, 3245.21it/s]101675it [00:40, 3152.52it/s]110032it [00:40, 3274.32it/s]94189it [00:40, 2866.50it/s]99897it [00:40, 3166.18it/s]102033it [00:40, 3271.46it/s]110396it [00:40, 3378.52it/s]94549it [00:40, 3062.24it/s]100247it [00:40, 3253.45it/s]102376it [00:40, 3314.46it/s]110755it [00:40, 3439.67it/s]94878it [00:40, 3042.08it/s]100602it [00:40, 3337.69it/s]102710it [00:40, 3207.02it/s]111101it [00:40, 3316.75it/s]95236it [00:40, 3191.74it/s]100938it [00:40, 3219.39it/s]103059it [00:40, 3287.66it/s]111448it [00:40, 3356.85it/s]95594it [00:40, 3300.66it/s]101284it [00:40, 3287.54it/s]111786it [00:41, 3256.99it/s]103390it [00:40, 3125.58it/s]95934it [00:40, 3166.20it/s]101615it [00:40, 3172.40it/s]112148it [00:41, 3356.69it/s]103736it [00:40, 3219.25it/s]96293it [00:40, 3284.40it/s]101947it [00:40, 3212.45it/s]104084it [00:40, 3292.80it/s]112498it [00:41, 3259.59it/s]96628it [00:41, 3191.54it/s]102296it [00:40, 3290.49it/s]112859it [00:41, 3356.97it/s]104416it [00:41, 3149.94it/s]96988it [00:41, 3304.97it/s]102627it [00:41, 3155.74it/s]113223it [00:41, 3437.53it/s]104738it [00:41, 3168.24it/s]97333it [00:41, 3345.72it/s]102973it [00:41, 3240.71it/s]105057it [00:41, 3086.67it/s]113569it [00:41, 3306.82it/s]97671it [00:41, 3191.64it/s]103299it [00:41, 3133.75it/s]105411it [00:41, 3215.93it/s]113932it [00:41, 3397.61it/s]98031it [00:41, 3305.97it/s]103647it [00:41, 3232.28it/s]105768it [00:41, 3318.06it/s]114274it [00:41, 3289.37it/s]98365it [00:41, 3204.14it/s]103992it [00:41, 3295.16it/s]114637it [00:41, 3385.73it/s]106102it [00:41, 3187.61it/s]98725it [00:41, 3316.17it/s]104323it [00:41, 3175.42it/s]115000it [00:42, 3454.59it/s]106460it [00:41, 3298.85it/s]99059it [00:41, 3180.51it/s]104677it [00:41, 3278.65it/s]106792it [00:41, 3198.58it/s]115347it [00:42, 3283.24it/s]99420it [00:41, 3301.90it/s]105007it [00:41, 3129.52it/s]107148it [00:41, 3300.42it/s]115708it [00:42, 3375.89it/s]99779it [00:42, 3383.63it/s]105361it [00:41, 3245.24it/s]107480it [00:42, 3211.51it/s]116048it [00:42, 3273.86it/s]100120it [00:42, 3226.40it/s]105716it [00:41, 3331.16it/s]107828it [00:42, 3286.05it/s]116413it [00:42, 3378.92it/s]100473it [00:42, 3310.78it/s]108173it [00:42, 3333.00it/s]106052it [00:42, 3181.88it/s]116753it [00:42, 3275.55it/s]100807it [00:42, 3176.54it/s]106406it [00:42, 3275.45it/s]108508it [00:42, 3230.74it/s]117117it [00:42, 3373.58it/s]101156it [00:42, 3262.97it/s]108871it [00:42, 3345.85it/s]106736it [00:42, 3194.58it/s]117478it [00:42, 3441.95it/s]101515it [00:42, 3355.48it/s]107095it [00:42, 3305.67it/s]109207it [00:42, 3245.70it/s]117824it [00:42, 3327.91it/s]101853it [00:42, 3248.23it/s]107453it [00:42, 3384.03it/s]109555it [00:42, 3312.87it/s]118185it [00:42, 3406.52it/s]102206it [00:42, 3304.69it/s]109911it [00:42, 3383.51it/s]107793it [00:42, 3264.69it/s]118528it [00:43, 3297.20it/s]102538it [00:42, 3206.15it/s]108142it [00:42, 3329.26it/s]110251it [00:42, 3278.65it/s]118891it [00:43, 3391.82it/s]102898it [00:42, 3317.16it/s]110609it [00:42, 3363.33it/s]108477it [00:42, 3233.19it/s]119232it [00:43, 3241.01it/s]103256it [00:43, 3221.16it/s]108839it [00:42, 3343.14it/s]110947it [00:43, 3231.82it/s]119592it [00:43, 3341.90it/s]103600it [00:43, 3282.15it/s]109175it [00:43, 3234.52it/s]111304it [00:43, 3321.51it/s]119954it [00:43, 3419.86it/s]103944it [00:43, 3327.31it/s]109518it [00:43, 3290.25it/s]111650it [00:43, 3359.50it/s]104279it [00:43, 3211.31it/s]109874it [00:43, 3367.13it/s]111988it [00:43, 3255.52it/s]104623it [00:43, 3275.02it/s]112347it [00:43, 3350.65it/s]110212it [00:43, 3254.79it/s]104952it [00:43, 3174.86it/s]110569it [00:43, 3344.93it/s]112684it [00:43, 3215.28it/s]105305it [00:43, 3276.08it/s]113042it [00:43, 3318.01it/s]110905it [00:43, 3210.06it/s]105645it [00:43, 3311.30it/s]111266it [00:43, 3322.17it/s]113376it [00:43, 3228.30it/s]105978it [00:43, 3213.02it/s]111624it [00:43, 3396.10it/s]113735it [00:43, 3329.54it/s]106336it [00:44, 3309.92it/s]114097it [00:44, 3412.77it/s]111966it [00:43, 3276.38it/s]106669it [00:44, 3211.00it/s]112322it [00:43, 3357.16it/s]114440it [00:44, 3259.82it/s]107020it [00:44, 3278.80it/s]112660it [00:44, 3246.72it/s]114799it [00:44, 3352.32it/s]107370it [00:44, 3341.06it/s]113003it [00:44, 3296.91it/s]115137it [00:44, 3127.46it/s]107706it [00:44, 3186.97it/s]113336it [00:44, 3205.55it/s]115494it [00:44, 3248.74it/s]108066it [00:44, 3303.77it/s]113684it [00:44, 3283.49it/s]115855it [00:44, 3191.26it/s]108399it [00:44, 3116.62it/s]114041it [00:44, 3364.83it/s]116204it [00:44, 3274.20it/s]108761it [00:44, 3256.88it/s]114379it [00:44, 3262.63it/s]116562it [00:44, 3360.59it/s]109122it [00:44, 3356.29it/s]114737it [00:44, 3345.62it/s]116901it [00:44, 3275.47it/s]109461it [00:45, 3222.22it/s]115073it [00:44, 3242.87it/s]117256it [00:44, 3353.56it/s]109817it [00:45, 3315.93it/s]115431it [00:44, 3337.29it/s]117594it [00:45, 3229.77it/s]110152it [00:45, 3222.06it/s]115788it [00:45, 3404.51it/s]117955it [00:45, 3328.25it/s]110500it [00:45, 3294.50it/s]116130it [00:45, 3274.41it/s]118305it [00:45, 3373.26it/s]110832it [00:45, 3164.89it/s]116464it [00:45, 3291.26it/s]118644it [00:45, 3205.75it/s]111195it [00:45, 3295.96it/s]116795it [00:45, 3204.61it/s]119003it [00:45, 3312.95it/s]111545it [00:45, 3354.31it/s]117157it [00:45, 3323.10it/s]119337it [00:45, 3192.03it/s]111883it [00:45, 3249.10it/s]117516it [00:45, 3398.36it/s]119692it [00:45, 3292.17it/s]112243it [00:45, 3349.68it/s]117857it [00:45, 3282.22it/s]120050it [00:45, 3374.26it/s]112580it [00:45, 3222.16it/s]118213it [00:45, 3361.75it/s]112939it [00:46, 3326.19it/s]118551it [00:45, 3249.60it/s]113301it [00:46, 3408.78it/s]118910it [00:45, 3346.14it/s]113644it [00:46, 3277.79it/s]119247it [00:46, 3183.12it/s]113989it [00:46, 3325.91it/s]119598it [00:46, 3272.88it/s]119942it [00:46, 3320.11it/s]114324it [00:46, 3099.86it/s]114683it [00:46, 3234.20it/s]115016it [00:46, 3134.97it/s]115375it [00:46, 3260.11it/s]120298it [00:47, 305.51it/s] 115734it [00:46, 3353.35it/s]120663it [00:47, 425.24it/s]116072it [00:47, 3204.25it/s]120967it [00:47, 551.96it/s]116432it [00:47, 3315.66it/s]121330it [00:47, 751.80it/s]116767it [00:47, 3214.05it/s]121694it [00:47, 996.87it/s]117118it [00:47, 3297.13it/s]122019it [00:47, 1229.84it/s]117475it [00:47, 3375.38it/s]122382it [00:47, 1548.37it/s]117815it [00:47, 3236.14it/s]122714it [00:47, 1806.02it/s]118153it [00:47, 3277.04it/s]123062it [00:47, 2112.14it/s]118483it [00:47, 3191.58it/s]123420it [00:48, 2416.34it/s]118845it [00:47, 3313.48it/s]123759it [00:48, 2574.62it/s]119204it [00:47, 3393.43it/s]124117it [00:48, 2817.40it/s]119545it [00:48, 3220.57it/s]124455it [00:48, 2885.29it/s]119902it [00:48, 3318.84it/s]124817it [00:48, 3072.74it/s]125167it [00:48, 3065.33it/s]125528it [00:48, 3213.15it/s]125892it [00:48, 3331.62it/s]126238it [00:48, 3245.73it/s]126599it [00:48, 3346.47it/s]126941it [00:49, 3219.75it/s]127306it [00:49, 3338.85it/s]127669it [00:49, 3422.27it/s]128015it [00:49, 3305.23it/s]128376it [00:49, 3389.67it/s]128718it [00:49, 3285.04it/s]129082it [00:49, 3385.73it/s]129423it [00:49, 3282.82it/s]129786it [00:49, 3381.18it/s]130148it [00:50, 3448.18it/s]130495it [00:50, 3321.27it/s]120390it [00:49, 272.52it/s] 130846it [00:50, 3372.87it/s]120749it [00:49, 380.22it/s]131185it [00:50, 3271.93it/s]121043it [00:50, 493.66it/s]131542it [00:50, 3356.21it/s]121403it [00:50, 678.58it/s]121749it [00:50, 896.58it/s]131888it [00:50, 3256.78it/s]120276it [00:50, 277.33it/s] 132252it [00:50, 3363.66it/s]122066it [00:50, 1117.70it/s]120635it [00:50, 388.33it/s]132615it [00:50, 3439.88it/s]122426it [00:50, 1428.01it/s]120966it [00:50, 515.61it/s]132961it [00:50, 3307.57it/s]122752it [00:50, 1640.74it/s]121318it [00:50, 697.60it/s]133322it [00:51, 3392.27it/s]123108it [00:50, 1971.07it/s]121676it [00:50, 927.01it/s]123453it [00:50, 2262.08it/s]133663it [00:51, 3281.77it/s]121990it [00:50, 1139.94it/s]134027it [00:51, 3383.25it/s]123781it [00:50, 2432.40it/s]122348it [00:50, 1447.16it/s]134388it [00:51, 3447.84it/s]124135it [00:50, 2692.32it/s]122668it [00:50, 1691.61it/s]134735it [00:51, 3294.88it/s]124465it [00:51, 2770.11it/s]123024it [00:51, 2021.67it/s]135094it [00:51, 3377.24it/s]124814it [00:51, 2947.11it/s]123380it [00:51, 2331.65it/s]135434it [00:51, 3270.65it/s]125164it [00:51, 2968.82it/s]123715it [00:51, 2488.35it/s]135796it [00:51, 3368.43it/s]125518it [00:51, 3122.50it/s]124072it [00:51, 2743.18it/s]125872it [00:51, 3236.80it/s]136135it [00:51, 3257.06it/s]124406it [00:51, 2692.68it/s]136494it [00:51, 3349.62it/s]126209it [00:51, 3062.52it/s]124746it [00:51, 2870.29it/s]136856it [00:52, 3426.09it/s]126552it [00:51, 3161.30it/s]125101it [00:51, 3049.73it/s]137201it [00:52, 3306.53it/s]126877it [00:51, 3101.08it/s]125431it [00:51, 3013.54it/s]137561it [00:52, 3389.67it/s]127233it [00:51, 3228.29it/s]125788it [00:51, 3164.62it/s]137902it [00:52, 3287.34it/s]127591it [00:52, 3327.52it/s]126119it [00:51, 3094.68it/s]138262it [00:52, 3374.97it/s]127928it [00:52, 3217.21it/s]126474it [00:52, 3220.12it/s]138608it [00:52, 3241.78it/s]128271it [00:52, 3276.71it/s]126829it [00:52, 3311.92it/s]138971it [00:52, 3350.15it/s]128602it [00:52, 3186.86it/s]127167it [00:52, 3206.36it/s]139336it [00:52, 3434.69it/s]128960it [00:52, 3297.40it/s]127513it [00:52, 3276.23it/s]139682it [00:52, 3314.96it/s]129316it [00:52, 3371.44it/s]127845it [00:52, 3038.54it/s]140047it [00:53, 3409.50it/s]129655it [00:52, 3117.86it/s]120237it [00:52, 238.15it/s] 128198it [00:52, 3172.65it/s]140390it [00:53, 3286.41it/s]130002it [00:52, 3215.84it/s]120599it [00:52, 335.09it/s]128526it [00:52, 3112.61it/s]140751it [00:53, 3377.84it/s]130328it [00:52, 3149.30it/s]120956it [00:52, 462.35it/s]128881it [00:52, 3234.28it/s]141113it [00:53, 3445.38it/s]130681it [00:53, 3256.14it/s]121255it [00:53, 596.25it/s]129238it [00:52, 3329.44it/s]141460it [00:53, 3316.32it/s]131035it [00:53, 3337.87it/s]121599it [00:53, 796.88it/s]129574it [00:53, 3223.44it/s]141820it [00:53, 3394.99it/s]121910it [00:53, 1005.24it/s]131371it [00:53, 3207.26it/s]129931it [00:53, 3321.09it/s]142162it [00:53, 3279.76it/s]122270it [00:53, 1304.39it/s]131723it [00:53, 3294.69it/s]130266it [00:53, 3200.58it/s]142510it [00:53, 3336.76it/s]122635it [00:53, 1635.82it/s]132055it [00:53, 3203.18it/s]130625it [00:53, 3309.37it/s]142846it [00:53, 3235.99it/s]132413it [00:53, 3310.59it/s]122971it [00:53, 1881.11it/s]130977it [00:53, 3369.38it/s]143207it [00:53, 3340.88it/s]123325it [00:53, 2195.97it/s]132746it [00:53, 3105.29it/s]143568it [00:54, 3416.99it/s]131316it [00:53, 3135.02it/s]133080it [00:53, 3169.50it/s]123658it [00:53, 2320.95it/s]131673it [00:53, 3254.49it/s]143912it [00:54, 3299.69it/s]133437it [00:53, 3281.99it/s]124003it [00:53, 2573.91it/s]132003it [00:53, 3163.22it/s]144274it [00:54, 3390.71it/s]124326it [00:54, 2674.73it/s]133768it [00:53, 3198.36it/s]132359it [00:53, 3274.47it/s]144615it [00:54, 3296.42it/s]124683it [00:54, 2901.56it/s]134123it [00:54, 3298.62it/s]132706it [00:53, 3329.85it/s]144978it [00:54, 3388.68it/s]125026it [00:54, 3038.84it/s]134455it [00:54, 3203.37it/s]133042it [00:54, 3218.53it/s]145328it [00:54, 3282.97it/s]125358it [00:54, 3028.79it/s]134803it [00:54, 3281.66it/s]133399it [00:54, 3318.09it/s]145691it [00:54, 3380.15it/s]125716it [00:54, 3179.95it/s]135164it [00:54, 3376.34it/s]133733it [00:54, 3210.97it/s]146056it [00:54, 3455.40it/s]126049it [00:54, 3129.48it/s]135504it [00:54, 3271.34it/s]134092it [00:54, 3318.15it/s]146403it [00:54, 3286.63it/s]135863it [00:54, 3362.92it/s]126403it [00:54, 3211.31it/s]146765it [00:55, 3379.80it/s]134426it [00:54, 3067.59it/s]126742it [00:54, 3260.00it/s]136201it [00:54, 3116.25it/s]134783it [00:54, 3205.15it/s]147106it [00:55, 3266.52it/s]127074it [00:54, 3177.52it/s]136544it [00:54, 3201.41it/s]135141it [00:54, 3309.56it/s]147469it [00:55, 3368.04it/s]127432it [00:54, 3291.99it/s]136898it [00:54, 3297.18it/s]147830it [00:55, 3436.66it/s]135476it [00:54, 3177.81it/s]127765it [00:55, 3193.59it/s]137231it [00:55, 3198.00it/s]135832it [00:54, 3284.54it/s]148176it [00:55, 3311.46it/s]128106it [00:55, 3250.09it/s]137587it [00:55, 3300.98it/s]148538it [00:55, 3399.14it/s]128462it [00:55, 3338.04it/s]136164it [00:55, 3183.48it/s]137920it [00:55, 3202.75it/s]136520it [00:55, 3287.82it/s]148880it [00:55, 3282.69it/s]128798it [00:55, 3245.82it/s]138268it [00:55, 3280.04it/s]136877it [00:55, 3367.64it/s]149244it [00:55, 3383.92it/s]129156it [00:55, 3340.90it/s]138605it [00:55, 3191.78it/s]149585it [00:55, 3273.93it/s]137216it [00:55, 3241.66it/s]129492it [00:55, 3205.21it/s]138962it [00:55, 3299.51it/s]149943it [00:55, 3359.54it/s]137573it [00:55, 3333.42it/s]129848it [00:55, 3306.06it/s]139317it [00:55, 3370.55it/s]150291it [00:56, 3391.89it/s]130191it [00:55, 3341.61it/s]137909it [00:55, 3102.71it/s]139656it [00:55, 3128.83it/s]150632it [00:56, 3273.40it/s]130527it [00:55, 3233.95it/s]138257it [00:55, 3206.33it/s]140015it [00:55, 3255.75it/s]150994it [00:56, 3371.74it/s]130882it [00:56, 3324.69it/s]138607it [00:55, 3135.91it/s]140345it [00:55, 3180.20it/s]151333it [00:56, 3259.00it/s]131216it [00:56, 3181.29it/s]138966it [00:55, 3260.36it/s]140704it [00:56, 3294.82it/s]151693it [00:56, 3354.77it/s]131573it [00:56, 3290.42it/s]139327it [00:56, 3357.62it/s]141061it [00:56, 3373.77it/s]152048it [00:56, 3258.53it/s]131905it [00:56, 3195.78it/s]139666it [00:56, 3245.65it/s]141401it [00:56, 3243.82it/s]152409it [00:56, 3358.03it/s]132265it [00:56, 3309.39it/s]140028it [00:56, 3351.97it/s]141755it [00:56, 3322.12it/s]152771it [00:56, 3432.92it/s]132615it [00:56, 3362.20it/s]140366it [00:56, 3229.38it/s]142090it [00:56, 3223.14it/s]153116it [00:56, 3312.35it/s]132953it [00:56, 3246.63it/s]140720it [00:56, 3317.19it/s]142444it [00:56, 3313.48it/s]153477it [00:57, 3397.31it/s]133295it [00:56, 3296.23it/s]141054it [00:56, 3297.92it/s]142781it [00:56, 3327.99it/s]153819it [00:57, 3288.65it/s]133626it [00:56, 3202.94it/s]141386it [00:56, 3191.00it/s]143116it [00:56, 3187.82it/s]154173it [00:57, 3359.62it/s]133975it [00:56, 3284.26it/s]141741it [00:56, 3291.79it/s]143474it [00:56, 3298.29it/s]154537it [00:57, 3438.51it/s]134335it [00:57, 3374.22it/s]142072it [00:56, 3187.68it/s]143806it [00:57, 3208.50it/s]154883it [00:57, 3319.75it/s]134674it [00:57, 3259.70it/s]142426it [00:56, 3288.08it/s]144161it [00:57, 3304.56it/s]155249it [00:57, 3417.22it/s]135036it [00:57, 3353.76it/s]142782it [00:57, 3364.78it/s]144494it [00:57, 3218.34it/s]155593it [00:57, 3301.73it/s]135373it [00:57, 3237.10it/s]143120it [00:57, 3241.50it/s]144843it [00:57, 3296.07it/s]155956it [00:57, 3393.94it/s]135732it [00:57, 3335.95it/s]143478it [00:57, 3337.87it/s]145204it [00:57, 3386.68it/s]156298it [00:57, 3291.15it/s]136086it [00:57, 3234.89it/s]143814it [00:57, 3228.05it/s]145544it [00:57, 3272.63it/s]156662it [00:57, 3388.64it/s]136435it [00:57, 3306.51it/s]144141it [00:57, 3238.29it/s]145892it [00:57, 3332.06it/s]157021it [00:58, 3445.30it/s]136769it [00:57, 3315.88it/s]144487it [00:57, 3153.98it/s]146227it [00:57, 3230.74it/s]157367it [00:58, 3326.60it/s]137102it [00:57, 3213.66it/s]144850it [00:57, 3288.18it/s]146570it [00:57, 3287.29it/s]157730it [00:58, 3411.25it/s]137460it [00:58, 3317.54it/s]145212it [00:57, 3381.63it/s]146927it [00:57, 3369.11it/s]137794it [00:58, 3220.01it/s]145552it [00:57, 3260.88it/s]147265it [00:58, 3253.94it/s]138126it [00:58, 3247.55it/s]145909it [00:58, 3348.50it/s]147623it [00:58, 3346.01it/s]138483it [00:58, 3340.90it/s]146246it [00:58, 3226.20it/s]147959it [00:58, 3236.20it/s]138819it [00:58, 3233.07it/s]146600it [00:58, 3314.50it/s]148303it [00:58, 3294.20it/s]139180it [00:58, 3341.36it/s]146958it [00:58, 3391.21it/s]148658it [00:58, 3366.44it/s]139516it [00:58, 3246.45it/s]148996it [00:58, 3255.15it/s]147299it [00:58, 3236.77it/s]139842it [00:58, 3210.05it/s]149349it [00:58, 3331.50it/s]147654it [00:58, 3324.43it/s]140187it [00:58, 3279.03it/s]147989it [00:58, 3182.15it/s]149684it [00:58, 3147.70it/s]140516it [00:58, 3189.07it/s]148345it [00:58, 3286.45it/s]150040it [00:58, 3263.48it/s]140873it [00:59, 3298.32it/s]150369it [00:59, 3183.49it/s]148687it [00:58, 3186.14it/s]141204it [00:59, 3203.22it/s]150727it [00:59, 3294.62it/s]149047it [00:59, 3302.08it/s]141561it [00:59, 3306.88it/s]151083it [00:59, 3369.66it/s]149405it [00:59, 3380.08it/s]141918it [00:59, 3382.08it/s]151422it [00:59, 3223.66it/s]149745it [00:59, 3256.18it/s]142258it [00:59, 3224.57it/s]151779it [00:59, 3320.12it/s]150087it [00:59, 3302.57it/s]142614it [00:59, 3320.11it/s]152114it [00:59, 3228.09it/s]150419it [00:59, 3204.48it/s]142948it [00:59, 3218.11it/s]152468it [00:59, 3315.48it/s]150775it [00:59, 3305.70it/s]143303it [00:59, 3312.04it/s]152817it [00:59, 3363.65it/s]151122it [00:59, 3353.21it/s]143646it [00:59, 3189.25it/s]151459it [00:59, 3171.98it/s]153155it [00:59, 3152.52it/s]143989it [01:00, 3255.49it/s]151815it [00:59, 3281.90it/s]153512it [00:59, 3267.56it/s]144346it [01:00, 3340.84it/s]153842it [01:00, 3188.27it/s]152146it [00:59, 3183.52it/s]144682it [01:00, 3257.14it/s]154202it [01:00, 3303.96it/s]152503it [01:00, 3291.42it/s]145042it [01:00, 3355.78it/s]154559it [01:00, 3378.33it/s]152848it [01:00, 3334.83it/s]145379it [01:00, 3251.39it/s]153184it [01:00, 3226.14it/s]154899it [01:00, 3239.37it/s]145729it [01:00, 3320.30it/s]153543it [01:00, 3328.41it/s]155260it [01:00, 3344.30it/s]146089it [01:00, 3400.84it/s]153878it [01:00, 3230.79it/s]155597it [01:00, 3243.24it/s]146431it [01:00, 3267.14it/s]154238it [01:00, 3335.15it/s]155955it [01:00, 3338.47it/s]146778it [01:00, 3324.25it/s]154574it [01:00, 3191.66it/s]156291it [01:00, 3162.78it/s]147112it [01:00, 3185.25it/s]154935it [01:00, 3308.75it/s]156650it [01:00, 3282.01it/s]147473it [01:01, 3305.02it/s]155296it [01:00, 3394.20it/s]157010it [01:01, 3372.75it/s]147832it [01:01, 3385.06it/s]155638it [01:01, 3237.50it/s]157350it [01:01, 3262.83it/s]148173it [01:01, 3266.15it/s]155995it [01:01, 3331.42it/s]157712it [01:01, 3364.16it/s]148521it [01:01, 3325.98it/s]156331it [01:01, 3229.10it/s]148856it [01:01, 3221.42it/s]156690it [01:01, 3329.63it/s]149218it [01:01, 3333.70it/s]157048it [01:01, 3401.69it/s]149554it [01:01, 3232.84it/s]157390it [01:01, 3276.37it/s]149895it [01:01, 3282.46it/s]157751it [01:01, 3370.81it/s]150241it [01:01, 3332.50it/s]150576it [01:02, 3153.51it/s]150923it [01:02, 3233.35it/s]151249it [01:02, 3126.58it/s]151603it [01:02, 3243.53it/s]151958it [01:02, 3331.09it/s]152293it [01:02, 3202.80it/s]158073it [01:02, 244.66it/s] 152647it [01:02, 3297.99it/s]158440it [01:02, 343.64it/s]158794it [01:03, 470.36it/s]152979it [01:02, 3136.51it/s]159099it [01:03, 608.56it/s]153331it [01:02, 3242.44it/s]159466it [01:03, 825.78it/s]153670it [01:02, 3282.32it/s]159787it [01:03, 1041.25it/s]154001it [01:03, 3184.07it/s]160148it [01:03, 1338.78it/s]154355it [01:03, 3284.29it/s]160511it [01:03, 1664.09it/s]154686it [01:03, 3141.93it/s]160851it [01:03, 1919.59it/s]155043it [01:03, 3260.96it/s]161216it [01:03, 2248.29it/s]155401it [01:03, 3351.42it/s]161556it [01:03, 2439.27it/s]155739it [01:03, 3218.81it/s]161924it [01:03, 2723.68it/s]156095it [01:03, 3314.09it/s]162265it [01:04, 2824.80it/s]156429it [01:03, 3156.05it/s]162621it [01:04, 3012.69it/s]156778it [01:03, 3250.09it/s]162987it [01:04, 3184.84it/s]157106it [01:04, 3110.52it/s]163334it [01:04, 3160.51it/s]157462it [01:04, 3236.39it/s]163704it [01:04, 3309.49it/s]157811it [01:04, 3279.75it/s]164051it [01:04, 3247.78it/s]164422it [01:04, 3377.10it/s]164769it [01:04, 3296.81it/s]165142it [01:04, 3417.59it/s]165515it [01:05, 3505.89it/s]165870it [01:05, 3396.51it/s]166246it [01:05, 3500.07it/s]166599it [01:05, 3354.25it/s]166969it [01:05, 3451.60it/s]167317it [01:05, 3358.09it/s]167690it [01:05, 3463.18it/s]168067it [01:05, 3549.71it/s]168424it [01:05, 3423.32it/s]168799it [01:05, 3515.05it/s]169153it [01:06, 3393.48it/s]169523it [01:06, 3480.08it/s]169873it [01:06, 3361.14it/s]170247it [01:06, 3459.53it/s]170597it [01:06, 3320.64it/s]170967it [01:06, 3427.37it/s]171338it [01:06, 3508.43it/s]158051it [01:06, 213.66it/s] 171691it [01:06, 3379.41it/s]158411it [01:06, 300.28it/s]172064it [01:06, 3478.39it/s]158767it [01:06, 415.20it/s]172414it [01:07, 3371.43it/s]159067it [01:06, 539.11it/s]172785it [01:07, 3468.07it/s]159429it [01:06, 736.49it/s]173134it [01:07, 3378.32it/s]158090it [01:06, 215.96it/s] 159747it [01:06, 937.33it/s]173502it [01:07, 3462.65it/s]158436it [01:06, 299.81it/s]160099it [01:07, 1212.53it/s]173873it [01:07, 3532.22it/s]158776it [01:06, 410.19it/s]160450it [01:07, 1515.92it/s]174228it [01:07, 3409.86it/s]159074it [01:07, 534.53it/s]160783it [01:07, 1757.16it/s]174592it [01:07, 3474.03it/s]159435it [01:07, 733.42it/s]161144it [01:07, 2089.75it/s]159749it [01:07, 932.81it/s]174941it [01:07, 3345.70it/s]161475it [01:07, 2296.36it/s]160105it [01:07, 1214.76it/s]175316it [01:07, 3458.82it/s]161839it [01:07, 2595.73it/s]160462it [01:07, 1527.70it/s]175664it [01:07, 3345.92it/s]162194it [01:07, 2718.89it/s]160796it [01:07, 1781.97it/s]176031it [01:08, 3437.04it/s]162558it [01:07, 2946.92it/s]161145it [01:07, 2094.07it/s]176396it [01:08, 3497.23it/s]162921it [01:07, 3126.19it/s]161476it [01:07, 2294.08it/s]176748it [01:08, 3385.16it/s]163266it [01:08, 3089.96it/s]161834it [01:07, 2582.21it/s]177108it [01:08, 3446.51it/s]163612it [01:08, 3190.50it/s]162179it [01:08, 2792.43it/s]177454it [01:08, 3352.20it/s]163948it [01:08, 3147.04it/s]162515it [01:08, 2855.22it/s]177822it [01:08, 3444.50it/s]164314it [01:08, 3289.54it/s]162876it [01:08, 3052.61it/s]178168it [01:08, 3363.10it/s]164678it [01:08, 3388.53it/s]178525it [01:08, 3420.41it/s]163213it [01:08, 3044.87it/s]165024it [01:08, 3320.35it/s]163577it [01:08, 3208.15it/s]178903it [01:08, 3523.26it/s]165380it [01:08, 3387.70it/s]179257it [01:09, 3391.44it/s]163915it [01:08, 3128.29it/s]165723it [01:08, 3295.95it/s]179628it [01:09, 3482.65it/s]164283it [01:08, 3281.35it/s]166092it [01:08, 3408.53it/s]164646it [01:08, 3376.21it/s]179978it [01:09, 3354.01it/s]166436it [01:08, 3314.09it/s]164991it [01:08, 3300.42it/s]180361it [01:09, 3489.73it/s]166809it [01:09, 3433.40it/s]165348it [01:08, 3375.55it/s]180712it [01:09, 3359.02it/s]167156it [01:09, 3442.55it/s]181090it [01:09, 3478.72it/s]165690it [01:09, 3180.62it/s]167502it [01:09, 3274.33it/s]181453it [01:09, 3519.74it/s]166058it [01:09, 3319.13it/s]167867it [01:09, 3379.71it/s]181807it [01:09, 3405.75it/s]166396it [01:09, 3244.15it/s]168208it [01:09, 3309.13it/s]182185it [01:09, 3512.93it/s]166758it [01:09, 3348.72it/s]168574it [01:09, 3408.03it/s]167124it [01:09, 3437.95it/s]182538it [01:09, 3361.40it/s]168917it [01:09, 3312.68it/s]158141it [01:09, 196.34it/s] 167471it [01:09, 3332.87it/s]182917it [01:10, 3479.80it/s]169286it [01:09, 3420.35it/s]167839it [01:09, 3431.63it/s]158492it [01:09, 276.03it/s]183268it [01:10, 3386.77it/s]169651it [01:09, 3485.84it/s]183641it [01:10, 3484.81it/s]168185it [01:09, 3334.58it/s]158835it [01:10, 379.17it/s]170001it [01:10, 3319.16it/s]184017it [01:10, 3563.39it/s]168556it [01:09, 3440.25it/s]159201it [01:10, 527.15it/s]170375it [01:10, 3437.58it/s]159564it [01:10, 714.92it/s]184375it [01:10, 3447.33it/s]168916it [01:10, 3224.03it/s]170721it [01:10, 3199.08it/s]184751it [01:10, 3536.61it/s]169272it [01:10, 3317.18it/s]159886it [01:10, 895.22it/s]171084it [01:10, 3311.67it/s]185107it [01:10, 3430.40it/s]169623it [01:10, 3371.43it/s]160242it [01:10, 1161.99it/s]171434it [01:10, 3242.42it/s]185474it [01:10, 3498.45it/s]160560it [01:10, 1402.57it/s]169963it [01:10, 3266.88it/s]171793it [01:10, 3338.74it/s]185826it [01:10, 3394.89it/s]160919it [01:10, 1731.83it/s]170329it [01:10, 3377.88it/s]172153it [01:10, 3411.23it/s]186207it [01:11, 3513.07it/s]161282it [01:10, 2067.24it/s]170670it [01:10, 3280.73it/s]172497it [01:10, 3322.78it/s]186560it [01:11, 3385.63it/s]161619it [01:10, 2263.85it/s]171034it [01:10, 3382.94it/s]172869it [01:10, 3435.34it/s]186957it [01:11, 3550.06it/s]171400it [01:10, 3460.53it/s]161985it [01:10, 2564.55it/s]173215it [01:10, 3333.37it/s]187329it [01:11, 3598.69it/s]162321it [01:11, 2688.67it/s]171748it [01:10, 3323.70it/s]173579it [01:11, 3419.35it/s]187691it [01:11, 3477.66it/s]162685it [01:11, 2921.23it/s]172119it [01:10, 3433.74it/s]173941it [01:11, 3476.99it/s]188071it [01:11, 3569.44it/s]172465it [01:11, 3295.02it/s]163035it [01:11, 2922.13it/s]174290it [01:11, 3296.79it/s]188430it [01:11, 3444.08it/s]172819it [01:11, 3363.62it/s]163386it [01:11, 3075.91it/s]174660it [01:11, 3410.20it/s]188804it [01:11, 3528.02it/s]163753it [01:11, 3237.05it/s]173158it [01:11, 3283.97it/s]175004it [01:11, 3313.80it/s]189159it [01:11, 3394.19it/s]173522it [01:11, 3385.79it/s]164096it [01:11, 3190.46it/s]175369it [01:11, 3409.69it/s]189536it [01:11, 3499.19it/s]173886it [01:11, 3456.49it/s]164452it [01:11, 3293.17it/s]175712it [01:11, 3321.77it/s]189914it [01:12, 3579.98it/s]174233it [01:11, 3348.94it/s]164792it [01:11, 3232.82it/s]176068it [01:11, 3389.85it/s]190274it [01:12, 3467.98it/s]174607it [01:11, 3459.00it/s]165161it [01:11, 3362.32it/s]176428it [01:11, 3448.77it/s]190634it [01:12, 3505.89it/s]165529it [01:12, 3453.35it/s]174955it [01:11, 3332.47it/s]176775it [01:12, 3316.98it/s]190986it [01:12, 3389.61it/s]175312it [01:11, 3398.69it/s]165879it [01:12, 3322.77it/s]177133it [01:12, 3389.88it/s]191360it [01:12, 3489.98it/s]166251it [01:12, 3436.13it/s]175654it [01:12, 3293.31it/s]177474it [01:12, 3291.38it/s]191711it [01:12, 3385.22it/s]176006it [01:12, 3357.26it/s]166598it [01:12, 3308.31it/s]177826it [01:12, 3354.86it/s]192092it [01:12, 3506.19it/s]176359it [01:12, 3405.77it/s]166951it [01:12, 3371.16it/s]178163it [01:12, 3291.96it/s]192445it [01:12, 3403.27it/s]176701it [01:12, 3303.13it/s]167291it [01:12, 3282.80it/s]178523it [01:12, 3380.77it/s]192832it [01:12, 3536.78it/s]177061it [01:12, 3386.82it/s]167657it [01:12, 3390.24it/s]178878it [01:12, 3427.59it/s]193209it [01:13, 3604.04it/s]168023it [01:12, 3468.28it/s]177401it [01:12, 3287.24it/s]179222it [01:12, 3311.00it/s]177763it [01:12, 3380.78it/s]193571it [01:13, 3441.58it/s]168372it [01:12, 3330.60it/s]179587it [01:12, 3407.99it/s]193956it [01:13, 3557.92it/s]178122it [01:12, 3439.40it/s]168742it [01:12, 3434.39it/s]179930it [01:12, 3308.93it/s]194314it [01:13, 3449.33it/s]178468it [01:12, 3320.75it/s]169088it [01:13, 3324.23it/s]180305it [01:13, 3433.74it/s]194673it [01:13, 3488.34it/s]178840it [01:12, 3434.08it/s]169453it [01:13, 3417.24it/s]180662it [01:13, 3473.24it/s]195024it [01:13, 3385.81it/s]179185it [01:13, 3308.53it/s]169797it [01:13, 3267.61it/s]181011it [01:13, 3333.38it/s]195412it [01:13, 3526.01it/s]179550it [01:13, 3404.86it/s]170162it [01:13, 3375.19it/s]181364it [01:13, 3388.41it/s]195788it [01:13, 3593.25it/s]170506it [01:13, 3391.23it/s]179893it [01:13, 3171.90it/s]181705it [01:13, 3239.77it/s]196149it [01:13, 3440.48it/s]170847it [01:13, 3278.77it/s]180257it [01:13, 3300.71it/s]182066it [01:13, 3343.70it/s]196528it [01:14, 3538.88it/s]171191it [01:13, 3322.83it/s]180610it [01:13, 3365.23it/s]182403it [01:13, 3248.31it/s]196884it [01:14, 3460.02it/s]171525it [01:13, 3228.31it/s]180950it [01:13, 3253.69it/s]182761it [01:13, 3342.65it/s]197266it [01:14, 3562.40it/s]171887it [01:13, 3339.67it/s]181306it [01:13, 3336.45it/s]183116it [01:13, 3402.53it/s]172240it [01:14, 3392.53it/s]197624it [01:14, 3427.69it/s]181642it [01:13, 3212.39it/s]183458it [01:14, 3265.76it/s]198009it [01:14, 3546.03it/s]172581it [01:14, 3226.13it/s]181998it [01:13, 3309.93it/s]183810it [01:14, 3337.19it/s]198366it [01:14, 3419.50it/s]172939it [01:14, 3325.08it/s]182352it [01:14, 3375.25it/s]184146it [01:14, 3160.14it/s]198737it [01:14, 3495.79it/s]173274it [01:14, 3146.79it/s]182692it [01:14, 3179.60it/s]184492it [01:14, 3243.63it/s]199105it [01:14, 3546.35it/s]173607it [01:14, 3197.62it/s]183044it [01:14, 3272.78it/s]184824it [01:14, 3263.92it/s]199462it [01:14, 3382.98it/s]183375it [01:14, 3108.59it/s]173955it [01:14, 3060.06it/s]185153it [01:14, 3117.36it/s]199831it [01:14, 3469.41it/s]183716it [01:14, 3190.92it/s]174308it [01:14, 3189.29it/s]185491it [01:14, 3190.45it/s]200181it [01:15, 3329.80it/s]174668it [01:14, 3303.73it/s]184038it [01:14, 3041.43it/s]185813it [01:14, 3058.73it/s]200545it [01:15, 3417.50it/s]184388it [01:14, 3167.65it/s]175002it [01:14, 3120.16it/s]186162it [01:14, 3179.78it/s]200889it [01:15, 3283.59it/s]184747it [01:14, 3285.85it/s]175342it [01:15, 3196.34it/s]186520it [01:14, 3292.27it/s]201255it [01:15, 3388.49it/s]185079it [01:14, 3140.42it/s]175665it [01:15, 3056.83it/s]186852it [01:15, 3159.32it/s]201612it [01:15, 3439.73it/s]185414it [01:15, 3198.79it/s]176004it [01:15, 3148.45it/s]187187it [01:15, 3211.77it/s]201958it [01:15, 3330.55it/s]176351it [01:15, 3237.98it/s]185737it [01:15, 3052.39it/s]187511it [01:15, 3073.88it/s]202339it [01:15, 3466.47it/s]186103it [01:15, 3221.09it/s]176678it [01:15, 3077.88it/s]187874it [01:15, 3230.24it/s]202688it [01:15, 3392.58it/s]186450it [01:15, 3290.87it/s]177027it [01:15, 3192.17it/s]188230it [01:15, 3323.86it/s]203071it [01:15, 3517.67it/s]186782it [01:15, 3179.66it/s]177350it [01:15, 3058.05it/s]188565it [01:15, 3154.49it/s]203425it [01:16, 3386.80it/s]187130it [01:15, 3264.41it/s]177696it [01:15, 3170.42it/s]188916it [01:15, 3253.08it/s]203777it [01:16, 3416.66it/s]178045it [01:15, 3260.16it/s]187459it [01:15, 3124.63it/s]189244it [01:15, 3065.08it/s]204145it [01:16, 3490.18it/s]187816it [01:15, 3248.66it/s]178374it [01:15, 3114.48it/s]189594it [01:15, 3184.27it/s]204496it [01:16, 3368.72it/s]188180it [01:15, 3360.14it/s]178732it [01:16, 3244.84it/s]189916it [01:16, 3065.38it/s]204868it [01:16, 3468.15it/s]188519it [01:15, 3188.24it/s]179060it [01:16, 3070.86it/s]190280it [01:16, 3225.07it/s]188865it [01:16, 3258.01it/s]179410it [01:16, 3189.08it/s]190633it [01:16, 3310.75it/s]179769it [01:16, 3302.09it/s]189194it [01:16, 3101.82it/s]190967it [01:16, 3107.31it/s]189550it [01:16, 3227.81it/s]180103it [01:16, 3129.17it/s]191321it [01:16, 3225.85it/s]189907it [01:16, 3322.93it/s]180445it [01:16, 3210.64it/s]191648it [01:16, 3062.84it/s]190242it [01:16, 3156.37it/s]180770it [01:16, 3054.58it/s]192000it [01:16, 3188.29it/s]190582it [01:16, 3223.30it/s]181122it [01:16, 3182.20it/s]192352it [01:16, 3282.03it/s]181469it [01:16, 3262.47it/s]190907it [01:16, 3076.06it/s]192684it [01:16, 3149.05it/s]191262it [01:16, 3206.66it/s]181798it [01:17, 3110.18it/s]193031it [01:17, 3237.96it/s]182145it [01:17, 3211.07it/s]191596it [01:16, 3041.06it/s]193358it [01:17, 3080.13it/s]191956it [01:17, 3195.00it/s]182469it [01:17, 3064.25it/s]193712it [01:17, 3207.78it/s]192310it [01:17, 3290.89it/s]182833it [01:17, 3225.92it/s]194074it [01:17, 3325.24it/s]183187it [01:17, 3313.28it/s]192643it [01:17, 3157.64it/s]194410it [01:17, 3149.57it/s]193002it [01:17, 3278.01it/s]183521it [01:17, 3123.93it/s]194763it [01:17, 3255.55it/s]183878it [01:17, 3248.14it/s]193333it [01:17, 3128.91it/s]195092it [01:17, 3089.33it/s]193685it [01:17, 3238.19it/s]184207it [01:17, 3108.83it/s]195444it [01:17, 3204.87it/s]194046it [01:17, 3342.68it/s]184565it [01:17, 3240.91it/s]195794it [01:17, 3086.06it/s]194383it [01:17, 3129.86it/s]184893it [01:18, 3073.94it/s]196139it [01:18, 3185.64it/s]194721it [01:17, 3198.76it/s]185235it [01:18, 3168.63it/s]196482it [01:18, 3254.57it/s]185583it [01:18, 3256.94it/s]195045it [01:18, 3060.43it/s]196811it [01:18, 3116.38it/s]195416it [01:18, 3240.91it/s]185912it [01:18, 3090.82it/s]197147it [01:18, 3182.52it/s]195771it [01:18, 3327.62it/s]186274it [01:18, 3237.16it/s]197474it [01:18, 3065.21it/s]196107it [01:18, 3129.83it/s]186601it [01:18, 3098.74it/s]197834it [01:18, 3215.19it/s]196459it [01:18, 3237.89it/s]186974it [01:18, 3274.96it/s]198187it [01:18, 3304.55it/s]187316it [01:18, 3316.15it/s]196787it [01:18, 3101.30it/s]198520it [01:18, 3155.55it/s]197128it [01:18, 3186.47it/s]187651it [01:18, 3153.78it/s]198866it [01:18, 3241.37it/s]188003it [01:19, 3255.63it/s]197476it [01:18, 3053.01it/s]199193it [01:18, 3060.87it/s]188332it [01:19, 3102.87it/s]197838it [01:18, 3207.97it/s]199559it [01:19, 3227.69it/s]188686it [01:19, 3225.40it/s]198202it [01:19, 3328.55it/s]199907it [01:19, 3298.23it/s]189039it [01:19, 3310.12it/s]198539it [01:19, 3170.88it/s]200240it [01:19, 3106.79it/s]198893it [01:19, 3273.70it/s]189373it [01:19, 3137.31it/s]200580it [01:19, 3187.45it/s]189724it [01:19, 3241.35it/s]199224it [01:19, 3119.46it/s]200903it [01:19, 3046.21it/s]190052it [01:19, 3083.71it/s]199588it [01:19, 3218.05it/s]201245it [01:19, 3148.78it/s]190406it [01:19, 3209.80it/s]199931it [01:19, 3275.69it/s]201604it [01:19, 3265.46it/s]190756it [01:19, 3089.22it/s]200261it [01:19, 3102.84it/s]201934it [01:19, 3117.99it/s]191104it [01:19, 3195.63it/s]200614it [01:19, 3221.84it/s]202304it [01:19, 3272.44it/s]191454it [01:20, 3280.70it/s]200940it [01:19, 3042.23it/s]202635it [01:20, 3136.98it/s]191785it [01:20, 3124.84it/s]201294it [01:20, 3179.41it/s]202988it [01:20, 3245.69it/s]192151it [01:20, 3274.40it/s]201655it [01:20, 3299.97it/s]203342it [01:20, 3327.43it/s]192482it [01:20, 3105.00it/s]201989it [01:20, 3143.92it/s]203677it [01:20, 3152.52it/s]192850it [01:20, 3264.90it/s]202356it [01:20, 3291.11it/s]204029it [01:20, 3254.72it/s]193213it [01:20, 3366.81it/s]202689it [01:20, 3124.71it/s]204358it [01:20, 3109.82it/s]193553it [01:20, 3173.44it/s]203058it [01:20, 3282.02it/s]204716it [01:20, 3240.51it/s]193905it [01:20, 3269.81it/s]203390it [01:20, 3109.73it/s]205043it [01:20, 3090.56it/s]194236it [01:20, 3127.73it/s]203744it [01:20, 3228.52it/s]194597it [01:21, 3260.62it/s]204095it [01:20, 3307.03it/s]194954it [01:21, 3346.74it/s]204429it [01:20, 3139.11it/s]195292it [01:21, 3180.94it/s]204784it [01:21, 3253.94it/s]195649it [01:21, 3290.26it/s]195982it [01:21, 3127.56it/s]196329it [01:21, 3221.64it/s]196655it [01:21, 3110.92it/s]197018it [01:21, 3256.30it/s]197385it [01:21, 3374.25it/s]197725it [01:22, 3177.67it/s]205217it [01:22, 194.57it/s] 198096it [01:22, 3326.64it/s]205591it [01:22, 275.14it/s]198433it [01:22, 3170.87it/s]205946it [01:22, 376.71it/s]198798it [01:22, 3302.61it/s]206324it [01:22, 522.44it/s]199147it [01:22, 3355.67it/s]206700it [01:22, 709.11it/s]207036it [01:22, 906.29it/s]199486it [01:22, 3204.20it/s]207413it [01:22, 1185.79it/s]199839it [01:22, 3295.30it/s]207757it [01:23, 1443.98it/s]200172it [01:22, 3121.26it/s]208119it [01:23, 1764.26it/s]200505it [01:22, 3178.53it/s]208466it [01:23, 2035.46it/s]200836it [01:23, 3038.86it/s]208848it [01:23, 2386.61it/s]201194it [01:23, 3187.05it/s]209227it [01:23, 2694.28it/s]201541it [01:23, 3266.93it/s]209586it [01:23, 2802.25it/s]201871it [01:23, 3144.28it/s]209955it [01:23, 3021.43it/s]202236it [01:23, 3279.65it/s]210307it [01:23, 3048.99it/s]202567it [01:23, 3167.53it/s]210679it [01:23, 3226.52it/s]202926it [01:23, 3286.93it/s]211029it [01:24, 3185.09it/s]203288it [01:23, 3380.85it/s]211402it [01:24, 3334.09it/s]203629it [01:23, 3211.16it/s]211771it [01:24, 3433.83it/s]203980it [01:23, 3293.97it/s]212126it [01:24, 3341.03it/s]204312it [01:24, 3142.56it/s]212488it [01:24, 3402.36it/s]204657it [01:24, 3226.94it/s]212835it [01:24, 3314.55it/s]205015it [01:24, 3327.06it/s]213208it [01:24, 3432.35it/s]213555it [01:24, 3332.21it/s]213930it [01:24, 3450.93it/s]214303it [01:24, 3530.97it/s]214659it [01:25, 3396.63it/s]215028it [01:25, 3480.09it/s]215379it [01:25, 3381.51it/s]215749it [01:25, 3470.33it/s]216098it [01:25, 3381.90it/s]216477it [01:25, 3498.23it/s]216863it [01:25, 3601.28it/s]217225it [01:25, 3433.00it/s]217611it [01:25, 3554.03it/s]217969it [01:26, 3477.89it/s]218351it [01:26, 3575.36it/s]218711it [01:26, 3505.80it/s]219097it [01:26, 3607.68it/s]219460it [01:26, 3509.83it/s]219852it [01:26, 3625.93it/s]220227it [01:26, 3533.80it/s]220626it [01:26, 3663.07it/s]221009it [01:26, 3710.35it/s]221382it [01:26, 3581.65it/s]221767it [01:27, 3655.29it/s]222134it [01:27, 3520.87it/s]222510it [01:27, 3588.69it/s]222871it [01:27, 3462.79it/s]223245it [01:27, 3541.61it/s]223601it [01:27, 3413.60it/s]205356it [01:27, 164.69it/s] 223980it [01:27, 3518.74it/s]205709it [01:27, 234.68it/s]224355it [01:27, 3584.14it/s]205976it [01:27, 304.93it/s]224715it [01:27, 3432.06it/s]206337it [01:27, 435.73it/s]225084it [01:28, 3505.04it/s]206696it [01:27, 604.52it/s]225437it [01:28, 3383.36it/s]207008it [01:27, 776.57it/s]225808it [01:28, 3474.77it/s]205113it [01:27, 165.12it/s] 207362it [01:27, 1028.92it/s]205470it [01:27, 234.10it/s]226158it [01:28, 3284.65it/s]207680it [01:28, 1254.55it/s]205825it [01:27, 327.36it/s]226525it [01:28, 3392.13it/s]208043it [01:28, 1585.57it/s]206120it [01:28, 427.30it/s]226886it [01:28, 3452.57it/s]208412it [01:28, 1935.01it/s]206479it [01:28, 592.20it/s]227234it [01:28, 3323.02it/s]208748it [01:28, 2129.13it/s]206789it [01:28, 758.39it/s]227599it [01:28, 3413.54it/s]209112it [01:28, 2443.99it/s]207147it [01:28, 1010.16it/s]227943it [01:28, 3300.32it/s]209445it [01:28, 2522.57it/s]207485it [01:28, 1277.33it/s]228309it [01:28, 3401.52it/s]209795it [01:28, 2754.62it/s]207807it [01:28, 1513.91it/s]228652it [01:29, 3300.02it/s]210142it [01:28, 2756.93it/s]208177it [01:28, 1868.97it/s]229013it [01:29, 3386.45it/s]210497it [01:28, 2957.92it/s]229390it [01:29, 3495.30it/s]208504it [01:28, 2075.58it/s]210848it [01:28, 3103.13it/s]208869it [01:28, 2402.31it/s]229742it [01:29, 3381.43it/s]211180it [01:29, 2980.44it/s]209235it [01:29, 2682.54it/s]230105it [01:29, 3451.26it/s]211535it [01:29, 3132.76it/s]209575it [01:29, 2715.89it/s]230452it [01:29, 3291.04it/s]211861it [01:29, 3020.20it/s]209909it [01:29, 2871.01it/s]230822it [01:29, 3405.04it/s]212214it [01:29, 3158.86it/s]231165it [01:29, 3324.17it/s]210235it [01:29, 2829.15it/s]212574it [01:29, 3283.18it/s]231533it [01:29, 3423.83it/s]210593it [01:29, 3025.58it/s]212909it [01:29, 3126.60it/s]231903it [01:30, 3502.88it/s]210950it [01:29, 3172.11it/s]213253it [01:29, 3213.72it/s]232255it [01:30, 3366.54it/s]211284it [01:29, 3045.34it/s]213579it [01:29, 3062.59it/s]232619it [01:30, 3443.21it/s]211640it [01:29, 3186.17it/s]213939it [01:29, 3212.05it/s]232966it [01:30, 3339.35it/s]211969it [01:29, 3057.04it/s]214288it [01:30, 3290.89it/s]233338it [01:30, 3447.86it/s]212314it [01:29, 3164.70it/s]214621it [01:30, 3127.08it/s]233685it [01:30, 3338.32it/s]212665it [01:30, 3043.56it/s]214973it [01:30, 3231.69it/s]234057it [01:30, 3446.83it/s]213022it [01:30, 3186.53it/s]215300it [01:30, 3082.12it/s]234423it [01:30, 3507.85it/s]213378it [01:30, 3290.41it/s]215656it [01:30, 3214.81it/s]234776it [01:30, 3365.08it/s]213712it [01:30, 3118.59it/s]216009it [01:30, 3304.72it/s]235141it [01:30, 3446.13it/s]214069it [01:30, 3244.28it/s]216343it [01:30, 3143.85it/s]235488it [01:31, 3319.45it/s]214398it [01:30, 3061.66it/s]216699it [01:30, 3261.08it/s]235852it [01:31, 3408.85it/s]214750it [01:30, 3186.86it/s]217029it [01:30, 3111.92it/s]236195it [01:31, 3307.78it/s]215103it [01:30, 3283.75it/s]217378it [01:31, 3216.85it/s]236559it [01:31, 3402.55it/s]215435it [01:30, 3104.20it/s]236926it [01:31, 3479.68it/s]217703it [01:31, 3101.05it/s]215790it [01:31, 3227.56it/s]218076it [01:31, 3277.94it/s]237276it [01:31, 3341.99it/s]216117it [01:31, 3066.50it/s]218437it [01:31, 3371.89it/s]237640it [01:31, 3425.99it/s]216477it [01:31, 3214.32it/s]218777it [01:31, 3254.20it/s]237985it [01:31, 3307.98it/s]205350it [01:31, 152.73it/s] 216842it [01:31, 3338.12it/s]219133it [01:31, 3339.40it/s]238333it [01:31, 3356.95it/s]205709it [01:31, 216.82it/s]217180it [01:31, 3190.83it/s]238695it [01:32, 3431.65it/s]219469it [01:31, 3204.74it/s]205983it [01:31, 282.28it/s]217543it [01:31, 3314.25it/s]219842it [01:31, 3353.07it/s]206349it [01:31, 402.99it/s]239040it [01:32, 3305.73it/s]217878it [01:31, 3189.14it/s]220218it [01:31, 3470.07it/s]206698it [01:31, 552.94it/s]239401it [01:32, 3390.49it/s]218232it [01:31, 3286.28it/s]207014it [01:32, 716.18it/s]239742it [01:32, 3290.21it/s]220568it [01:32, 3314.41it/s]218564it [01:31, 3141.98it/s]207375it [01:32, 959.66it/s]240095it [01:32, 3358.86it/s]220930it [01:32, 3399.20it/s]218936it [01:32, 3303.31it/s]240433it [01:32, 3253.46it/s]207697it [01:32, 1183.69it/s]221273it [01:32, 3210.74it/s]219315it [01:32, 3435.16it/s]240796it [01:32, 3360.46it/s]208066it [01:32, 1512.04it/s]221660it [01:32, 3393.39it/s]219662it [01:32, 3264.87it/s]241163it [01:32, 3448.03it/s]208429it [01:32, 1845.99it/s]222003it [01:32, 3240.48it/s]220039it [01:32, 3406.48it/s]241510it [01:32, 3327.91it/s]208767it [01:32, 2062.01it/s]222355it [01:32, 3316.22it/s]220383it [01:32, 3239.45it/s]241883it [01:32, 3443.05it/s]209138it [01:32, 2395.57it/s]222719it [01:32, 3407.38it/s]220757it [01:32, 3377.76it/s]242229it [01:33, 3292.49it/s]209475it [01:32, 2500.50it/s]223063it [01:32, 3185.13it/s]221099it [01:32, 3220.52it/s]242597it [01:33, 3400.40it/s]209817it [01:32, 2715.63it/s]223421it [01:32, 3292.94it/s]221482it [01:32, 3388.16it/s]242940it [01:33, 3292.78it/s]210145it [01:33, 2741.14it/s]223755it [01:32, 3143.50it/s]221858it [01:32, 3492.07it/s]243311it [01:33, 3410.64it/s]210503it [01:33, 2954.79it/s]224121it [01:33, 3287.18it/s]243678it [01:33, 3483.90it/s]222211it [01:33, 3281.87it/s]210860it [01:33, 3118.75it/s]224454it [01:33, 3124.08it/s]222559it [01:33, 3336.08it/s]244029it [01:33, 3331.31it/s]211196it [01:33, 3031.01it/s]224811it [01:33, 3245.79it/s]244393it [01:33, 3418.70it/s]222897it [01:33, 3167.91it/s]211545it [01:33, 3149.21it/s]225162it [01:33, 3319.88it/s]223260it [01:33, 3295.00it/s]244737it [01:33, 3310.52it/s]211873it [01:33, 3068.32it/s]225497it [01:33, 3147.98it/s]245102it [01:33, 3406.44it/s]223594it [01:33, 3133.55it/s]212231it [01:33, 3210.06it/s]225849it [01:33, 3250.53it/s]223957it [01:33, 3270.75it/s]245445it [01:34, 3294.09it/s]212590it [01:33, 3316.49it/s]226178it [01:33, 3095.70it/s]224321it [01:33, 3339.92it/s]245777it [01:34, 3249.82it/s]212928it [01:33, 3126.75it/s]226533it [01:33, 3222.03it/s]246141it [01:34, 3360.71it/s]224658it [01:33, 3168.43it/s]213284it [01:33, 3246.37it/s]226884it [01:33, 3301.99it/s]246479it [01:34, 3268.99it/s]225018it [01:33, 3286.47it/s]213614it [01:34, 3099.27it/s]227217it [01:34, 3112.15it/s]246847it [01:34, 3385.93it/s]225350it [01:34, 3119.79it/s]213975it [01:34, 3241.42it/s]227566it [01:34, 3216.31it/s]247187it [01:34, 3284.13it/s]225709it [01:34, 3249.23it/s]214333it [01:34, 3337.30it/s]227891it [01:34, 3068.67it/s]247551it [01:34, 3385.80it/s]226060it [01:34, 3323.08it/s]214671it [01:34, 3161.05it/s]228242it [01:34, 3190.14it/s]247918it [01:34, 3465.69it/s]226396it [01:34, 3105.47it/s]215012it [01:34, 3229.00it/s]228595it [01:34, 3286.03it/s]248266it [01:34, 3343.25it/s]226751it [01:34, 3228.06it/s]215339it [01:34, 3102.97it/s]228927it [01:34, 3092.11it/s]248631it [01:34, 3429.66it/s]215697it [01:34, 3236.18it/s]227078it [01:34, 3081.08it/s]229274it [01:34, 3196.45it/s]248976it [01:35, 3279.33it/s]227430it [01:34, 3203.19it/s]216025it [01:34, 3102.39it/s]229598it [01:34, 3094.21it/s]249343it [01:35, 3388.98it/s]227783it [01:34, 3293.79it/s]216379it [01:34, 3224.89it/s]229953it [01:34, 3212.57it/s]249684it [01:35, 3266.42it/s]216731it [01:35, 3309.04it/s]228116it [01:34, 3096.08it/s]230300it [01:35, 3286.16it/s]250053it [01:35, 3385.54it/s]228459it [01:34, 3188.97it/s]217065it [01:35, 3192.35it/s]230631it [01:35, 3111.01it/s]250426it [01:35, 3484.34it/s]217420it [01:35, 3293.54it/s]228782it [01:35, 3044.46it/s]230982it [01:35, 3222.47it/s]250777it [01:35, 3369.96it/s]217752it [01:35, 3190.47it/s]229131it [01:35, 3166.28it/s]231308it [01:35, 3085.79it/s]251144it [01:35, 3455.67it/s]218111it [01:35, 3304.30it/s]229465it [01:35, 3065.01it/s]231664it [01:35, 3217.44it/s]251492it [01:35, 3291.71it/s]218476it [01:35, 3401.87it/s]229825it [01:35, 3214.48it/s]231989it [01:35, 3071.19it/s]251856it [01:35, 3389.11it/s]218818it [01:35, 3256.71it/s]230170it [01:35, 3279.97it/s]232337it [01:35, 3185.04it/s]252198it [01:36, 3287.29it/s]219186it [01:35, 3377.43it/s]230501it [01:35, 3081.85it/s]232683it [01:35, 3257.67it/s]252561it [01:36, 3383.19it/s]219526it [01:35, 3247.73it/s]230859it [01:35, 3218.70it/s]233012it [01:35, 3111.67it/s]252924it [01:36, 3453.98it/s]219887it [01:36, 3350.56it/s]231185it [01:35, 3100.20it/s]233371it [01:36, 3244.41it/s]253271it [01:36, 3316.83it/s]220226it [01:36, 3207.70it/s]231529it [01:35, 3193.99it/s]233699it [01:36, 3070.41it/s]253621it [01:36, 3367.20it/s]220602it [01:36, 3363.22it/s]231884it [01:36, 3294.53it/s]234054it [01:36, 3204.25it/s]253960it [01:36, 3278.64it/s]220968it [01:36, 3445.85it/s]232216it [01:36, 3074.01it/s]234400it [01:36, 3276.24it/s]254317it [01:36, 3355.14it/s]221315it [01:36, 3251.54it/s]232573it [01:36, 3209.81it/s]234731it [01:36, 3087.83it/s]254667it [01:36, 3267.75it/s]221700it [01:36, 3418.26it/s]232898it [01:36, 3064.58it/s]235084it [01:36, 3210.05it/s]255032it [01:36, 3374.20it/s]222046it [01:36, 3252.06it/s]233260it [01:36, 3218.09it/s]255398it [01:37, 3455.63it/s]235409it [01:36, 3067.53it/s]222404it [01:36, 3342.35it/s]233616it [01:36, 3314.10it/s]235763it [01:36, 3190.21it/s]255745it [01:37, 3342.72it/s]222746it [01:36, 3155.31it/s]233951it [01:36, 3133.96it/s]236113it [01:36, 3270.64it/s]256091it [01:37, 3375.34it/s]223109it [01:37, 3285.93it/s]234299it [01:36, 3230.13it/s]256430it [01:37, 3288.00it/s]236443it [01:36, 3112.53it/s]223469it [01:37, 3373.65it/s]234626it [01:36, 3081.29it/s]256799it [01:37, 3402.26it/s]236789it [01:37, 3208.27it/s]223810it [01:37, 3194.65it/s]234982it [01:37, 3213.05it/s]257169it [01:37, 3486.17it/s]237113it [01:37, 3054.61it/s]224177it [01:37, 3326.81it/s]235334it [01:37, 3298.71it/s]257519it [01:37, 3343.39it/s]237461it [01:37, 3173.62it/s]224514it [01:37, 3143.29it/s]235667it [01:37, 3121.55it/s]257886it [01:37, 3434.48it/s]237811it [01:37, 3266.60it/s]224870it [01:37, 3258.64it/s]236017it [01:37, 3227.54it/s]258232it [01:37, 3279.87it/s]238141it [01:37, 3091.63it/s]225231it [01:37, 3356.63it/s]258600it [01:37, 3392.72it/s]236343it [01:37, 3047.96it/s]238482it [01:37, 3180.55it/s]225570it [01:37, 3188.84it/s]236695it [01:37, 3177.82it/s]258942it [01:38, 3303.64it/s]238803it [01:37, 3033.49it/s]225923it [01:37, 3282.11it/s]259309it [01:38, 3408.05it/s]237025it [01:37, 3050.74it/s]239151it [01:37, 3157.51it/s]259679it [01:38, 3490.58it/s]226255it [01:37, 3089.95it/s]237375it [01:37, 3175.61it/s]239501it [01:37, 3253.84it/s]226611it [01:38, 3219.70it/s]237725it [01:37, 3267.24it/s]260030it [01:38, 3372.82it/s]239829it [01:38, 3094.23it/s]260386it [01:38, 3425.59it/s]226946it [01:38, 3073.88it/s]238055it [01:38, 3055.58it/s]240178it [01:38, 3203.97it/s]260731it [01:38, 3329.01it/s]227298it [01:38, 3195.04it/s]238404it [01:38, 3175.20it/s]240502it [01:38, 3038.04it/s]261104it [01:38, 3441.78it/s]227639it [01:38, 3254.70it/s]238726it [01:38, 3029.88it/s]240855it [01:38, 3173.44it/s]261450it [01:38, 3343.93it/s]227968it [01:38, 3102.71it/s]239079it [01:38, 3168.80it/s]241212it [01:38, 3285.61it/s]261819it [01:38, 3442.33it/s]228324it [01:38, 3229.79it/s]239429it [01:38, 3262.86it/s]241544it [01:38, 3119.51it/s]262172it [01:39, 3467.07it/s]228651it [01:38, 3081.55it/s]239759it [01:38, 3105.16it/s]241904it [01:38, 3253.61it/s]262520it [01:39, 3343.42it/s]228999it [01:38, 3191.62it/s]240113it [01:38, 3225.57it/s]242233it [01:38, 3077.90it/s]262890it [01:39, 3444.90it/s]229362it [01:38, 3316.49it/s]240439it [01:38, 3048.04it/s]242588it [01:38, 3208.50it/s]263236it [01:39, 3326.44it/s]229697it [01:39, 3110.92it/s]240795it [01:38, 3188.93it/s]242913it [01:39, 3064.27it/s]263604it [01:39, 3426.63it/s]230055it [01:39, 3239.77it/s]241153it [01:38, 3298.23it/s]243272it [01:39, 3208.87it/s]263949it [01:39, 3282.28it/s]230383it [01:39, 3088.92it/s]241486it [01:39, 3126.17it/s]243626it [01:39, 3302.72it/s]230734it [01:39, 3206.20it/s]241848it [01:39, 3262.86it/s]243960it [01:39, 3137.56it/s]231104it [01:39, 3344.46it/s]242178it [01:39, 3104.46it/s]244304it [01:39, 3220.85it/s]231442it [01:39, 3119.45it/s]242519it [01:39, 3189.60it/s]244630it [01:39, 3070.39it/s]231800it [01:39, 3246.37it/s]242873it [01:39, 3287.22it/s]244987it [01:39, 3209.69it/s]232129it [01:39, 3103.41it/s]243205it [01:39, 3125.02it/s]245338it [01:39, 3294.90it/s]232482it [01:39, 3220.71it/s]243552it [01:39, 3220.75it/s]245671it [01:39, 3102.91it/s]243877it [01:39, 3094.66it/s]232826it [01:40, 3087.56it/s]246026it [01:39, 3225.61it/s]244229it [01:39, 3214.03it/s]233190it [01:40, 3238.02it/s]246353it [01:40, 3037.62it/s]244579it [01:40, 3294.58it/s]233526it [01:40, 3264.17it/s]246707it [01:40, 3174.10it/s]233856it [01:40, 3118.80it/s]244911it [01:40, 3043.08it/s]247057it [01:40, 3266.31it/s]234217it [01:40, 3255.65it/s]245263it [01:40, 3175.02it/s]247387it [01:40, 3093.22it/s]234546it [01:40, 3092.27it/s]245586it [01:40, 3035.15it/s]247738it [01:40, 3208.32it/s]234902it [01:40, 3220.52it/s]245936it [01:40, 3162.29it/s]248063it [01:40, 3024.11it/s]235247it [01:40, 3283.27it/s]246265it [01:40, 3001.68it/s]248404it [01:40, 3129.83it/s]235578it [01:40, 3059.63it/s]246619it [01:40, 3148.49it/s]248756it [01:40, 3239.59it/s]235934it [01:41, 3196.79it/s]246975it [01:40, 3263.01it/s]249084it [01:40, 3086.67it/s]236258it [01:41, 3060.54it/s]249437it [01:41, 3210.70it/s]247305it [01:40, 3050.47it/s]236612it [01:41, 3194.13it/s]247659it [01:41, 3185.67it/s]249762it [01:41, 3065.56it/s]236973it [01:41, 3311.75it/s]250103it [01:41, 3160.17it/s]247983it [01:41, 3034.57it/s]237308it [01:41, 3075.11it/s]248338it [01:41, 3176.54it/s]250463it [01:41, 3051.61it/s]237660it [01:41, 3196.89it/s]248690it [01:41, 3273.54it/s]250821it [01:41, 3194.65it/s]237985it [01:41, 3056.07it/s]249021it [01:41, 3103.75it/s]251173it [01:41, 3285.56it/s]238334it [01:41, 3175.57it/s]249374it [01:41, 3220.68it/s]251505it [01:41, 3105.87it/s]238686it [01:41, 3271.37it/s]251856it [01:41, 3217.96it/s]249700it [01:41, 2999.10it/s]239017it [01:42, 3081.30it/s]250050it [01:41, 3135.89it/s]252182it [01:41, 3056.88it/s]239337it [01:42, 3111.57it/s]250395it [01:41, 3223.70it/s]252509it [01:42, 3114.44it/s]239652it [01:42, 2944.46it/s]250722it [01:42, 3080.83it/s]252856it [01:42, 3215.08it/s]240000it [01:42, 3092.17it/s]251070it [01:42, 3192.43it/s]253181it [01:42, 3054.94it/s]240345it [01:42, 3191.99it/s]253534it [01:42, 3186.00it/s]251393it [01:42, 3031.05it/s]240668it [01:42, 3041.43it/s]251743it [01:42, 3160.65it/s]253856it [01:42, 3032.86it/s]241000it [01:42, 3118.83it/s]252068it [01:42, 3184.91it/s]254185it [01:42, 3103.13it/s]241315it [01:42, 3003.80it/s]254534it [01:42, 3211.92it/s]252390it [01:42, 3027.16it/s]241668it [01:42, 3152.21it/s]252742it [01:42, 3164.09it/s]254858it [01:42, 3042.94it/s]242009it [01:42, 3225.13it/s]255196it [01:42, 3137.04it/s]253062it [01:42, 2984.30it/s]242334it [01:43, 3069.06it/s]253409it [01:42, 3117.98it/s]255513it [01:43, 2980.53it/s]242689it [01:43, 3203.97it/s]253763it [01:42, 3237.34it/s]255869it [01:43, 3141.35it/s]243013it [01:43, 3016.65it/s]256210it [01:43, 3216.83it/s]254090it [01:43, 3021.35it/s]243370it [01:43, 3168.24it/s]254438it [01:43, 3147.17it/s]256535it [01:43, 3067.22it/s]243728it [01:43, 3283.43it/s]256893it [01:43, 3209.63it/s]254758it [01:43, 3011.16it/s]244060it [01:43, 3125.42it/s]255112it [01:43, 3158.35it/s]257217it [01:43, 3050.38it/s]244414it [01:43, 3241.82it/s]255465it [01:43, 3261.68it/s]257569it [01:43, 3181.42it/s]244742it [01:43, 3077.98it/s]257919it [01:43, 3272.19it/s]255795it [01:43, 3091.35it/s]245082it [01:43, 3166.73it/s]256150it [01:43, 3217.82it/s]258249it [01:43, 3076.94it/s]245426it [01:44, 3044.97it/s]258608it [01:44, 3218.26it/s]256476it [01:43, 3029.43it/s]245782it [01:44, 3186.33it/s]256832it [01:43, 3174.05it/s]258934it [01:44, 3072.30it/s]246130it [01:44, 3268.70it/s]257177it [01:44, 3250.45it/s]259285it [01:44, 3193.96it/s]246460it [01:44, 3104.20it/s]259642it [01:44, 3299.13it/s]257506it [01:44, 3077.69it/s]246796it [01:44, 3173.51it/s]257857it [01:44, 3196.59it/s]259975it [01:44, 3112.67it/s]247117it [01:44, 3046.92it/s]260333it [01:44, 3233.86it/s]258181it [01:44, 3054.46it/s]247468it [01:44, 3176.48it/s]258535it [01:44, 3188.19it/s]260660it [01:44, 3074.74it/s]247824it [01:44, 3285.02it/s]261019it [01:44, 3216.85it/s]258865it [01:44, 2921.75it/s]248155it [01:44, 3110.44it/s]261372it [01:44, 3305.34it/s]259219it [01:44, 3087.63it/s]248506it [01:45, 3213.11it/s]261706it [01:44, 3126.52it/s]259570it [01:44, 3203.75it/s]248831it [01:45, 3038.39it/s]262054it [01:45, 3224.06it/s]259896it [01:44, 3057.35it/s]249179it [01:45, 3159.00it/s]262380it [01:45, 3053.64it/s]260251it [01:45, 3193.83it/s]249533it [01:45, 3266.81it/s]262733it [01:45, 3184.70it/s]260575it [01:45, 3035.10it/s]249863it [01:45, 3096.23it/s]263063it [01:45, 3043.38it/s]260930it [01:45, 3176.64it/s]250219it [01:45, 3224.84it/s]263414it [01:45, 3171.20it/s]261268it [01:45, 3234.11it/s]250545it [01:45, 3091.22it/s]263773it [01:45, 3288.24it/s]261595it [01:45, 3078.67it/s]250886it [01:45, 3180.28it/s]261953it [01:45, 3218.81it/s]251241it [01:45, 3285.83it/s]262279it [01:45, 3062.31it/s]251573it [01:46, 3117.58it/s]262635it [01:45, 3199.82it/s]251921it [01:46, 3217.45it/s]262990it [01:45, 3297.42it/s]252246it [01:46, 3082.88it/s]263323it [01:46, 3122.88it/s]252580it [01:46, 3152.86it/s]263665it [01:46, 3203.95it/s]252934it [01:46, 3263.29it/s]253263it [01:46, 3104.25it/s]253619it [01:46, 3231.88it/s]253945it [01:46, 3098.95it/s]264280it [01:47, 149.23it/s] 254296it [01:46, 3213.36it/s]264654it [01:47, 214.08it/s]254632it [01:46, 3255.09it/s]264959it [01:47, 285.40it/s]254960it [01:47, 3102.24it/s]265337it [01:47, 406.57it/s]255313it [01:47, 3221.71it/s]265655it [01:47, 535.85it/s]266030it [01:47, 738.24it/s]255638it [01:47, 3091.05it/s]266406it [01:47, 987.70it/s]255986it [01:47, 3200.19it/s]256333it [01:47, 3277.63it/s]266749it [01:47, 1233.53it/s]267125it [01:47, 1560.02it/s]256663it [01:47, 3057.77it/s]267471it [01:47, 1831.59it/s]257012it [01:47, 3177.99it/s]267847it [01:48, 2179.25it/s]257334it [01:47, 3032.60it/s]268196it [01:48, 2377.33it/s]257686it [01:47, 3167.94it/s]268573it [01:48, 2685.41it/s]258026it [01:48, 3035.09it/s]268945it [01:48, 2932.21it/s]258365it [01:48, 3131.58it/s]269300it [01:48, 2988.78it/s]258721it [01:48, 3251.44it/s]269675it [01:48, 3185.10it/s]259050it [01:48, 3101.18it/s]270027it [01:48, 3179.84it/s]259397it [01:48, 3202.35it/s]270387it [01:48, 3294.21it/s]259721it [01:48, 3045.86it/s]270734it [01:48, 3244.39it/s]260079it [01:48, 3193.41it/s]271106it [01:49, 3375.72it/s]260412it [01:48, 3229.79it/s]271475it [01:49, 3464.52it/s]260738it [01:48, 3088.19it/s]271829it [01:49, 3357.91it/s]261093it [01:49, 3218.68it/s]272203it [01:49, 3465.87it/s]261418it [01:49, 3065.91it/s]272554it [01:49, 3349.72it/s]261780it [01:49, 3219.22it/s]272908it [01:49, 3401.76it/s]262138it [01:49, 3319.83it/s]273252it [01:49, 3291.14it/s]262473it [01:49, 3094.64it/s]273625it [01:49, 3414.91it/s]262825it [01:49, 3212.33it/s]273990it [01:49, 3482.56it/s]263151it [01:49, 3058.68it/s]274341it [01:49, 3343.06it/s]263506it [01:49, 3193.44it/s]274710it [01:50, 3440.47it/s]263863it [01:49, 3300.02it/s]275057it [01:50, 3276.07it/s]275431it [01:50, 3404.46it/s]275775it [01:50, 3296.52it/s]276148it [01:50, 3419.13it/s]276517it [01:50, 3496.75it/s]276869it [01:50, 3385.80it/s]277246it [01:50, 3493.81it/s]277598it [01:50, 3340.43it/s]277973it [01:51, 3456.34it/s]278321it [01:51, 3346.36it/s]278694it [01:51, 3454.85it/s]279071it [01:51, 3543.83it/s]279428it [01:51, 3376.66it/s]279798it [01:51, 3467.65it/s]280148it [01:51, 3354.25it/s]280520it [01:51, 3456.59it/s]280868it [01:51, 3366.13it/s]281237it [01:52, 3457.37it/s]281592it [01:52, 3483.62it/s]281942it [01:52, 3367.67it/s]282313it [01:52, 3465.76it/s]282662it [01:52, 3358.19it/s]283029it [01:52, 3447.00it/s]283376it [01:52, 3337.07it/s]283735it [01:52, 3407.96it/s]284108it [01:52, 3498.87it/s]284460it [01:52, 3351.52it/s]284833it [01:53, 3459.64it/s]285181it [01:53, 3347.89it/s]285541it [01:53, 3419.18it/s]285885it [01:53, 3324.72it/s]286256it [01:53, 3433.24it/s]286632it [01:53, 3525.61it/s]286986it [01:53, 3402.99it/s]287112it [01:53, 2524.60it/s]
264105it [01:54, 123.47it/s] 264469it [01:54, 176.89it/s]264811it [01:54, 244.66it/s]265162it [01:54, 340.39it/s]265528it [01:55, 473.81it/s]265847it [01:55, 616.88it/s]266206it [01:55, 829.40it/s]266529it [01:55, 1037.87it/s]266888it [01:55, 1328.03it/s]267247it [01:55, 1647.20it/s]267580it [01:55, 1877.90it/s]263989it [01:55, 116.06it/s] 267939it [01:55, 2201.61it/s]264356it [01:55, 167.74it/s]264709it [01:55, 236.17it/s]268270it [01:55, 2347.31it/s]268623it [01:56, 2613.62it/s]265004it [01:55, 311.80it/s]268978it [01:56, 2841.18it/s]265368it [01:55, 440.52it/s]265677it [01:56, 576.04it/s]269313it [01:56, 2804.82it/s]266039it [01:56, 786.70it/s]269672it [01:56, 3006.86it/s]266405it [01:56, 1045.07it/s]270001it [01:56, 2951.90it/s]266738it [01:56, 1280.03it/s]270360it [01:56, 3122.69it/s]267102it [01:56, 1604.53it/s]270691it [01:56, 3022.69it/s]267434it [01:56, 1819.71it/s]271049it [01:56, 3173.75it/s]267790it [01:56, 2140.88it/s]271398it [01:56, 3261.85it/s]268156it [01:56, 2457.63it/s]271732it [01:57, 3081.36it/s]268495it [01:56, 2561.14it/s]272086it [01:57, 3206.35it/s]268851it [01:57, 2798.54it/s]272413it [01:57, 3048.80it/s]269184it [01:57, 2799.76it/s]272764it [01:57, 3176.38it/s]269541it [01:57, 2997.14it/s]273117it [01:57, 3274.63it/s]269870it [01:57, 2901.56it/s]273449it [01:57, 3104.59it/s]270232it [01:57, 3091.21it/s]273805it [01:57, 3229.69it/s]270590it [01:57, 3225.41it/s]274132it [01:57, 3032.64it/s]270926it [01:57, 3085.41it/s]274488it [01:57, 3177.43it/s]271283it [01:57, 3217.91it/s]274844it [01:57, 3284.24it/s]271613it [01:57, 3072.84it/s]275177it [01:58, 3122.40it/s]271972it [01:58, 3215.86it/s]275535it [01:58, 3249.02it/s]272307it [01:58, 3253.68it/s]275864it [01:58, 3093.54it/s]272637it [01:58, 3104.09it/s]276221it [01:58, 3222.67it/s]272995it [01:58, 3236.10it/s]276567it [01:58, 3289.41it/s]273323it [01:58, 3080.71it/s]276899it [01:58, 3126.04it/s]273682it [01:58, 3220.86it/s]277254it [01:58, 3243.35it/s]274037it [01:58, 3314.11it/s]277582it [01:58, 3084.55it/s]274372it [01:58, 3151.23it/s]277941it [01:58, 3225.35it/s]274704it [01:58, 3198.62it/s]278267it [01:59, 3045.94it/s]275027it [01:59, 3068.55it/s]278626it [01:59, 3195.30it/s]275382it [01:59, 3203.44it/s]278983it [01:59, 3299.05it/s]275734it [01:59, 3063.99it/s]279317it [01:59, 3128.15it/s]276090it [01:59, 3199.35it/s]279673it [01:59, 3248.26it/s]276447it [01:59, 3303.10it/s]264197it [01:59, 114.21it/s] 280002it [01:59, 3070.89it/s]276781it [01:59, 3141.05it/s]264558it [01:59, 163.23it/s]280352it [01:59, 3187.67it/s]277121it [01:59, 3212.52it/s]264831it [01:59, 213.58it/s]280708it [01:59, 3292.58it/s]265195it [01:59, 307.07it/s]277446it [01:59, 3063.25it/s]281041it [01:59, 3138.63it/s]265559it [02:00, 432.08it/s]277804it [01:59, 3207.62it/s]281380it [02:00, 3208.76it/s]278154it [01:59, 3289.42it/s]265878it [02:00, 567.39it/s]281704it [02:00, 3046.91it/s]266237it [02:00, 770.38it/s]278486it [02:00, 3123.71it/s]282058it [02:00, 3182.76it/s]266558it [02:00, 973.60it/s]278842it [02:00, 3246.19it/s]282413it [02:00, 3285.23it/s]266921it [02:00, 1265.97it/s]279170it [02:00, 3054.92it/s]282745it [02:00, 3100.74it/s]267271it [02:00, 1569.29it/s]279528it [02:00, 3198.52it/s]283063it [02:00, 3122.22it/s]267604it [02:00, 1809.58it/s]279881it [02:00, 3291.57it/s]283378it [02:00, 2991.87it/s]267966it [02:00, 2143.84it/s]280214it [02:00, 3119.94it/s]283734it [02:00, 3150.40it/s]268299it [02:00, 2313.41it/s]280564it [02:00, 3222.32it/s]284082it [02:00, 3242.50it/s]268662it [02:01, 2606.92it/s]280890it [02:00, 3091.99it/s]284409it [02:01, 3099.38it/s]269011it [02:01, 2820.19it/s]281242it [02:00, 3210.47it/s]284759it [02:01, 3211.30it/s]269348it [02:01, 2824.56it/s]281595it [02:01, 3301.71it/s]285083it [02:01, 3065.60it/s]269709it [02:01, 3026.99it/s]281928it [02:01, 3105.90it/s]285414it [02:01, 3133.10it/s]270042it [02:01, 2974.37it/s]282281it [02:01, 3223.70it/s]285767it [02:01, 3246.06it/s]270397it [02:01, 3128.76it/s]282607it [02:01, 3077.77it/s]286094it [02:01, 3094.50it/s]270727it [02:01, 3028.93it/s]282960it [02:01, 3202.67it/s]286453it [02:01, 3233.40it/s]271089it [02:01, 3189.85it/s]283294it [02:01, 3060.71it/s]286780it [02:01, 3108.01it/s]271448it [02:01, 3301.00it/s]287112it [02:01, 2356.05it/s]
2022-07-05 16:41:48 | INFO | root | success load 287112 data
2022-07-05 16:41:48 | INFO | transformer.tokenization_utils | Model name '/data/yukangliang/预训练模型/bert-base-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming '/data/yukangliang/预训练模型/bert-base-uncased' is a path or url to a directory containing tokenizer files.
2022-07-05 16:41:48 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/added_tokens.json. We won't load it.
2022-07-05 16:41:48 | INFO | transformer.tokenization_utils | Didn't find file /data/yukangliang/预训练模型/bert-base-uncased/special_tokens_map.json. We won't load it.
2022-07-05 16:41:48 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/vocab.txt
2022-07-05 16:41:48 | INFO | transformer.tokenization_utils | loading file None
2022-07-05 16:41:48 | INFO | transformer.tokenization_utils | loading file None
2022-07-05 16:41:48 | INFO | transformer.tokenization_utils | loading file /data/yukangliang/预训练模型/bert-base-uncased/tokenizer_config.json
283643it [02:01, 3178.65it/s]271786it [02:02, 3150.36it/s]283995it [02:01, 3273.06it/s]272137it [02:02, 3249.67it/s]284326it [02:01, 3119.84it/s]272468it [02:02, 3102.16it/s]284675it [02:02, 3223.74it/s]272827it [02:02, 3237.29it/s]285001it [02:02, 3080.30it/s]273185it [02:02, 3332.30it/s]285355it [02:02, 3208.85it/s]273522it [02:02, 3144.17it/s]285702it [02:02, 3281.98it/s]273886it [02:02, 3281.01it/s]286033it [02:02, 3123.03it/s]274219it [02:02, 3126.58it/s]286393it [02:02, 3257.49it/s]274572it [02:02, 3238.14it/s]286722it [02:02, 3102.36it/s]274900it [02:02, 3077.69it/s]287085it [02:02, 3249.72it/s]287112it [02:02, 2337.92it/s]
275265it [02:03, 3234.94it/s]275623it [02:03, 3332.93it/s]275960it [02:03, 3170.48it/s]276318it [02:03, 3284.08it/s]276650it [02:03, 3111.37it/s]277013it [02:03, 3255.06it/s]277373it [02:03, 3350.91it/s]277712it [02:03, 3182.61it/s]278069it [02:03, 3289.94it/s]278402it [02:04, 3081.34it/s]278737it [02:04, 3153.50it/s]279085it [02:04, 3243.95it/s]279413it [02:04, 3105.66it/s]279764it [02:04, 3219.17it/s]280089it [02:04, 3088.13it/s]280449it [02:04, 3231.37it/s]280775it [02:04, 3104.00it/s]281136it [02:04, 3246.22it/s]281485it [02:05, 3315.86it/s]281819it [02:05, 3154.84it/s]282146it [02:05, 3186.29it/s]282467it [02:05, 2994.32it/s]282809it [02:05, 3111.01it/s]283166it [02:05, 3238.86it/s]283493it [02:05, 3104.03it/s]283854it [02:05, 3246.54it/s]284182it [02:05, 3085.46it/s]284540it [02:06, 3223.42it/s]284902it [02:06, 3336.60it/s]285239it [02:06, 3174.04it/s]285593it [02:06, 3275.69it/s]285924it [02:06, 3111.75it/s]286288it [02:06, 3257.51it/s]286653it [02:06, 3368.73it/s]286993it [02:06, 3214.52it/s]287112it [02:06, 2264.48it/s]
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq/optim/adam.py:179: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
2022-07-05 16:45:56 | INFO | train_inner | epoch 001:    100 / 1122 loss=nan, nll_loss=13.99, mask_ins=7.554, word_ins_ml=14.161, word_reposition=2.465, kpe=nan, ppl=nan, wps=8348.5, ups=0.42, wpb=19974.3, bsz=256, num_updates=100, lr=1.0098e-05, gnorm=20.104, clip=0, loss_scale=128, train_wall=206, wall=370
2022-07-05 16:49:57 | INFO | train_inner | epoch 001:    200 / 1122 loss=19.652, nll_loss=12.05, mask_ins=4.349, word_ins_ml=12.422, word_reposition=1.563, kpe=1.318, ppl=823829, wps=8266.8, ups=0.41, wpb=19964.4, bsz=256, num_updates=200, lr=2.0096e-05, gnorm=18.936, clip=0, loss_scale=128, train_wall=202, wall=611
2022-07-05 16:53:57 | INFO | train_inner | epoch 001:    300 / 1122 loss=nan, nll_loss=11.102, mask_ins=2.216, word_ins_ml=11.598, word_reposition=1.474, kpe=nan, ppl=nan, wps=8305.9, ups=0.42, wpb=19922, bsz=256, num_updates=300, lr=3.0094e-05, gnorm=3.98, clip=0, loss_scale=128, train_wall=201, wall=851
2022-07-05 16:57:56 | INFO | train_inner | epoch 001:    400 / 1122 loss=15.754, nll_loss=10.67, mask_ins=1.982, word_ins_ml=11.259, word_reposition=1.446, kpe=1.066, ppl=55263.7, wps=8337.7, ups=0.42, wpb=19941.1, bsz=256, num_updates=400, lr=4.0092e-05, gnorm=1.921, clip=0, loss_scale=128, train_wall=200, wall=1090
2022-07-05 17:01:57 | INFO | train_inner | epoch 001:    500 / 1122 loss=15.526, nll_loss=10.558, mask_ins=1.862, word_ins_ml=11.18, word_reposition=1.453, kpe=1.031, ppl=47172.3, wps=8306, ups=0.42, wpb=19970, bsz=256, num_updates=500, lr=5.009e-05, gnorm=1.856, clip=0, loss_scale=128, train_wall=200, wall=1331
2022-07-05 17:05:57 | INFO | train_inner | epoch 001:    600 / 1122 loss=15.421, nll_loss=10.516, mask_ins=1.834, word_ins_ml=11.148, word_reposition=1.436, kpe=1.004, ppl=43875.4, wps=8304.4, ups=0.42, wpb=19904.3, bsz=256, num_updates=600, lr=6.0088e-05, gnorm=1.587, clip=0, loss_scale=242, train_wall=200, wall=1570
2022-07-05 17:09:56 | INFO | train_inner | epoch 001:    700 / 1122 loss=15.356, nll_loss=10.462, mask_ins=1.835, word_ins_ml=11.102, word_reposition=1.427, kpe=0.991, ppl=41931.1, wps=8341.9, ups=0.42, wpb=19941.3, bsz=256, num_updates=700, lr=7.0086e-05, gnorm=1.733, clip=0, loss_scale=256, train_wall=200, wall=1809
2022-07-05 17:14:11 | INFO | train_inner | epoch 001:    800 / 1122 loss=15.271, nll_loss=10.39, mask_ins=1.834, word_ins_ml=11.039, word_reposition=1.421, kpe=0.977, ppl=39532.8, wps=7807.4, ups=0.39, wpb=19916.1, bsz=256, num_updates=800, lr=8.0084e-05, gnorm=1.707, clip=0, loss_scale=256, train_wall=215, wall=2064
2022-07-05 17:18:10 | INFO | train_inner | epoch 001:    900 / 1122 loss=15.202, nll_loss=10.329, mask_ins=1.832, word_ins_ml=10.987, word_reposition=1.418, kpe=0.964, ppl=37697.2, wps=8275.7, ups=0.42, wpb=19826.8, bsz=256, num_updates=900, lr=9.0082e-05, gnorm=1.548, clip=0, loss_scale=256, train_wall=200, wall=2304
2022-07-05 17:22:09 | INFO | train_inner | epoch 001:   1000 / 1122 loss=15.119, nll_loss=10.243, mask_ins=1.844, word_ins_ml=10.913, word_reposition=1.409, kpe=0.952, ppl=35576.4, wps=8371.4, ups=0.42, wpb=19960.1, bsz=256, num_updates=1000, lr=0.00010008, gnorm=1.706, clip=0, loss_scale=256, train_wall=199, wall=2542
2022-07-05 17:26:08 | INFO | train_inner | epoch 001:   1100 / 1122 loss=15.031, nll_loss=10.179, mask_ins=1.831, word_ins_ml=10.86, word_reposition=1.403, kpe=0.937, ppl=33485.2, wps=8326.8, ups=0.42, wpb=19883.5, bsz=256, num_updates=1100, lr=0.000110078, gnorm=1.622, clip=0, loss_scale=453, train_wall=199, wall=2781
2022-07-05 17:26:59 | INFO | train | epoch 001 | loss nan | nll_loss 10.938 | mask_ins 2.619 | word_ins_ml 11.502 | word_reposition 1.535 | kpe nan | ppl nan | wps 8266.9 | ups 0.42 | wpb 19912.5 | bsz 255.8 | num_updates 1122 | lr 0.000112278 | gnorm 5.103 | clip 0 | loss_scale 220 | train_wall 2266 | wall 2833
2022-07-05 17:28:06 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 16.178 | nll_loss 10.25 | mask_ins 2.511 | word_ins_ml 10.931 | word_reposition 1.31 | kpe 1.426 | ppl 74159.9 | wps 14328.2 | wpb 2279.4 | bsz 32 | num_updates 1122
2022-07-05 17:28:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 1 @ 1122 updates, score 16.178) (writing took 6.251785086467862 seconds)
2022-07-05 17:31:18 | INFO | train_inner | epoch 002:     78 / 1122 loss=14.95, nll_loss=10.103, mask_ins=1.84, word_ins_ml=10.795, word_reposition=1.398, kpe=0.916, ppl=31640.9, wps=6359.7, ups=0.32, wpb=19740.9, bsz=253.8, num_updates=1200, lr=0.000120076, gnorm=1.81, clip=0, loss_scale=512, train_wall=198, wall=3092
2022-07-05 17:35:17 | INFO | train_inner | epoch 002:    178 / 1122 loss=14.892, nll_loss=10.039, mask_ins=1.846, word_ins_ml=10.741, word_reposition=1.399, kpe=0.906, ppl=30404.9, wps=8366.1, ups=0.42, wpb=19979.9, bsz=256, num_updates=1300, lr=0.000130074, gnorm=1.552, clip=0, loss_scale=512, train_wall=200, wall=3330
2022-07-05 17:39:16 | INFO | train_inner | epoch 002:    278 / 1122 loss=14.81, nll_loss=9.981, mask_ins=1.814, word_ins_ml=10.692, word_reposition=1.405, kpe=0.899, ppl=28728.3, wps=8359.4, ups=0.42, wpb=19990.6, bsz=256, num_updates=1400, lr=0.000140072, gnorm=1.598, clip=0, loss_scale=512, train_wall=200, wall=3570
2022-07-05 17:43:15 | INFO | train_inner | epoch 002:    378 / 1122 loss=14.773, nll_loss=9.939, mask_ins=1.828, word_ins_ml=10.657, word_reposition=1.386, kpe=0.902, ppl=28002.9, wps=8261.6, ups=0.42, wpb=19731.5, bsz=256, num_updates=1500, lr=0.00015007, gnorm=1.731, clip=0, loss_scale=512, train_wall=200, wall=3808
2022-07-05 17:47:13 | INFO | train_inner | epoch 002:    478 / 1122 loss=14.72, nll_loss=9.883, mask_ins=1.825, word_ins_ml=10.609, word_reposition=1.387, kpe=0.899, ppl=26992, wps=8368.5, ups=0.42, wpb=19935.8, bsz=256, num_updates=1600, lr=0.000160068, gnorm=1.599, clip=0, loss_scale=845, train_wall=199, wall=4047
2022-07-05 17:51:11 | INFO | train_inner | epoch 002:    578 / 1122 loss=14.677, nll_loss=9.846, mask_ins=1.817, word_ins_ml=10.578, word_reposition=1.381, kpe=0.9, ppl=26193, wps=8372.6, ups=0.42, wpb=19940.7, bsz=256, num_updates=1700, lr=0.000170066, gnorm=1.538, clip=0, loss_scale=1024, train_wall=199, wall=4285
2022-07-05 17:55:09 | INFO | train_inner | epoch 002:    678 / 1122 loss=nan, nll_loss=9.793, mask_ins=1.829, word_ins_ml=10.534, word_reposition=1.384, kpe=nan, ppl=nan, wps=8355.9, ups=0.42, wpb=19899.1, bsz=256, num_updates=1800, lr=0.000180064, gnorm=1.599, clip=0, loss_scale=1024, train_wall=199, wall=4523
2022-07-05 17:59:07 | INFO | train_inner | epoch 002:    778 / 1122 loss=nan, nll_loss=9.755, mask_ins=1.828, word_ins_ml=10.501, word_reposition=1.389, kpe=nan, ppl=nan, wps=8418.2, ups=0.42, wpb=19987.5, bsz=256, num_updates=1900, lr=0.000190062, gnorm=1.619, clip=0, loss_scale=1024, train_wall=198, wall=4760
2022-07-05 18:03:05 | INFO | train_inner | epoch 002:    878 / 1122 loss=14.563, nll_loss=9.716, mask_ins=1.826, word_ins_ml=10.468, word_reposition=1.376, kpe=0.892, ppl=24206, wps=8337.1, ups=0.42, wpb=19828.7, bsz=256, num_updates=2000, lr=0.00020006, gnorm=1.613, clip=0, loss_scale=1024, train_wall=199, wall=4998
2022-07-05 18:07:36 | INFO | train_inner | epoch 002:    978 / 1122 loss=14.514, nll_loss=9.683, mask_ins=1.813, word_ins_ml=10.44, word_reposition=1.369, kpe=0.892, ppl=23399.4, wps=7329.6, ups=0.37, wpb=19915.7, bsz=256, num_updates=2100, lr=0.000210058, gnorm=1.506, clip=0, loss_scale=1567, train_wall=232, wall=5270
2022-07-05 18:11:34 | INFO | train_inner | epoch 002:   1078 / 1122 loss=14.503, nll_loss=9.65, mask_ins=1.824, word_ins_ml=10.411, word_reposition=1.374, kpe=0.893, ppl=23225.7, wps=8442.5, ups=0.42, wpb=20105.2, bsz=256, num_updates=2200, lr=0.000220056, gnorm=1.531, clip=0, loss_scale=2048, train_wall=199, wall=5508
2022-07-05 18:13:18 | INFO | train | epoch 002 | loss nan | nll_loss 9.839 | mask_ins 1.827 | word_ins_ml 10.572 | word_reposition 1.385 | kpe nan | ppl nan | wps 8039.5 | ups 0.4 | wpb 19913.7 | bsz 255.8 | num_updates 2244 | lr 0.000224455 | gnorm 1.59 | clip 0 | loss_scale 1015 | train_wall 2265 | wall 5612
2022-07-05 18:14:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 15.521 | nll_loss 9.828 | mask_ins 2.276 | word_ins_ml 10.578 | word_reposition 1.266 | kpe 1.401 | ppl 47028.9 | wps 14357.2 | wpb 2279.4 | bsz 32 | num_updates 2244 | best_loss 15.521
2022-07-05 18:14:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 2 @ 2244 updates, score 15.521) (writing took 8.802015332505107 seconds)
2022-07-05 18:16:46 | INFO | train_inner | epoch 003:     56 / 1122 loss=nan, nll_loss=9.592, mask_ins=1.82, word_ins_ml=10.361, word_reposition=1.372, kpe=nan, ppl=nan, wps=6337.8, ups=0.32, wpb=19763.5, bsz=253.8, num_updates=2300, lr=0.000230054, gnorm=1.621, clip=0, loss_scale=2048, train_wall=198, wall=5820
2022-07-05 18:20:44 | INFO | train_inner | epoch 003:    156 / 1122 loss=14.327, nll_loss=9.529, mask_ins=1.817, word_ins_ml=10.307, word_reposition=1.367, kpe=0.836, ppl=20545.7, wps=8340.2, ups=0.42, wpb=19849.7, bsz=256, num_updates=2400, lr=0.000240052, gnorm=1.634, clip=0, loss_scale=2048, train_wall=199, wall=6058
2022-07-05 18:24:41 | INFO | train_inner | epoch 003:    256 / 1122 loss=14.316, nll_loss=9.508, mask_ins=1.813, word_ins_ml=10.29, word_reposition=1.372, kpe=0.841, ppl=20390.7, wps=8457.7, ups=0.42, wpb=20010.3, bsz=256, num_updates=2500, lr=0.00025005, gnorm=1.61, clip=0, loss_scale=2048, train_wall=198, wall=6295
2022-07-05 18:28:38 | INFO | train_inner | epoch 003:    356 / 1122 loss=14.298, nll_loss=9.481, mask_ins=1.816, word_ins_ml=10.266, word_reposition=1.37, kpe=0.845, ppl=20138.4, wps=8409.5, ups=0.42, wpb=19973.9, bsz=256, num_updates=2600, lr=0.000260048, gnorm=1.661, clip=0, loss_scale=2888, train_wall=199, wall=6532
2022-07-05 18:32:35 | INFO | train_inner | epoch 003:    456 / 1122 loss=14.28, nll_loss=9.415, mask_ins=1.811, word_ins_ml=10.208, word_reposition=1.406, kpe=0.855, ppl=19898.1, wps=8343.1, ups=0.42, wpb=19759, bsz=256, num_updates=2700, lr=0.000270046, gnorm=1.609, clip=0, loss_scale=4096, train_wall=198, wall=6769
2022-07-05 18:36:33 | INFO | train_inner | epoch 003:    556 / 1122 loss=14.206, nll_loss=9.274, mask_ins=1.821, word_ins_ml=10.083, word_reposition=1.444, kpe=0.858, ppl=18898.6, wps=8363.6, ups=0.42, wpb=19874.3, bsz=256, num_updates=2800, lr=0.000280044, gnorm=1.878, clip=0, loss_scale=4096, train_wall=198, wall=7006
2022-07-05 18:40:30 | INFO | train_inner | epoch 003:    656 / 1122 loss=14.169, nll_loss=9.175, mask_ins=1.826, word_ins_ml=9.996, word_reposition=1.482, kpe=0.865, ppl=18415.2, wps=8436.2, ups=0.42, wpb=19975.5, bsz=256, num_updates=2900, lr=0.000290042, gnorm=2.281, clip=0, loss_scale=4096, train_wall=198, wall=7243
2022-07-05 18:44:27 | INFO | train_inner | epoch 003:    756 / 1122 loss=14.094, nll_loss=9.081, mask_ins=1.822, word_ins_ml=9.914, word_reposition=1.488, kpe=0.871, ppl=17486.7, wps=8432.5, ups=0.42, wpb=20009.2, bsz=256, num_updates=3000, lr=0.00030004, gnorm=2.652, clip=0, loss_scale=4096, train_wall=198, wall=7481
2022-07-05 18:47:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-05 18:48:26 | INFO | train_inner | epoch 003:    857 / 1122 loss=nan, nll_loss=9.008, mask_ins=1.822, word_ins_ml=9.85, word_reposition=1.502, kpe=nan, ppl=nan, wps=8354, ups=0.42, wpb=19966.7, bsz=256, num_updates=3100, lr=0.000310038, gnorm=3.146, clip=0, loss_scale=4339, train_wall=200, wall=7720
2022-07-05 18:52:24 | INFO | train_inner | epoch 003:    957 / 1122 loss=13.981, nll_loss=8.931, mask_ins=1.814, word_ins_ml=9.783, word_reposition=1.509, kpe=0.876, ppl=16172.6, wps=8392.8, ups=0.42, wpb=19944.3, bsz=256, num_updates=3200, lr=0.000320036, gnorm=3.143, clip=0, loss_scale=4096, train_wall=199, wall=7957
2022-07-05 18:56:21 | INFO | train_inner | epoch 003:   1057 / 1122 loss=13.943, nll_loss=8.871, mask_ins=1.821, word_ins_ml=9.731, word_reposition=1.514, kpe=0.877, ppl=15745.3, wps=8386.9, ups=0.42, wpb=19933.5, bsz=256, num_updates=3300, lr=0.000330034, gnorm=3.043, clip=0, loss_scale=4096, train_wall=198, wall=8195
2022-07-05 18:59:34 | INFO | train | epoch 003 | loss nan | nll_loss 9.222 | mask_ins 1.818 | word_ins_ml 10.038 | word_reposition 1.446 | kpe nan | ppl nan | wps 8043.8 | ups 0.4 | wpb 19913.8 | bsz 255.8 | num_updates 3365 | lr 0.000336533 | gnorm 2.295 | clip 0 | loss_scale 3543 | train_wall 2263 | wall 8387
2022-07-05 19:00:40 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 15.634 | nll_loss 9.568 | mask_ins 2.081 | word_ins_ml 10.374 | word_reposition 1.788 | kpe 1.392 | ppl 50854.1 | wps 14359.8 | wpb 2279.4 | bsz 32 | num_updates 3365 | best_loss 15.521
2022-07-05 19:00:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 3 @ 3365 updates, score 15.634) (writing took 5.561408358626068 seconds)
2022-07-05 19:02:08 | INFO | train_inner | epoch 004:     35 / 1122 loss=13.86, nll_loss=8.794, mask_ins=1.817, word_ins_ml=9.664, word_reposition=1.516, kpe=0.863, ppl=14864.9, wps=5697.1, ups=0.29, wpb=19768.2, bsz=253.8, num_updates=3400, lr=0.000340032, gnorm=3.174, clip=0, loss_scale=4096, train_wall=236, wall=8542
2022-07-05 19:06:06 | INFO | train_inner | epoch 004:    135 / 1122 loss=13.716, nll_loss=8.644, mask_ins=1.832, word_ins_ml=9.532, word_reposition=1.534, kpe=0.818, ppl=13454, wps=8374, ups=0.42, wpb=19942.7, bsz=256, num_updates=3500, lr=0.00035003, gnorm=3.045, clip=0, loss_scale=4096, train_wall=199, wall=8780
2022-07-05 19:10:04 | INFO | train_inner | epoch 004:    235 / 1122 loss=13.648, nll_loss=8.589, mask_ins=1.802, word_ins_ml=9.485, word_reposition=1.537, kpe=0.825, ppl=12835.5, wps=8437.7, ups=0.42, wpb=20036.6, bsz=256, num_updates=3600, lr=0.000360028, gnorm=2.66, clip=0, loss_scale=4588, train_wall=198, wall=9018
2022-07-05 19:14:01 | INFO | train_inner | epoch 004:    335 / 1122 loss=13.67, nll_loss=8.554, mask_ins=1.831, word_ins_ml=9.455, word_reposition=1.548, kpe=0.836, ppl=13030.6, wps=8357.2, ups=0.42, wpb=19802.6, bsz=256, num_updates=3700, lr=0.000370026, gnorm=2.654, clip=0, loss_scale=8192, train_wall=198, wall=9254
2022-07-05 19:17:58 | INFO | train_inner | epoch 004:    435 / 1122 loss=13.61, nll_loss=8.511, mask_ins=1.814, word_ins_ml=9.419, word_reposition=1.535, kpe=0.842, ppl=12507.4, wps=8421.8, ups=0.42, wpb=20001, bsz=256, num_updates=3800, lr=0.000380024, gnorm=2.61, clip=0, loss_scale=8192, train_wall=199, wall=9492
2022-07-05 19:21:56 | INFO | train_inner | epoch 004:    535 / 1122 loss=13.586, nll_loss=8.459, mask_ins=1.809, word_ins_ml=9.374, word_reposition=1.55, kpe=0.852, ppl=12295.3, wps=8373.7, ups=0.42, wpb=19876.8, bsz=256, num_updates=3900, lr=0.000390022, gnorm=2.63, clip=0, loss_scale=8192, train_wall=198, wall=9729
2022-07-05 19:25:53 | INFO | train_inner | epoch 004:    635 / 1122 loss=nan, nll_loss=8.413, mask_ins=1.81, word_ins_ml=9.333, word_reposition=1.557, kpe=nan, ppl=nan, wps=8377.2, ups=0.42, wpb=19890.8, bsz=256, num_updates=4000, lr=0.00040002, gnorm=2.637, clip=0, loss_scale=8192, train_wall=199, wall=9967
2022-07-05 19:29:50 | INFO | train_inner | epoch 004:    735 / 1122 loss=13.52, nll_loss=8.352, mask_ins=1.809, word_ins_ml=9.28, word_reposition=1.568, kpe=0.863, ppl=11748, wps=8471, ups=0.42, wpb=20081.6, bsz=256, num_updates=4100, lr=0.000410018, gnorm=2.825, clip=0, loss_scale=8192, train_wall=198, wall=10204
2022-07-05 19:29:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2022-07-05 19:33:49 | INFO | train_inner | epoch 004:    836 / 1122 loss=nan, nll_loss=8.277, mask_ins=1.8, word_ins_ml=9.214, word_reposition=1.555, kpe=nan, ppl=nan, wps=8310.8, ups=0.42, wpb=19852.9, bsz=256, num_updates=4200, lr=0.000420016, gnorm=3.064, clip=0, loss_scale=8354, train_wall=199, wall=10443
2022-07-05 19:37:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-05 19:37:49 | INFO | train_inner | epoch 004:    937 / 1122 loss=13.407, nll_loss=8.16, mask_ins=1.82, word_ins_ml=9.112, word_reposition=1.592, kpe=0.883, ppl=10861.7, wps=8328.5, ups=0.42, wpb=20001.3, bsz=256, num_updates=4300, lr=0.000430014, gnorm=3.133, clip=0, loss_scale=7503, train_wall=201, wall=10683
2022-07-05 19:41:46 | INFO | train_inner | epoch 004:   1037 / 1122 loss=13.21, nll_loss=7.965, mask_ins=1.795, word_ins_ml=8.941, word_reposition=1.587, kpe=0.888, ppl=9475.14, wps=8358.4, ups=0.42, wpb=19822.9, bsz=256, num_updates=4400, lr=0.000440012, gnorm=3.172, clip=0, loss_scale=4096, train_wall=198, wall=10920
2022-07-05 19:45:07 | INFO | train | epoch 004 | loss nan | nll_loss 8.356 | mask_ins 1.812 | word_ins_ml 9.283 | word_reposition 1.56 | kpe nan | ppl nan | wps 8159.1 | ups 0.41 | wpb 19913.3 | bsz 255.8 | num_updates 4485 | lr 0.00044851 | gnorm 2.902 | clip 0 | loss_scale 6655 | train_wall 2224 | wall 11121
2022-07-05 19:46:13 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 15.355 | nll_loss 9.077 | mask_ins 2.042 | word_ins_ml 9.993 | word_reposition 1.752 | kpe 1.568 | ppl 41902 | wps 14376.2 | wpb 2279.4 | bsz 32 | num_updates 4485 | best_loss 15.355
2022-07-05 19:46:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 4 @ 4485 updates, score 15.355) (writing took 9.10729747172445 seconds)
2022-07-05 19:46:58 | INFO | train_inner | epoch 005:     15 / 1122 loss=13.095, nll_loss=7.762, mask_ins=1.812, word_ins_ml=8.763, word_reposition=1.623, kpe=0.896, ppl=8750.19, wps=6337.8, ups=0.32, wpb=19726, bsz=253.8, num_updates=4500, lr=0.00045001, gnorm=3.655, clip=0, loss_scale=4096, train_wall=197, wall=11231
2022-07-05 19:50:54 | INFO | train_inner | epoch 005:    115 / 1122 loss=12.703, nll_loss=7.409, mask_ins=1.8, word_ins_ml=8.454, word_reposition=1.602, kpe=0.847, ppl=6669.77, wps=8477.3, ups=0.42, wpb=20012.6, bsz=256, num_updates=4600, lr=0.000460008, gnorm=3.579, clip=0, loss_scale=4096, train_wall=197, wall=11467
2022-07-05 19:55:28 | INFO | train_inner | epoch 005:    215 / 1122 loss=12.576, nll_loss=7.235, mask_ins=1.796, word_ins_ml=8.304, word_reposition=1.618, kpe=0.858, ppl=6105.33, wps=7258.9, ups=0.36, wpb=19922.4, bsz=256, num_updates=4700, lr=0.000470006, gnorm=4.054, clip=0, loss_scale=4096, train_wall=235, wall=11742
2022-07-05 19:59:25 | INFO | train_inner | epoch 005:    315 / 1122 loss=12.371, nll_loss=7.003, mask_ins=1.797, word_ins_ml=8.102, word_reposition=1.602, kpe=0.869, ppl=5295.78, wps=8434.9, ups=0.42, wpb=19989.4, bsz=256, num_updates=4800, lr=0.000480004, gnorm=3.815, clip=0, loss_scale=4301, train_wall=198, wall=11979
2022-07-05 20:01:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2022-07-05 20:03:25 | INFO | train_inner | epoch 005:    416 / 1122 loss=12.176, nll_loss=6.781, mask_ins=1.785, word_ins_ml=7.909, word_reposition=1.605, kpe=0.877, ppl=4625.88, wps=8303.1, ups=0.42, wpb=19907.8, bsz=256, num_updates=4900, lr=0.000490002, gnorm=3.819, clip=0, loss_scale=6448, train_wall=200, wall=12219
2022-07-05 20:03:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-05 20:07:24 | INFO | train_inner | epoch 005:    517 / 1122 loss=nan, nll_loss=6.509, mask_ins=1.781, word_ins_ml=7.674, word_reposition=1.59, kpe=nan, ppl=nan, wps=8275, ups=0.42, wpb=19759.5, bsz=256, num_updates=5000, lr=0.0005, gnorm=4.032, clip=0, loss_scale=2210, train_wall=199, wall=12457
2022-07-05 20:11:21 | INFO | train_inner | epoch 005:    617 / 1122 loss=11.657, nll_loss=6.237, mask_ins=1.747, word_ins_ml=7.438, word_reposition=1.575, kpe=0.898, ppl=3229.61, wps=8392.1, ups=0.42, wpb=19878.6, bsz=256, num_updates=5100, lr=0.000495074, gnorm=4.173, clip=0, loss_scale=2048, train_wall=198, wall=12694
2022-07-05 20:15:18 | INFO | train_inner | epoch 005:    717 / 1122 loss=11.392, nll_loss=5.919, mask_ins=1.773, word_ins_ml=7.16, word_reposition=1.557, kpe=0.902, ppl=2687.94, wps=8387.8, ups=0.42, wpb=19904.7, bsz=256, num_updates=5200, lr=0.00049029, gnorm=3.979, clip=0, loss_scale=2048, train_wall=198, wall=12932
2022-07-05 20:19:15 | INFO | train_inner | epoch 005:    817 / 1122 loss=11.067, nll_loss=5.601, mask_ins=1.741, word_ins_ml=6.885, word_reposition=1.538, kpe=0.903, ppl=2145.5, wps=8446.5, ups=0.42, wpb=20018.7, bsz=256, num_updates=5300, lr=0.000485643, gnorm=4.063, clip=0, loss_scale=2048, train_wall=198, wall=13169
2022-07-05 20:22:34 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 20:23:14 | INFO | train_inner | epoch 005:    918 / 1122 loss=10.808, nll_loss=5.35, mask_ins=1.716, word_ins_ml=6.667, word_reposition=1.508, kpe=0.916, ppl=1792.44, wps=8336.9, ups=0.42, wpb=19942.4, bsz=256, num_updates=5400, lr=0.000481125, gnorm=4.078, clip=0, loss_scale=1866, train_wall=200, wall=13408
2022-07-05 20:27:11 | INFO | train_inner | epoch 005:   1018 / 1122 loss=nan, nll_loss=5.088, mask_ins=1.671, word_ins_ml=6.44, word_reposition=1.481, kpe=nan, ppl=nan, wps=8375.4, ups=0.42, wpb=19826.1, bsz=256, num_updates=5500, lr=0.000476731, gnorm=4.133, clip=0, loss_scale=1024, train_wall=198, wall=13645
2022-07-05 20:31:08 | INFO | train_inner | epoch 005:   1118 / 1122 loss=10.26, nll_loss=4.861, mask_ins=1.64, word_ins_ml=6.242, word_reposition=1.456, kpe=0.921, ppl=1226.09, wps=8448.3, ups=0.42, wpb=20030.2, bsz=256, num_updates=5600, lr=0.000472456, gnorm=4.103, clip=0, loss_scale=1024, train_wall=198, wall=13882
2022-07-05 20:31:17 | INFO | train | epoch 005 | loss nan | nll_loss 6.197 | mask_ins 1.75 | word_ins_ml 7.403 | word_reposition 1.558 | kpe nan | ppl nan | wps 8045.4 | ups 0.4 | wpb 19913.4 | bsz 255.8 | num_updates 5604 | lr 0.000472287 | gnorm 4.002 | clip 0 | loss_scale 2849 | train_wall 2257 | wall 13890
2022-07-05 20:32:23 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 13.323 | nll_loss 7.325 | mask_ins 1.848 | word_ins_ml 8.486 | word_reposition 1.642 | kpe 1.347 | ppl 10248.1 | wps 14325.8 | wpb 2279.4 | bsz 32 | num_updates 5604 | best_loss 13.323
2022-07-05 20:32:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 5 @ 5604 updates, score 13.323) (writing took 8.887907080352306 seconds)
2022-07-05 20:36:20 | INFO | train_inner | epoch 006:     96 / 1122 loss=9.937, nll_loss=4.618, mask_ins=1.596, word_ins_ml=6.03, word_reposition=1.425, kpe=0.886, ppl=980.12, wps=6319.5, ups=0.32, wpb=19734.8, bsz=253.8, num_updates=5700, lr=0.000468293, gnorm=4.437, clip=0, loss_scale=1024, train_wall=198, wall=14194
2022-07-05 20:40:18 | INFO | train_inner | epoch 006:    196 / 1122 loss=9.599, nll_loss=4.379, mask_ins=1.515, word_ins_ml=5.822, word_reposition=1.379, kpe=0.883, ppl=775.41, wps=8417.5, ups=0.42, wpb=19981.3, bsz=256, num_updates=5800, lr=0.000464238, gnorm=4.162, clip=0, loss_scale=1024, train_wall=199, wall=14431
2022-07-05 20:43:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-05 20:44:17 | INFO | train_inner | epoch 006:    297 / 1122 loss=9.354, nll_loss=4.181, mask_ins=1.46, word_ins_ml=5.648, word_reposition=1.358, kpe=0.888, ppl=654.53, wps=8340.3, ups=0.42, wpb=19958.8, bsz=256, num_updates=5900, lr=0.000460287, gnorm=4.165, clip=0, loss_scale=963, train_wall=200, wall=14671
2022-07-05 20:48:52 | INFO | train_inner | epoch 006:    397 / 1122 loss=9.114, nll_loss=4.001, mask_ins=1.401, word_ins_ml=5.49, word_reposition=1.334, kpe=0.889, ppl=554.01, wps=7264.8, ups=0.36, wpb=19950.1, bsz=256, num_updates=6000, lr=0.000456435, gnorm=4.064, clip=0, loss_scale=512, train_wall=235, wall=14945
2022-07-05 20:52:48 | INFO | train_inner | epoch 006:    497 / 1122 loss=nan, nll_loss=3.886, mask_ins=1.359, word_ins_ml=5.388, word_reposition=1.291, kpe=nan, ppl=nan, wps=8408, ups=0.42, wpb=19909.2, bsz=256, num_updates=6100, lr=0.000452679, gnorm=4.054, clip=0, loss_scale=512, train_wall=198, wall=15182
2022-07-05 20:56:45 | INFO | train_inner | epoch 006:    597 / 1122 loss=8.692, nll_loss=3.707, mask_ins=1.311, word_ins_ml=5.23, word_reposition=1.258, kpe=0.893, ppl=413.62, wps=8364.2, ups=0.42, wpb=19778.1, bsz=256, num_updates=6200, lr=0.000449013, gnorm=4.039, clip=0, loss_scale=512, train_wall=197, wall=15419
2022-07-05 21:00:42 | INFO | train_inner | epoch 006:    697 / 1122 loss=8.602, nll_loss=3.65, mask_ins=1.28, word_ins_ml=5.178, word_reposition=1.24, kpe=0.903, ppl=388.43, wps=8459.1, ups=0.42, wpb=20041.9, bsz=256, num_updates=6300, lr=0.000445435, gnorm=4.097, clip=0, loss_scale=512, train_wall=198, wall=15655
2022-07-05 21:04:39 | INFO | train_inner | epoch 006:    797 / 1122 loss=nan, nll_loss=3.553, mask_ins=1.249, word_ins_ml=5.091, word_reposition=1.213, kpe=nan, ppl=nan, wps=8377.6, ups=0.42, wpb=19881.4, bsz=256, num_updates=6400, lr=0.000441942, gnorm=3.943, clip=0, loss_scale=512, train_wall=198, wall=15893
2022-07-05 21:08:36 | INFO | train_inner | epoch 006:    897 / 1122 loss=8.3, nll_loss=3.448, mask_ins=1.228, word_ins_ml=4.998, word_reposition=1.171, kpe=0.904, ppl=315.18, wps=8445.8, ups=0.42, wpb=20038.7, bsz=256, num_updates=6500, lr=0.000438529, gnorm=3.814, clip=0, loss_scale=1024, train_wall=198, wall=16130
2022-07-05 21:12:33 | INFO | train_inner | epoch 006:    997 / 1122 loss=8.227, nll_loss=3.405, mask_ins=1.202, word_ins_ml=4.959, word_reposition=1.161, kpe=0.906, ppl=299.64, wps=8407.2, ups=0.42, wpb=19917.3, bsz=256, num_updates=6600, lr=0.000435194, gnorm=3.588, clip=0, loss_scale=1024, train_wall=198, wall=16367
2022-07-05 21:16:30 | INFO | train_inner | epoch 006:   1097 / 1122 loss=8.084, nll_loss=3.325, mask_ins=1.17, word_ins_ml=4.887, word_reposition=1.128, kpe=0.899, ppl=271.29, wps=8400.6, ups=0.42, wpb=19889.3, bsz=256, num_updates=6700, lr=0.000431934, gnorm=3.563, clip=0, loss_scale=1024, train_wall=198, wall=16604
2022-07-05 21:17:29 | INFO | train | epoch 006 | loss nan | nll_loss 3.817 | mask_ins 1.338 | word_ins_ml 5.325 | word_reposition 1.265 | kpe nan | ppl nan | wps 8052.8 | ups 0.4 | wpb 19913.4 | bsz 255.8 | num_updates 6725 | lr 0.000431131 | gnorm 3.972 | clip 0 | loss_scale 790 | train_wall 2259 | wall 16662
2022-07-05 21:18:35 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 12.554 | nll_loss 6.522 | mask_ins 1.805 | word_ins_ml 7.795 | word_reposition 1.542 | kpe 1.413 | ppl 6014.24 | wps 14388.3 | wpb 2279.4 | bsz 32 | num_updates 6725 | best_loss 12.554
2022-07-05 21:18:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 6 @ 6725 updates, score 12.554) (writing took 9.427634899504483 seconds)
2022-07-05 21:21:42 | INFO | train_inner | epoch 007:     75 / 1122 loss=7.977, nll_loss=3.255, mask_ins=1.164, word_ins_ml=4.825, word_reposition=1.109, kpe=0.878, ppl=251.88, wps=6299.1, ups=0.32, wpb=19672.8, bsz=253.8, num_updates=6800, lr=0.000428746, gnorm=3.714, clip=0, loss_scale=1024, train_wall=198, wall=16916
2022-07-05 21:25:40 | INFO | train_inner | epoch 007:    175 / 1122 loss=7.869, nll_loss=3.197, mask_ins=1.142, word_ins_ml=4.772, word_reposition=1.093, kpe=0.863, ppl=233.83, wps=8398.2, ups=0.42, wpb=19963.6, bsz=256, num_updates=6900, lr=0.000425628, gnorm=3.575, clip=0, loss_scale=1024, train_wall=199, wall=17154
2022-07-05 21:29:38 | INFO | train_inner | epoch 007:    275 / 1122 loss=7.78, nll_loss=3.133, mask_ins=1.124, word_ins_ml=4.714, word_reposition=1.081, kpe=0.861, ppl=219.82, wps=8429.2, ups=0.42, wpb=20038.3, bsz=256, num_updates=7000, lr=0.000422577, gnorm=3.489, clip=0, loss_scale=1925, train_wall=198, wall=17391
2022-07-05 21:33:36 | INFO | train_inner | epoch 007:    375 / 1122 loss=7.712, nll_loss=3.089, mask_ins=1.108, word_ins_ml=4.674, word_reposition=1.06, kpe=0.869, ppl=209.67, wps=8364.8, ups=0.42, wpb=19910, bsz=256, num_updates=7100, lr=0.000419591, gnorm=3.546, clip=0, loss_scale=2048, train_wall=199, wall=17629
2022-07-05 21:37:33 | INFO | train_inner | epoch 007:    475 / 1122 loss=7.671, nll_loss=3.062, mask_ins=1.094, word_ins_ml=4.649, word_reposition=1.051, kpe=0.876, ppl=203.76, wps=8394.5, ups=0.42, wpb=19903.4, bsz=256, num_updates=7200, lr=0.000416667, gnorm=3.395, clip=0, loss_scale=2048, train_wall=198, wall=17867
2022-07-05 21:42:08 | INFO | train_inner | epoch 007:    575 / 1122 loss=nan, nll_loss=3.037, mask_ins=1.095, word_ins_ml=4.626, word_reposition=1.039, kpe=nan, ppl=nan, wps=7208.6, ups=0.36, wpb=19803.5, bsz=256, num_updates=7300, lr=0.000413803, gnorm=3.466, clip=0, loss_scale=2048, train_wall=235, wall=18141
2022-07-05 21:42:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 21:46:07 | INFO | train_inner | epoch 007:    676 / 1122 loss=nan, nll_loss=2.962, mask_ins=1.067, word_ins_ml=4.559, word_reposition=1.039, kpe=nan, ppl=nan, wps=8332.6, ups=0.42, wpb=19966.7, bsz=256, num_updates=7400, lr=0.000410997, gnorm=3.495, clip=0, loss_scale=1065, train_wall=200, wall=18381
2022-07-05 21:50:04 | INFO | train_inner | epoch 007:    776 / 1122 loss=7.527, nll_loss=2.962, mask_ins=1.064, word_ins_ml=4.557, word_reposition=1.028, kpe=0.877, ppl=184.49, wps=8425.8, ups=0.42, wpb=19950.3, bsz=256, num_updates=7500, lr=0.000408248, gnorm=3.335, clip=0, loss_scale=1024, train_wall=198, wall=18618
2022-07-05 21:54:01 | INFO | train_inner | epoch 007:    876 / 1122 loss=7.433, nll_loss=2.908, mask_ins=1.042, word_ins_ml=4.509, word_reposition=1.007, kpe=0.875, ppl=172.8, wps=8408.7, ups=0.42, wpb=19958.9, bsz=256, num_updates=7600, lr=0.000405554, gnorm=3.309, clip=0, loss_scale=1024, train_wall=198, wall=18855
2022-07-05 21:57:58 | INFO | train_inner | epoch 007:    976 / 1122 loss=7.422, nll_loss=2.906, mask_ins=1.036, word_ins_ml=4.505, word_reposition=1.005, kpe=0.876, ppl=171.47, wps=8455.9, ups=0.42, wpb=20022.8, bsz=256, num_updates=7700, lr=0.000402911, gnorm=3.201, clip=0, loss_scale=1024, train_wall=198, wall=19092
2022-07-05 22:01:55 | INFO | train_inner | epoch 007:   1076 / 1122 loss=7.452, nll_loss=2.932, mask_ins=1.04, word_ins_ml=4.527, word_reposition=0.999, kpe=0.885, ppl=175.04, wps=8408.6, ups=0.42, wpb=19940.5, bsz=256, num_updates=7800, lr=0.00040032, gnorm=3.278, clip=0, loss_scale=1024, train_wall=198, wall=19329
2022-07-05 22:03:44 | INFO | train | epoch 007 | loss nan | nll_loss 3.027 | mask_ins 1.086 | word_ins_ml 4.617 | word_reposition 1.043 | kpe nan | ppl nan | wps 8045.2 | ups 0.4 | wpb 19913.8 | bsz 255.8 | num_updates 7846 | lr 0.000399145 | gnorm 3.437 | clip 0 | loss_scale 1410 | train_wall 2261 | wall 19437
2022-07-05 22:04:50 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 12.212 | nll_loss 6.349 | mask_ins 1.656 | word_ins_ml 7.639 | word_reposition 1.548 | kpe 1.369 | ppl 4744.83 | wps 14400.6 | wpb 2279.4 | bsz 32 | num_updates 7846 | best_loss 12.212
2022-07-05 22:04:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 7 @ 7846 updates, score 12.212) (writing took 9.152253886684775 seconds)
2022-07-05 22:07:07 | INFO | train_inner | epoch 008:     54 / 1122 loss=7.36, nll_loss=2.857, mask_ins=1.043, word_ins_ml=4.461, word_reposition=0.993, kpe=0.863, ppl=164.29, wps=6323.6, ups=0.32, wpb=19725.6, bsz=253.8, num_updates=7900, lr=0.000397779, gnorm=3.51, clip=0, loss_scale=1894, train_wall=197, wall=19641
2022-07-05 22:11:05 | INFO | train_inner | epoch 008:    154 / 1122 loss=7.2, nll_loss=2.753, mask_ins=1.019, word_ins_ml=4.368, word_reposition=0.975, kpe=0.838, ppl=147.05, wps=8390.3, ups=0.42, wpb=19913.5, bsz=256, num_updates=8000, lr=0.000395285, gnorm=3.231, clip=0, loss_scale=2048, train_wall=198, wall=19878
2022-07-05 22:15:02 | INFO | train_inner | epoch 008:    254 / 1122 loss=7.263, nll_loss=2.836, mask_ins=1.016, word_ins_ml=4.441, word_reposition=0.964, kpe=0.842, ppl=153.58, wps=8381.8, ups=0.42, wpb=19875.2, bsz=256, num_updates=8100, lr=0.000392837, gnorm=3.492, clip=0, loss_scale=2048, train_wall=198, wall=20115
2022-07-05 22:15:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 22:19:01 | INFO | train_inner | epoch 008:    355 / 1122 loss=7.139, nll_loss=2.727, mask_ins=0.994, word_ins_ml=4.343, word_reposition=0.958, kpe=0.845, ppl=140.98, wps=8338.1, ups=0.42, wpb=19970.3, bsz=256, num_updates=8200, lr=0.000390434, gnorm=3.216, clip=0, loss_scale=1115, train_wall=200, wall=20355
2022-07-05 22:22:59 | INFO | train_inner | epoch 008:    455 / 1122 loss=7.153, nll_loss=2.768, mask_ins=0.983, word_ins_ml=4.378, word_reposition=0.956, kpe=0.835, ppl=142.29, wps=8400.9, ups=0.42, wpb=19927.4, bsz=256, num_updates=8300, lr=0.000388075, gnorm=3.05, clip=0, loss_scale=1024, train_wall=198, wall=20592
2022-07-05 22:26:55 | INFO | train_inner | epoch 008:    555 / 1122 loss=nan, nll_loss=2.712, mask_ins=0.984, word_ins_ml=4.328, word_reposition=0.961, kpe=nan, ppl=nan, wps=8452.6, ups=0.42, wpb=20029.4, bsz=256, num_updates=8400, lr=0.000385758, gnorm=3.18, clip=0, loss_scale=1024, train_wall=198, wall=20829
2022-07-05 22:30:53 | INFO | train_inner | epoch 008:    655 / 1122 loss=7.141, nll_loss=2.759, mask_ins=0.976, word_ins_ml=4.368, word_reposition=0.94, kpe=0.856, ppl=141.09, wps=8422.2, ups=0.42, wpb=19977, bsz=256, num_updates=8500, lr=0.000383482, gnorm=3.752, clip=0, loss_scale=1024, train_wall=198, wall=21066
2022-07-05 22:35:26 | INFO | train_inner | epoch 008:    755 / 1122 loss=7.089, nll_loss=2.698, mask_ins=0.981, word_ins_ml=4.314, word_reposition=0.943, kpe=0.852, ppl=136.16, wps=7262.4, ups=0.37, wpb=19873.1, bsz=256, num_updates=8600, lr=0.000381246, gnorm=3.19, clip=0, loss_scale=1024, train_wall=234, wall=21340
2022-07-05 22:39:23 | INFO | train_inner | epoch 008:    855 / 1122 loss=nan, nll_loss=2.697, mask_ins=0.973, word_ins_ml=4.312, word_reposition=0.937, kpe=nan, ppl=nan, wps=8431.9, ups=0.42, wpb=19964.4, bsz=256, num_updates=8700, lr=0.000379049, gnorm=3.133, clip=0, loss_scale=1843, train_wall=198, wall=21577
2022-07-05 22:43:20 | INFO | train_inner | epoch 008:    955 / 1122 loss=7.066, nll_loss=2.691, mask_ins=0.968, word_ins_ml=4.306, word_reposition=0.937, kpe=0.855, ppl=134, wps=8408.6, ups=0.42, wpb=19910, bsz=256, num_updates=8800, lr=0.000376889, gnorm=3.147, clip=0, loss_scale=2048, train_wall=198, wall=21814
2022-07-05 22:47:17 | INFO | train_inner | epoch 008:   1055 / 1122 loss=7.004, nll_loss=2.639, mask_ins=0.963, word_ins_ml=4.259, word_reposition=0.931, kpe=0.851, ppl=128.32, wps=8348.6, ups=0.42, wpb=19762.9, bsz=256, num_updates=8900, lr=0.000374766, gnorm=3.044, clip=0, loss_scale=2048, train_wall=198, wall=22050
2022-07-05 22:48:32 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-05 22:49:54 | INFO | train | epoch 008 | loss nan | nll_loss 2.732 | mask_ins 0.987 | word_ins_ml 4.345 | word_reposition 0.951 | kpe nan | ppl nan | wps 8049.3 | ups 0.4 | wpb 19914 | bsz 255.8 | num_updates 8966 | lr 0.000373384 | gnorm 3.252 | clip 0 | loss_scale 1548 | train_wall 2258 | wall 22208
2022-07-05 22:51:01 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 12.158 | nll_loss 6.186 | mask_ins 1.733 | word_ins_ml 7.494 | word_reposition 1.555 | kpe 1.376 | ppl 4569.57 | wps 14338 | wpb 2279.4 | bsz 32 | num_updates 8966 | best_loss 12.158
2022-07-05 22:51:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 8 @ 8966 updates, score 12.158) (writing took 9.176835214719176 seconds)
2022-07-05 22:52:31 | INFO | train_inner | epoch 009:     34 / 1122 loss=7.007, nll_loss=2.66, mask_ins=0.96, word_ins_ml=4.277, word_reposition=0.928, kpe=0.841, ppl=128.59, wps=6281.9, ups=0.32, wpb=19747.3, bsz=253.8, num_updates=9000, lr=0.000372678, gnorm=3.198, clip=0, loss_scale=1338, train_wall=200, wall=22365
2022-07-05 22:56:28 | INFO | train_inner | epoch 009:    134 / 1122 loss=6.833, nll_loss=2.549, mask_ins=0.941, word_ins_ml=4.178, word_reposition=0.91, kpe=0.804, ppl=114.03, wps=8464.8, ups=0.42, wpb=20058, bsz=256, num_updates=9100, lr=0.000370625, gnorm=3.091, clip=0, loss_scale=1024, train_wall=198, wall=22602
2022-07-05 22:56:51 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-05 23:00:27 | INFO | train_inner | epoch 009:    235 / 1122 loss=6.896, nll_loss=2.596, mask_ins=0.957, word_ins_ml=4.219, word_reposition=0.911, kpe=0.809, ppl=119.1, wps=8315.5, ups=0.42, wpb=19868.3, bsz=256, num_updates=9200, lr=0.000368605, gnorm=3.036, clip=0, loss_scale=558, train_wall=199, wall=22841
2022-07-05 23:04:24 | INFO | train_inner | epoch 009:    335 / 1122 loss=6.862, nll_loss=2.575, mask_ins=0.938, word_ins_ml=4.2, word_reposition=0.913, kpe=0.812, ppl=116.34, wps=8372.3, ups=0.42, wpb=19830, bsz=256, num_updates=9300, lr=0.000366618, gnorm=2.908, clip=0, loss_scale=512, train_wall=198, wall=23077
2022-07-05 23:08:21 | INFO | train_inner | epoch 009:    435 / 1122 loss=nan, nll_loss=2.557, mask_ins=0.929, word_ins_ml=4.183, word_reposition=0.907, kpe=nan, ppl=nan, wps=8363.7, ups=0.42, wpb=19868.2, bsz=256, num_updates=9400, lr=0.000364662, gnorm=3.187, clip=0, loss_scale=512, train_wall=199, wall=23315
2022-07-05 23:12:18 | INFO | train_inner | epoch 009:    535 / 1122 loss=6.812, nll_loss=2.532, mask_ins=0.934, word_ins_ml=4.16, word_reposition=0.903, kpe=0.815, ppl=112.34, wps=8394.5, ups=0.42, wpb=19911.1, bsz=256, num_updates=9500, lr=0.000362738, gnorm=2.999, clip=0, loss_scale=512, train_wall=198, wall=23552
2022-07-05 23:16:16 | INFO | train_inner | epoch 009:    635 / 1122 loss=6.827, nll_loss=2.545, mask_ins=0.931, word_ins_ml=4.171, word_reposition=0.904, kpe=0.821, ppl=113.52, wps=8399.2, ups=0.42, wpb=19961.5, bsz=256, num_updates=9600, lr=0.000360844, gnorm=3.135, clip=0, loss_scale=512, train_wall=199, wall=23790
2022-07-05 23:20:12 | INFO | train_inner | epoch 009:    735 / 1122 loss=6.803, nll_loss=2.509, mask_ins=0.939, word_ins_ml=4.138, word_reposition=0.906, kpe=0.82, ppl=111.69, wps=8434, ups=0.42, wpb=19888, bsz=256, num_updates=9700, lr=0.000358979, gnorm=3.258, clip=0, loss_scale=922, train_wall=197, wall=24026
2022-07-05 23:24:34 | INFO | train_inner | epoch 009:    835 / 1122 loss=6.805, nll_loss=2.538, mask_ins=0.922, word_ins_ml=4.164, word_reposition=0.899, kpe=0.82, ppl=111.84, wps=7579.3, ups=0.38, wpb=19899.3, bsz=256, num_updates=9800, lr=0.000357143, gnorm=3.047, clip=0, loss_scale=1024, train_wall=224, wall=24288
2022-07-05 23:27:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-05 23:28:34 | INFO | train_inner | epoch 009:    936 / 1122 loss=6.777, nll_loss=2.512, mask_ins=0.919, word_ins_ml=4.139, word_reposition=0.891, kpe=0.827, ppl=109.65, wps=8336.6, ups=0.42, wpb=19991.1, bsz=256, num_updates=9900, lr=0.000355335, gnorm=2.987, clip=0, loss_scale=842, train_wall=200, wall=24528
2022-07-05 23:32:31 | INFO | train_inner | epoch 009:   1036 / 1122 loss=nan, nll_loss=2.51, mask_ins=0.916, word_ins_ml=4.137, word_reposition=0.891, kpe=nan, ppl=nan, wps=8418.1, ups=0.42, wpb=19939.5, bsz=256, num_updates=10000, lr=0.000353553, gnorm=2.883, clip=0, loss_scale=512, train_wall=198, wall=24765
2022-07-05 23:35:54 | INFO | train | epoch 009 | loss nan | nll_loss 2.542 | mask_ins 0.933 | word_ins_ml 4.169 | word_reposition 0.903 | kpe nan | ppl nan | wps 8080.6 | ups 0.41 | wpb 19913.1 | bsz 255.8 | num_updates 10086 | lr 0.000352043 | gnorm 3.051 | clip 0 | loss_scale 689 | train_wall 2247 | wall 24968
2022-07-05 23:37:01 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.274 | nll_loss 6.19 | mask_ins 1.63 | word_ins_ml 7.514 | word_reposition 1.682 | kpe 1.447 | ppl 4951.57 | wps 14433.2 | wpb 2279.4 | bsz 32 | num_updates 10086 | best_loss 12.158
2022-07-05 23:37:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 9 @ 10086 updates, score 12.274) (writing took 5.214343176223338 seconds)
2022-07-05 23:37:39 | INFO | train_inner | epoch 010:     14 / 1122 loss=6.764, nll_loss=2.508, mask_ins=0.919, word_ins_ml=4.135, word_reposition=0.889, kpe=0.822, ppl=108.72, wps=6462.5, ups=0.33, wpb=19875.6, bsz=253.8, num_updates=10100, lr=0.000351799, gnorm=3.017, clip=0, loss_scale=512, train_wall=198, wall=25072
2022-07-05 23:41:36 | INFO | train_inner | epoch 010:    114 / 1122 loss=6.621, nll_loss=2.425, mask_ins=0.907, word_ins_ml=4.061, word_reposition=0.886, kpe=0.767, ppl=98.44, wps=8405.9, ups=0.42, wpb=19939.5, bsz=256, num_updates=10200, lr=0.00035007, gnorm=2.962, clip=0, loss_scale=512, train_wall=198, wall=25310
2022-07-05 23:45:33 | INFO | train_inner | epoch 010:    214 / 1122 loss=6.65, nll_loss=2.452, mask_ins=0.904, word_ins_ml=4.084, word_reposition=0.887, kpe=0.774, ppl=100.44, wps=8403.2, ups=0.42, wpb=19883.9, bsz=256, num_updates=10300, lr=0.000348367, gnorm=3.046, clip=0, loss_scale=512, train_wall=198, wall=25546
2022-07-05 23:49:30 | INFO | train_inner | epoch 010:    314 / 1122 loss=6.646, nll_loss=2.436, mask_ins=0.908, word_ins_ml=4.07, word_reposition=0.887, kpe=0.781, ppl=100.14, wps=8435.7, ups=0.42, wpb=19994.8, bsz=256, num_updates=10400, lr=0.000346688, gnorm=3.03, clip=0, loss_scale=635, train_wall=198, wall=25783
2022-07-05 23:53:27 | INFO | train_inner | epoch 010:    414 / 1122 loss=6.59, nll_loss=2.406, mask_ins=0.896, word_ins_ml=4.042, word_reposition=0.873, kpe=0.779, ppl=96.33, wps=8428.8, ups=0.42, wpb=19984.3, bsz=256, num_updates=10500, lr=0.000345033, gnorm=2.791, clip=0, loss_scale=1024, train_wall=198, wall=26020
2022-07-05 23:54:38 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-05 23:57:26 | INFO | train_inner | epoch 010:    515 / 1122 loss=6.63, nll_loss=2.436, mask_ins=0.897, word_ins_ml=4.069, word_reposition=0.884, kpe=0.781, ppl=99.07, wps=8293.9, ups=0.42, wpb=19867.5, bsz=256, num_updates=10600, lr=0.000343401, gnorm=2.781, clip=0, loss_scale=659, train_wall=200, wall=26260
2022-07-06 00:01:23 | INFO | train_inner | epoch 010:    615 / 1122 loss=nan, nll_loss=2.415, mask_ins=0.89, word_ins_ml=4.049, word_reposition=0.88, kpe=nan, ppl=nan, wps=8483.9, ups=0.42, wpb=20107.7, bsz=256, num_updates=10700, lr=0.000341793, gnorm=2.976, clip=0, loss_scale=512, train_wall=198, wall=26497
2022-07-06 00:05:21 | INFO | train_inner | epoch 010:    715 / 1122 loss=6.59, nll_loss=2.396, mask_ins=0.892, word_ins_ml=4.032, word_reposition=0.878, kpe=0.788, ppl=96.36, wps=8372.7, ups=0.42, wpb=19868.2, bsz=256, num_updates=10800, lr=0.000340207, gnorm=2.81, clip=0, loss_scale=512, train_wall=198, wall=26734
2022-07-06 00:09:19 | INFO | train_inner | epoch 010:    815 / 1122 loss=nan, nll_loss=2.399, mask_ins=0.886, word_ins_ml=4.034, word_reposition=0.866, kpe=nan, ppl=nan, wps=8380.5, ups=0.42, wpb=19949.5, bsz=256, num_updates=10900, lr=0.000338643, gnorm=2.917, clip=0, loss_scale=512, train_wall=199, wall=26972
2022-07-06 00:13:15 | INFO | train_inner | epoch 010:    915 / 1122 loss=6.599, nll_loss=2.417, mask_ins=0.891, word_ins_ml=4.05, word_reposition=0.868, kpe=0.791, ppl=96.94, wps=8448.9, ups=0.42, wpb=19945.9, bsz=256, num_updates=11000, lr=0.0003371, gnorm=2.741, clip=0, loss_scale=512, train_wall=197, wall=27208
2022-07-06 00:17:49 | INFO | train_inner | epoch 010:   1015 / 1122 loss=6.517, nll_loss=2.337, mask_ins=0.879, word_ins_ml=3.978, word_reposition=0.867, kpe=0.793, ppl=91.61, wps=7241.6, ups=0.36, wpb=19867.3, bsz=256, num_updates=11100, lr=0.000335578, gnorm=2.911, clip=0, loss_scale=819, train_wall=235, wall=27483
2022-07-06 00:21:46 | INFO | train_inner | epoch 010:   1115 / 1122 loss=6.544, nll_loss=2.372, mask_ins=0.877, word_ins_ml=4.008, word_reposition=0.865, kpe=0.794, ppl=93.29, wps=8344.2, ups=0.42, wpb=19792.1, bsz=256, num_updates=11200, lr=0.000334077, gnorm=2.814, clip=0, loss_scale=1024, train_wall=198, wall=27720
2022-07-06 00:22:02 | INFO | train | epoch 010 | loss nan | nll_loss 2.409 | mask_ins 0.894 | word_ins_ml 4.044 | word_reposition 0.877 | kpe nan | ppl nan | wps 8066 | ups 0.41 | wpb 19914.1 | bsz 255.8 | num_updates 11207 | lr 0.000333972 | gnorm 2.897 | clip 0 | loss_scale 658 | train_wall 2258 | wall 27736
2022-07-06 00:23:08 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.925 | nll_loss 6.016 | mask_ins 1.637 | word_ins_ml 7.344 | word_reposition 1.521 | kpe 1.423 | ppl 3887.47 | wps 14405 | wpb 2279.4 | bsz 32 | num_updates 11207 | best_loss 11.925
2022-07-06 00:23:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 10 @ 11207 updates, score 11.925) (writing took 8.991244278848171 seconds)
2022-07-06 00:26:57 | INFO | train_inner | epoch 011:     93 / 1122 loss=6.434, nll_loss=2.321, mask_ins=0.875, word_ins_ml=3.964, word_reposition=0.856, kpe=0.739, ppl=86.45, wps=6348.9, ups=0.32, wpb=19758.7, bsz=253.8, num_updates=11300, lr=0.000332595, gnorm=2.974, clip=0, loss_scale=1024, train_wall=197, wall=28031
2022-07-06 00:30:54 | INFO | train_inner | epoch 011:    193 / 1122 loss=nan, nll_loss=2.274, mask_ins=0.864, word_ins_ml=3.921, word_reposition=0.849, kpe=nan, ppl=nan, wps=8380, ups=0.42, wpb=19851.2, bsz=256, num_updates=11400, lr=0.000331133, gnorm=2.793, clip=0, loss_scale=1024, train_wall=198, wall=28268
2022-07-06 00:34:50 | INFO | train_inner | epoch 011:    293 / 1122 loss=6.372, nll_loss=2.276, mask_ins=0.856, word_ins_ml=3.922, word_reposition=0.857, kpe=0.738, ppl=82.85, wps=8448.1, ups=0.42, wpb=19928.1, bsz=256, num_updates=11500, lr=0.00032969, gnorm=2.851, clip=0, loss_scale=1024, train_wall=197, wall=28504
2022-07-06 00:37:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 00:38:51 | INFO | train_inner | epoch 011:    394 / 1122 loss=6.406, nll_loss=2.293, mask_ins=0.867, word_ins_ml=3.937, word_reposition=0.856, kpe=0.745, ppl=84.78, wps=8296.4, ups=0.42, wpb=19934.7, bsz=256, num_updates=11600, lr=0.000328266, gnorm=2.879, clip=0, loss_scale=1085, train_wall=200, wall=28744
2022-07-06 00:42:47 | INFO | train_inner | epoch 011:    494 / 1122 loss=6.467, nll_loss=2.342, mask_ins=0.873, word_ins_ml=3.98, word_reposition=0.864, kpe=0.75, ppl=88.44, wps=8468.3, ups=0.42, wpb=20028.3, bsz=256, num_updates=11700, lr=0.00032686, gnorm=2.973, clip=0, loss_scale=1024, train_wall=197, wall=28981
2022-07-06 00:46:44 | INFO | train_inner | epoch 011:    594 / 1122 loss=6.393, nll_loss=2.294, mask_ins=0.861, word_ins_ml=3.937, word_reposition=0.85, kpe=0.746, ppl=84.07, wps=8427.5, ups=0.42, wpb=19968.4, bsz=256, num_updates=11800, lr=0.000325472, gnorm=2.679, clip=0, loss_scale=1024, train_wall=198, wall=29218
2022-07-06 00:50:41 | INFO | train_inner | epoch 011:    694 / 1122 loss=6.421, nll_loss=2.3, mask_ins=0.873, word_ins_ml=3.942, word_reposition=0.848, kpe=0.758, ppl=85.69, wps=8411.4, ups=0.42, wpb=19923.1, bsz=256, num_updates=11900, lr=0.000324102, gnorm=2.966, clip=0, loss_scale=1024, train_wall=198, wall=29455
2022-07-06 00:54:37 | INFO | train_inner | epoch 011:    794 / 1122 loss=6.389, nll_loss=2.284, mask_ins=0.855, word_ins_ml=3.928, word_reposition=0.853, kpe=0.753, ppl=83.79, wps=8369.8, ups=0.42, wpb=19796.4, bsz=256, num_updates=12000, lr=0.000322749, gnorm=2.791, clip=0, loss_scale=1024, train_wall=198, wall=29691
2022-07-06 00:58:32 | INFO | train_inner | epoch 011:    894 / 1122 loss=6.387, nll_loss=2.288, mask_ins=0.851, word_ins_ml=3.93, word_reposition=0.85, kpe=0.756, ppl=83.72, wps=8512.6, ups=0.43, wpb=20005.2, bsz=256, num_updates=12100, lr=0.000321412, gnorm=2.766, clip=0, loss_scale=1341, train_wall=197, wall=29926
2022-07-06 01:02:26 | INFO | train_inner | epoch 011:    994 / 1122 loss=6.404, nll_loss=2.278, mask_ins=0.875, word_ins_ml=3.921, word_reposition=0.851, kpe=0.756, ppl=84.69, wps=8486.5, ups=0.43, wpb=19858.3, bsz=256, num_updates=12200, lr=0.000320092, gnorm=2.651, clip=0, loss_scale=2048, train_wall=196, wall=30160
2022-07-06 01:06:20 | INFO | train_inner | epoch 011:   1094 / 1122 loss=nan, nll_loss=2.252, mask_ins=0.853, word_ins_ml=3.898, word_reposition=0.847, kpe=nan, ppl=nan, wps=8541.9, ups=0.43, wpb=19965.4, bsz=256, num_updates=12300, lr=0.000318788, gnorm=2.749, clip=0, loss_scale=2048, train_wall=196, wall=30394
2022-07-06 01:07:25 | INFO | train | epoch 011 | loss nan | nll_loss 2.29 | mask_ins 0.864 | word_ins_ml 3.934 | word_reposition 0.853 | kpe nan | ppl nan | wps 8199.3 | ups 0.41 | wpb 19913.5 | bsz 255.8 | num_updates 12328 | lr 0.000318426 | gnorm 2.823 | clip 0 | loss_scale 1266 | train_wall 2212 | wall 30458
2022-07-06 01:08:30 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 12.046 | nll_loss 5.961 | mask_ins 1.653 | word_ins_ml 7.301 | word_reposition 1.574 | kpe 1.518 | ppl 4229.06 | wps 14604.5 | wpb 2279.4 | bsz 32 | num_updates 12328 | best_loss 11.925
2022-07-06 01:08:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 11 @ 12328 updates, score 12.046) (writing took 4.8528410056605935 seconds)
2022-07-06 01:11:59 | INFO | train_inner | epoch 012:     72 / 1122 loss=6.325, nll_loss=2.26, mask_ins=0.852, word_ins_ml=3.905, word_reposition=0.853, kpe=0.715, ppl=80.17, wps=5850.6, ups=0.3, wpb=19817.8, bsz=253.8, num_updates=12400, lr=0.0003175, gnorm=2.837, clip=0, loss_scale=2048, train_wall=231, wall=30733
2022-07-06 01:15:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 01:15:56 | INFO | train_inner | epoch 012:    173 / 1122 loss=6.238, nll_loss=2.207, mask_ins=0.842, word_ins_ml=3.857, word_reposition=0.839, kpe=0.699, ppl=75.5, wps=8402.7, ups=0.42, wpb=19887.3, bsz=256, num_updates=12500, lr=0.000316228, gnorm=2.738, clip=0, loss_scale=1896, train_wall=198, wall=30969
2022-07-06 01:19:49 | INFO | train_inner | epoch 012:    273 / 1122 loss=6.266, nll_loss=2.232, mask_ins=0.85, word_ins_ml=3.879, word_reposition=0.834, kpe=0.703, ppl=76.96, wps=8541.8, ups=0.43, wpb=19911.4, bsz=256, num_updates=12600, lr=0.00031497, gnorm=2.871, clip=0, loss_scale=1024, train_wall=195, wall=31202
2022-07-06 01:23:42 | INFO | train_inner | epoch 012:    373 / 1122 loss=6.279, nll_loss=2.244, mask_ins=0.845, word_ins_ml=3.889, word_reposition=0.834, kpe=0.71, ppl=77.65, wps=8537.3, ups=0.43, wpb=19894.5, bsz=256, num_updates=12700, lr=0.000313728, gnorm=2.769, clip=0, loss_scale=1024, train_wall=195, wall=31435
2022-07-06 01:27:35 | INFO | train_inner | epoch 012:    473 / 1122 loss=6.248, nll_loss=2.219, mask_ins=0.836, word_ins_ml=3.867, word_reposition=0.836, kpe=0.708, ppl=75.99, wps=8535.9, ups=0.43, wpb=19886.1, bsz=256, num_updates=12800, lr=0.0003125, gnorm=2.715, clip=0, loss_scale=1024, train_wall=196, wall=31668
2022-07-06 01:31:28 | INFO | train_inner | epoch 012:    573 / 1122 loss=6.232, nll_loss=2.204, mask_ins=0.841, word_ins_ml=3.853, word_reposition=0.829, kpe=0.71, ppl=75.18, wps=8587.6, ups=0.43, wpb=20043.1, bsz=256, num_updates=12900, lr=0.000311286, gnorm=2.646, clip=0, loss_scale=1024, train_wall=196, wall=31902
2022-07-06 01:35:21 | INFO | train_inner | epoch 012:    673 / 1122 loss=6.235, nll_loss=2.198, mask_ins=0.841, word_ins_ml=3.848, word_reposition=0.831, kpe=0.715, ppl=75.32, wps=8501.7, ups=0.43, wpb=19769.9, bsz=256, num_updates=13000, lr=0.000310087, gnorm=2.796, clip=0, loss_scale=1055, train_wall=195, wall=32134
2022-07-06 01:39:14 | INFO | train_inner | epoch 012:    773 / 1122 loss=6.204, nll_loss=2.173, mask_ins=0.834, word_ins_ml=3.825, word_reposition=0.83, kpe=0.715, ppl=73.7, wps=8595.7, ups=0.43, wpb=20052.3, bsz=256, num_updates=13100, lr=0.000308901, gnorm=2.72, clip=0, loss_scale=2048, train_wall=196, wall=32368
2022-07-06 01:43:07 | INFO | train_inner | epoch 012:    873 / 1122 loss=6.197, nll_loss=2.162, mask_ins=0.837, word_ins_ml=3.814, word_reposition=0.827, kpe=0.718, ppl=73.34, wps=8572.5, ups=0.43, wpb=19954, bsz=256, num_updates=13200, lr=0.000307729, gnorm=2.737, clip=0, loss_scale=2048, train_wall=195, wall=32600
2022-07-06 01:47:00 | INFO | train_inner | epoch 012:    973 / 1122 loss=nan, nll_loss=2.177, mask_ins=0.823, word_ins_ml=3.828, word_reposition=0.827, kpe=nan, ppl=nan, wps=8528.1, ups=0.43, wpb=19885.2, bsz=256, num_updates=13300, lr=0.00030657, gnorm=2.776, clip=0, loss_scale=2048, train_wall=195, wall=32834
2022-07-06 01:50:53 | INFO | train_inner | epoch 012:   1073 / 1122 loss=nan, nll_loss=2.164, mask_ins=0.841, word_ins_ml=3.816, word_reposition=0.835, kpe=nan, ppl=nan, wps=8532.8, ups=0.43, wpb=19908.8, bsz=256, num_updates=13400, lr=0.000305424, gnorm=2.659, clip=0, loss_scale=2048, train_wall=196, wall=33067
2022-07-06 01:52:47 | INFO | train | epoch 012 | loss nan | nll_loss 2.204 | mask_ins 0.84 | word_ins_ml 3.853 | word_reposition 0.833 | kpe nan | ppl nan | wps 8199.7 | ups 0.41 | wpb 19912.5 | bsz 255.8 | num_updates 13449 | lr 0.000304867 | gnorm 2.75 | clip 0 | loss_scale 1581 | train_wall 2229 | wall 33181
2022-07-06 01:53:52 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.993 | nll_loss 5.931 | mask_ins 1.669 | word_ins_ml 7.266 | word_reposition 1.533 | kpe 1.525 | ppl 4076.67 | wps 14694.4 | wpb 2279.4 | bsz 32 | num_updates 13449 | best_loss 11.925
2022-07-06 01:53:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 12 @ 13449 updates, score 11.993) (writing took 4.956552014686167 seconds)
2022-07-06 01:55:56 | INFO | train_inner | epoch 013:     51 / 1122 loss=6.209, nll_loss=2.18, mask_ins=0.848, word_ins_ml=3.83, word_reposition=0.833, kpe=0.698, ppl=74, wps=6531.3, ups=0.33, wpb=19759.4, bsz=253.8, num_updates=13500, lr=0.00030429, gnorm=2.838, clip=0, loss_scale=2048, train_wall=195, wall=33369
2022-07-06 01:59:49 | INFO | train_inner | epoch 013:    151 / 1122 loss=6.095, nll_loss=2.128, mask_ins=0.827, word_ins_ml=3.783, word_reposition=0.824, kpe=0.66, ppl=68.35, wps=8577.1, ups=0.43, wpb=19979.3, bsz=256, num_updates=13600, lr=0.00030317, gnorm=2.713, clip=0, loss_scale=3912, train_wall=195, wall=33602
2022-07-06 02:01:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 02:04:20 | INFO | train_inner | epoch 013:    252 / 1122 loss=6.12, nll_loss=2.147, mask_ins=0.828, word_ins_ml=3.8, word_reposition=0.824, kpe=0.668, ppl=69.57, wps=7329.8, ups=0.37, wpb=19905.7, bsz=256, num_updates=13700, lr=0.000302061, gnorm=2.761, clip=0, loss_scale=3042, train_wall=233, wall=33874
2022-07-06 02:08:14 | INFO | train_inner | epoch 013:    352 / 1122 loss=nan, nll_loss=2.134, mask_ins=0.813, word_ins_ml=3.788, word_reposition=0.826, kpe=nan, ppl=nan, wps=8524.4, ups=0.43, wpb=19952.2, bsz=256, num_updates=13800, lr=0.000300965, gnorm=2.74, clip=0, loss_scale=2048, train_wall=196, wall=34108
2022-07-06 02:12:08 | INFO | train_inner | epoch 013:    452 / 1122 loss=6.119, nll_loss=2.136, mask_ins=0.83, word_ins_ml=3.79, word_reposition=0.829, kpe=0.671, ppl=69.52, wps=8521.7, ups=0.43, wpb=19902.8, bsz=256, num_updates=13900, lr=0.00029988, gnorm=2.719, clip=0, loss_scale=2048, train_wall=196, wall=34342
2022-07-06 02:16:01 | INFO | train_inner | epoch 013:    552 / 1122 loss=6.142, nll_loss=2.157, mask_ins=0.825, word_ins_ml=3.809, word_reposition=0.826, kpe=0.682, ppl=70.62, wps=8548.2, ups=0.43, wpb=19929, bsz=256, num_updates=14000, lr=0.000298807, gnorm=2.994, clip=0, loss_scale=2048, train_wall=196, wall=34575
2022-07-06 02:19:54 | INFO | train_inner | epoch 013:    652 / 1122 loss=nan, nll_loss=2.16, mask_ins=0.825, word_ins_ml=3.81, word_reposition=0.828, kpe=nan, ppl=nan, wps=8527.1, ups=0.43, wpb=19887.6, bsz=256, num_updates=14100, lr=0.000297746, gnorm=2.737, clip=0, loss_scale=2048, train_wall=196, wall=34808
2022-07-06 02:23:48 | INFO | train_inner | epoch 013:    752 / 1122 loss=6.135, nll_loss=2.167, mask_ins=0.817, word_ins_ml=3.817, word_reposition=0.818, kpe=0.683, ppl=70.26, wps=8517.7, ups=0.43, wpb=19894.5, bsz=256, num_updates=14200, lr=0.000296695, gnorm=2.833, clip=0, loss_scale=2867, train_wall=196, wall=35041
2022-07-06 02:27:42 | INFO | train_inner | epoch 013:    852 / 1122 loss=6.134, nll_loss=2.162, mask_ins=0.82, word_ins_ml=3.811, word_reposition=0.818, kpe=0.685, ppl=70.25, wps=8516.6, ups=0.43, wpb=19939.8, bsz=256, num_updates=14300, lr=0.000295656, gnorm=2.669, clip=0, loss_scale=4096, train_wall=197, wall=35276
2022-07-06 02:31:35 | INFO | train_inner | epoch 013:    952 / 1122 loss=6.107, nll_loss=2.122, mask_ins=0.82, word_ins_ml=3.776, word_reposition=0.824, kpe=0.687, ppl=68.92, wps=8538.5, ups=0.43, wpb=19894.4, bsz=256, num_updates=14400, lr=0.000294628, gnorm=2.627, clip=0, loss_scale=4096, train_wall=196, wall=35509
2022-07-06 02:31:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 02:35:31 | INFO | train_inner | epoch 013:   1053 / 1122 loss=6.124, nll_loss=2.134, mask_ins=0.82, word_ins_ml=3.786, word_reposition=0.825, kpe=0.693, ppl=69.75, wps=8472.9, ups=0.42, wpb=20006.9, bsz=256, num_updates=14500, lr=0.00029361, gnorm=2.68, clip=0, loss_scale=2230, train_wall=198, wall=35745
2022-07-06 02:38:11 | INFO | train | epoch 013 | loss nan | nll_loss 2.142 | mask_ins 0.823 | word_ins_ml 3.795 | word_reposition 0.824 | kpe nan | ppl nan | wps 8188.2 | ups 0.41 | wpb 19914 | bsz 255.8 | num_updates 14569 | lr 0.000292914 | gnorm 2.75 | clip 0 | loss_scale 2758 | train_wall 2232 | wall 35904
2022-07-06 02:39:16 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 12.08 | nll_loss 5.938 | mask_ins 1.679 | word_ins_ml 7.273 | word_reposition 1.573 | kpe 1.555 | ppl 4329.86 | wps 14635 | wpb 2279.4 | bsz 32 | num_updates 14569 | best_loss 11.925
2022-07-06 02:39:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 13 @ 14569 updates, score 12.08) (writing took 4.9748671762645245 seconds)
2022-07-06 02:40:33 | INFO | train_inner | epoch 014:     31 / 1122 loss=6.044, nll_loss=2.096, mask_ins=0.811, word_ins_ml=3.752, word_reposition=0.809, kpe=0.672, ppl=65.96, wps=6565.6, ups=0.33, wpb=19811.8, bsz=253.8, num_updates=14600, lr=0.000292603, gnorm=2.749, clip=0, loss_scale=2048, train_wall=194, wall=36046
2022-07-06 02:41:03 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 02:44:28 | INFO | train_inner | epoch 014:    132 / 1122 loss=5.991, nll_loss=2.075, mask_ins=0.817, word_ins_ml=3.734, word_reposition=0.811, kpe=0.628, ppl=63.58, wps=8454.9, ups=0.42, wpb=19901.8, bsz=256, num_updates=14700, lr=0.000291606, gnorm=2.697, clip=0, loss_scale=1146, train_wall=197, wall=36282
2022-07-06 02:48:22 | INFO | train_inner | epoch 014:    232 / 1122 loss=6, nll_loss=2.083, mask_ins=0.816, word_ins_ml=3.741, word_reposition=0.815, kpe=0.629, ppl=64, wps=8537.3, ups=0.43, wpb=19924.7, bsz=256, num_updates=14800, lr=0.000290619, gnorm=2.669, clip=0, loss_scale=1024, train_wall=196, wall=36515
2022-07-06 02:52:15 | INFO | train_inner | epoch 014:    332 / 1122 loss=5.981, nll_loss=2.086, mask_ins=0.805, word_ins_ml=3.742, word_reposition=0.799, kpe=0.636, ppl=63.17, wps=8545, ups=0.43, wpb=19939.9, bsz=256, num_updates=14900, lr=0.000289642, gnorm=2.775, clip=0, loss_scale=1024, train_wall=196, wall=36749
2022-07-06 02:56:46 | INFO | train_inner | epoch 014:    432 / 1122 loss=6.001, nll_loss=2.073, mask_ins=0.817, word_ins_ml=3.731, word_reposition=0.813, kpe=0.64, ppl=64.04, wps=7344, ups=0.37, wpb=19915.1, bsz=256, num_updates=15000, lr=0.000288675, gnorm=2.748, clip=0, loss_scale=1024, train_wall=233, wall=37020
2022-07-06 03:00:40 | INFO | train_inner | epoch 014:    532 / 1122 loss=5.973, nll_loss=2.044, mask_ins=0.808, word_ins_ml=3.705, word_reposition=0.813, kpe=0.647, ppl=62.82, wps=8505.9, ups=0.43, wpb=19914.1, bsz=256, num_updates=15100, lr=0.000287718, gnorm=2.784, clip=0, loss_scale=1024, train_wall=196, wall=37254
2022-07-06 03:04:33 | INFO | train_inner | epoch 014:    632 / 1122 loss=5.987, nll_loss=2.064, mask_ins=0.813, word_ins_ml=3.722, word_reposition=0.807, kpe=0.645, ppl=63.42, wps=8532.2, ups=0.43, wpb=19878.6, bsz=256, num_updates=15200, lr=0.00028677, gnorm=2.756, clip=0, loss_scale=1812, train_wall=195, wall=37487
2022-07-06 03:08:26 | INFO | train_inner | epoch 014:    732 / 1122 loss=5.979, nll_loss=2.072, mask_ins=0.798, word_ins_ml=3.729, word_reposition=0.804, kpe=0.647, ppl=63.06, wps=8606.7, ups=0.43, wpb=20048.2, bsz=256, num_updates=15300, lr=0.000285831, gnorm=2.717, clip=0, loss_scale=2048, train_wall=196, wall=37720
2022-07-06 03:12:20 | INFO | train_inner | epoch 014:    832 / 1122 loss=6.051, nll_loss=2.114, mask_ins=0.821, word_ins_ml=3.766, word_reposition=0.81, kpe=0.654, ppl=66.32, wps=8489.3, ups=0.43, wpb=19828.9, bsz=256, num_updates=15400, lr=0.000284901, gnorm=2.774, clip=0, loss_scale=2048, train_wall=196, wall=37953
2022-07-06 03:16:13 | INFO | train_inner | epoch 014:    932 / 1122 loss=6.003, nll_loss=2.067, mask_ins=0.81, word_ins_ml=3.724, word_reposition=0.812, kpe=0.657, ppl=64.11, wps=8529.7, ups=0.43, wpb=19893.3, bsz=256, num_updates=15500, lr=0.000283981, gnorm=2.712, clip=0, loss_scale=2048, train_wall=195, wall=38187
2022-07-06 03:20:06 | INFO | train_inner | epoch 014:   1032 / 1122 loss=5.96, nll_loss=2.039, mask_ins=0.804, word_ins_ml=3.699, word_reposition=0.805, kpe=0.653, ppl=62.24, wps=8543.7, ups=0.43, wpb=19942.2, bsz=256, num_updates=15600, lr=0.000283069, gnorm=2.711, clip=0, loss_scale=2048, train_wall=196, wall=38420
2022-07-06 03:23:36 | INFO | train | epoch 014 | loss nan | nll_loss 2.068 | mask_ins 0.81 | word_ins_ml 3.726 | word_reposition 0.807 | kpe nan | ppl nan | wps 8190.5 | ups 0.41 | wpb 19913.5 | bsz 255.8 | num_updates 15690 | lr 0.000282256 | gnorm 2.738 | clip 0 | loss_scale 1681 | train_wall 2233 | wall 38630
2022-07-06 03:24:42 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 12.227 | nll_loss 5.931 | mask_ins 1.661 | word_ins_ml 7.272 | word_reposition 1.638 | kpe 1.656 | ppl 4795.29 | wps 14590.2 | wpb 2279.4 | bsz 32 | num_updates 15690 | best_loss 11.925
2022-07-06 03:24:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 14 @ 15690 updates, score 12.227) (writing took 5.660993836820126 seconds)
2022-07-06 03:25:10 | INFO | train_inner | epoch 015:     10 / 1122 loss=nan, nll_loss=2.044, mask_ins=0.798, word_ins_ml=3.703, word_reposition=0.797, kpe=nan, ppl=nan, wps=6520.3, ups=0.33, wpb=19817.6, bsz=253.8, num_updates=15700, lr=0.000282166, gnorm=2.803, clip=0, loss_scale=3379, train_wall=196, wall=38724
2022-07-06 03:29:04 | INFO | train_inner | epoch 015:    110 / 1122 loss=5.884, nll_loss=2.015, mask_ins=0.803, word_ins_ml=3.678, word_reposition=0.805, kpe=0.597, ppl=59.05, wps=8572.2, ups=0.43, wpb=20030.5, bsz=256, num_updates=15800, lr=0.000281272, gnorm=2.667, clip=0, loss_scale=4096, train_wall=196, wall=38958
2022-07-06 03:32:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 03:32:59 | INFO | train_inner | epoch 015:    211 / 1122 loss=5.862, nll_loss=2.015, mask_ins=0.793, word_ins_ml=3.678, word_reposition=0.798, kpe=0.593, ppl=58.15, wps=8466.3, ups=0.42, wpb=19923.6, bsz=256, num_updates=15900, lr=0.000280386, gnorm=2.561, clip=0, loss_scale=3711, train_wall=198, wall=39193
2022-07-06 03:36:53 | INFO | train_inner | epoch 015:    311 / 1122 loss=nan, nll_loss=2.013, mask_ins=0.797, word_ins_ml=3.675, word_reposition=0.802, kpe=nan, ppl=nan, wps=8569.9, ups=0.43, wpb=19983.6, bsz=256, num_updates=16000, lr=0.000279508, gnorm=2.691, clip=0, loss_scale=2048, train_wall=195, wall=39426
2022-07-06 03:40:46 | INFO | train_inner | epoch 015:    411 / 1122 loss=5.881, nll_loss=2.009, mask_ins=0.799, word_ins_ml=3.671, word_reposition=0.803, kpe=0.607, ppl=58.95, wps=8560.1, ups=0.43, wpb=19973.2, bsz=256, num_updates=16100, lr=0.000278639, gnorm=2.676, clip=0, loss_scale=2048, train_wall=196, wall=39660
2022-07-06 03:44:40 | INFO | train_inner | epoch 015:    511 / 1122 loss=5.852, nll_loss=1.987, mask_ins=0.792, word_ins_ml=3.652, word_reposition=0.799, kpe=0.61, ppl=57.77, wps=8490.3, ups=0.43, wpb=19843, bsz=256, num_updates=16200, lr=0.000277778, gnorm=2.661, clip=0, loss_scale=2048, train_wall=196, wall=39893
2022-07-06 03:49:09 | INFO | train_inner | epoch 015:    611 / 1122 loss=5.897, nll_loss=2.027, mask_ins=0.798, word_ins_ml=3.687, word_reposition=0.797, kpe=0.615, ppl=59.58, wps=7369.6, ups=0.37, wpb=19886.5, bsz=256, num_updates=16300, lr=0.000276924, gnorm=2.711, clip=0, loss_scale=2048, train_wall=232, wall=40163
2022-07-06 03:53:04 | INFO | train_inner | epoch 015:    711 / 1122 loss=5.949, nll_loss=2.062, mask_ins=0.807, word_ins_ml=3.718, word_reposition=0.806, kpe=0.618, ppl=61.8, wps=8444.8, ups=0.43, wpb=19818.4, bsz=256, num_updates=16400, lr=0.000276079, gnorm=2.652, clip=0, loss_scale=2191, train_wall=197, wall=40398
2022-07-06 03:54:42 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 03:57:00 | INFO | train_inner | epoch 015:    812 / 1122 loss=5.935, nll_loss=2.059, mask_ins=0.8, word_ins_ml=3.714, word_reposition=0.8, kpe=0.62, ppl=61.18, wps=8482, ups=0.42, wpb=19983.3, bsz=256, num_updates=16500, lr=0.000275241, gnorm=2.669, clip=0, loss_scale=2879, train_wall=198, wall=40633
2022-07-06 04:00:53 | INFO | train_inner | epoch 015:    912 / 1122 loss=5.948, nll_loss=2.057, mask_ins=0.807, word_ins_ml=3.713, word_reposition=0.801, kpe=0.627, ppl=61.73, wps=8511.8, ups=0.43, wpb=19865.3, bsz=256, num_updates=16600, lr=0.000274411, gnorm=2.689, clip=0, loss_scale=2048, train_wall=196, wall=40867
2022-07-06 04:04:47 | INFO | train_inner | epoch 015:   1012 / 1122 loss=5.913, nll_loss=2.032, mask_ins=0.792, word_ins_ml=3.69, word_reposition=0.804, kpe=0.626, ppl=60.27, wps=8535, ups=0.43, wpb=19935.7, bsz=256, num_updates=16700, lr=0.000273588, gnorm=2.719, clip=0, loss_scale=2048, train_wall=196, wall=41100
2022-07-06 04:08:40 | INFO | train_inner | epoch 015:   1112 / 1122 loss=nan, nll_loss=1.997, mask_ins=0.785, word_ins_ml=3.66, word_reposition=0.797, kpe=nan, ppl=nan, wps=8594.5, ups=0.43, wpb=20047.9, bsz=256, num_updates=16800, lr=0.000272772, gnorm=2.651, clip=0, loss_scale=2048, train_wall=196, wall=41334
2022-07-06 04:09:02 | INFO | train | epoch 015 | loss nan | nll_loss 2.026 | mask_ins 0.798 | word_ins_ml 3.686 | word_reposition 0.801 | kpe nan | ppl nan | wps 8182.5 | ups 0.41 | wpb 19915.9 | bsz 255.8 | num_updates 16810 | lr 0.000272691 | gnorm 2.679 | clip 0 | loss_scale 2486 | train_wall 2233 | wall 41356
2022-07-06 04:10:07 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.904 | nll_loss 5.717 | mask_ins 1.641 | word_ins_ml 7.06 | word_reposition 1.525 | kpe 1.678 | ppl 3832.82 | wps 14666 | wpb 2279.4 | bsz 32 | num_updates 16810 | best_loss 11.904
2022-07-06 04:10:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 15 @ 16810 updates, score 11.904) (writing took 9.664220280945301 seconds)
2022-07-06 04:13:47 | INFO | train_inner | epoch 016:     90 / 1122 loss=nan, nll_loss=1.958, mask_ins=0.784, word_ins_ml=3.625, word_reposition=0.785, kpe=nan, ppl=nan, wps=6410.6, ups=0.33, wpb=19665.3, bsz=253.8, num_updates=16900, lr=0.000271964, gnorm=2.868, clip=0, loss_scale=2048, train_wall=195, wall=41640
2022-07-06 04:17:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 04:17:43 | INFO | train_inner | epoch 016:    191 / 1122 loss=5.792, nll_loss=1.975, mask_ins=0.786, word_ins_ml=3.64, word_reposition=0.793, kpe=0.573, ppl=55.39, wps=8423.1, ups=0.42, wpb=19940, bsz=256, num_updates=17000, lr=0.000271163, gnorm=2.744, clip=0, loss_scale=3001, train_wall=198, wall=41877
2022-07-06 04:21:36 | INFO | train_inner | epoch 016:    291 / 1122 loss=nan, nll_loss=1.946, mask_ins=0.784, word_ins_ml=3.614, word_reposition=0.783, kpe=nan, ppl=nan, wps=8589.9, ups=0.43, wpb=20010.2, bsz=256, num_updates=17100, lr=0.000270369, gnorm=2.641, clip=0, loss_scale=2048, train_wall=195, wall=42110
2022-07-06 04:25:30 | INFO | train_inner | epoch 016:    391 / 1122 loss=5.795, nll_loss=1.974, mask_ins=0.791, word_ins_ml=3.639, word_reposition=0.79, kpe=0.576, ppl=55.53, wps=8505.6, ups=0.43, wpb=19872.9, bsz=256, num_updates=17200, lr=0.000269582, gnorm=2.722, clip=0, loss_scale=2048, train_wall=196, wall=42344
2022-07-06 04:29:23 | INFO | train_inner | epoch 016:    491 / 1122 loss=5.797, nll_loss=1.967, mask_ins=0.79, word_ins_ml=3.631, word_reposition=0.796, kpe=0.579, ppl=55.58, wps=8563.1, ups=0.43, wpb=19969.9, bsz=256, num_updates=17300, lr=0.000268802, gnorm=2.684, clip=0, loss_scale=2048, train_wall=195, wall=42577
2022-07-06 04:33:17 | INFO | train_inner | epoch 016:    591 / 1122 loss=5.798, nll_loss=1.973, mask_ins=0.785, word_ins_ml=3.637, word_reposition=0.79, kpe=0.586, ppl=55.63, wps=8558, ups=0.43, wpb=19961.3, bsz=256, num_updates=17400, lr=0.000268028, gnorm=2.812, clip=0, loss_scale=2048, train_wall=195, wall=42810
2022-07-06 04:37:11 | INFO | train_inner | epoch 016:    691 / 1122 loss=5.793, nll_loss=1.97, mask_ins=0.778, word_ins_ml=3.634, word_reposition=0.789, kpe=0.591, ppl=55.45, wps=8513, ups=0.43, wpb=19953.2, bsz=256, num_updates=17500, lr=0.000267261, gnorm=2.828, clip=0, loss_scale=2048, train_wall=197, wall=43045
2022-07-06 04:41:42 | INFO | train_inner | epoch 016:    791 / 1122 loss=5.812, nll_loss=1.984, mask_ins=0.785, word_ins_ml=3.646, word_reposition=0.789, kpe=0.592, ppl=56.2, wps=7333, ups=0.37, wpb=19883.8, bsz=256, num_updates=17600, lr=0.000266501, gnorm=2.69, clip=0, loss_scale=3891, train_wall=233, wall=43316
2022-07-06 04:45:36 | INFO | train_inner | epoch 016:    891 / 1122 loss=5.761, nll_loss=1.931, mask_ins=0.78, word_ins_ml=3.599, word_reposition=0.785, kpe=0.597, ppl=54.22, wps=8512.1, ups=0.43, wpb=19923.2, bsz=256, num_updates=17700, lr=0.000265747, gnorm=2.659, clip=0, loss_scale=4096, train_wall=196, wall=43550
2022-07-06 04:46:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 04:49:31 | INFO | train_inner | epoch 016:    992 / 1122 loss=5.82, nll_loss=1.97, mask_ins=0.794, word_ins_ml=3.633, word_reposition=0.793, kpe=0.6, ppl=56.5, wps=8497.3, ups=0.43, wpb=19986.3, bsz=256, num_updates=17800, lr=0.000264999, gnorm=2.657, clip=0, loss_scale=2737, train_wall=197, wall=43785
2022-07-06 04:53:25 | INFO | train_inner | epoch 016:   1092 / 1122 loss=5.778, nll_loss=1.954, mask_ins=0.779, word_ins_ml=3.619, word_reposition=0.784, kpe=0.596, ppl=54.88, wps=8486.3, ups=0.43, wpb=19855.1, bsz=256, num_updates=17900, lr=0.000264258, gnorm=2.658, clip=0, loss_scale=2048, train_wall=196, wall=44019
2022-07-06 04:54:35 | INFO | train | epoch 016 | loss nan | nll_loss 1.965 | mask_ins 0.785 | word_ins_ml 3.63 | word_reposition 0.789 | kpe nan | ppl nan | wps 8162.6 | ups 0.41 | wpb 19913.2 | bsz 255.8 | num_updates 17930 | lr 0.000264037 | gnorm 2.723 | clip 0 | loss_scale 2543 | train_wall 2233 | wall 44088
2022-07-06 04:55:40 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 11.892 | nll_loss 5.83 | mask_ins 1.629 | word_ins_ml 7.177 | word_reposition 1.417 | kpe 1.669 | ppl 3800.6 | wps 14558.2 | wpb 2279.4 | bsz 32 | num_updates 17930 | best_loss 11.892
2022-07-06 04:55:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 16 @ 17930 updates, score 11.892) (writing took 9.559289315715432 seconds)
2022-07-06 04:58:33 | INFO | train_inner | epoch 017:     70 / 1122 loss=5.731, nll_loss=1.941, mask_ins=0.784, word_ins_ml=3.607, word_reposition=0.783, kpe=0.558, ppl=53.13, wps=6450.2, ups=0.32, wpb=19860.1, bsz=253.8, num_updates=18000, lr=0.000263523, gnorm=2.824, clip=0, loss_scale=2048, train_wall=195, wall=44327
2022-07-06 05:02:27 | INFO | train_inner | epoch 017:    170 / 1122 loss=5.707, nll_loss=1.93, mask_ins=0.782, word_ins_ml=3.598, word_reposition=0.783, kpe=0.544, ppl=52.22, wps=8558, ups=0.43, wpb=19977.3, bsz=256, num_updates=18100, lr=0.000262794, gnorm=2.707, clip=0, loss_scale=2048, train_wall=196, wall=44560
2022-07-06 05:04:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 05:06:23 | INFO | train_inner | epoch 017:    271 / 1122 loss=5.723, nll_loss=1.945, mask_ins=0.78, word_ins_ml=3.611, word_reposition=0.785, kpe=0.547, ppl=52.84, wps=8419.7, ups=0.42, wpb=19861.3, bsz=256, num_updates=18200, lr=0.000262071, gnorm=2.654, clip=0, loss_scale=1653, train_wall=198, wall=44796
2022-07-06 05:10:16 | INFO | train_inner | epoch 017:    371 / 1122 loss=nan, nll_loss=1.945, mask_ins=0.779, word_ins_ml=3.61, word_reposition=0.781, kpe=nan, ppl=nan, wps=8489.2, ups=0.43, wpb=19825.7, bsz=256, num_updates=18300, lr=0.000261354, gnorm=2.663, clip=0, loss_scale=1024, train_wall=196, wall=45030
2022-07-06 05:14:10 | INFO | train_inner | epoch 017:    471 / 1122 loss=5.715, nll_loss=1.94, mask_ins=0.77, word_ins_ml=3.606, word_reposition=0.78, kpe=0.559, ppl=52.53, wps=8493.7, ups=0.43, wpb=19855.7, bsz=256, num_updates=18400, lr=0.000260643, gnorm=2.869, clip=0, loss_scale=1024, train_wall=196, wall=45264
2022-07-06 05:18:04 | INFO | train_inner | epoch 017:    571 / 1122 loss=5.715, nll_loss=1.936, mask_ins=0.774, word_ins_ml=3.602, word_reposition=0.78, kpe=0.559, ppl=52.54, wps=8512.2, ups=0.43, wpb=19886.7, bsz=256, num_updates=18500, lr=0.000259938, gnorm=2.764, clip=0, loss_scale=1024, train_wall=196, wall=45497
2022-07-06 05:21:57 | INFO | train_inner | epoch 017:    671 / 1122 loss=5.689, nll_loss=1.908, mask_ins=0.771, word_ins_ml=3.578, word_reposition=0.78, kpe=0.56, ppl=51.58, wps=8556.6, ups=0.43, wpb=19993, bsz=256, num_updates=18600, lr=0.000259238, gnorm=2.785, clip=0, loss_scale=1024, train_wall=196, wall=45731
2022-07-06 05:25:50 | INFO | train_inner | epoch 017:    771 / 1122 loss=5.702, nll_loss=1.916, mask_ins=0.768, word_ins_ml=3.583, word_reposition=0.784, kpe=0.567, ppl=52.06, wps=8606.4, ups=0.43, wpb=20048.4, bsz=256, num_updates=18700, lr=0.000258544, gnorm=2.785, clip=0, loss_scale=1300, train_wall=195, wall=45964
2022-07-06 05:29:43 | INFO | train_inner | epoch 017:    871 / 1122 loss=5.679, nll_loss=1.894, mask_ins=0.775, word_ins_ml=3.564, word_reposition=0.773, kpe=0.567, ppl=51.24, wps=8552.6, ups=0.43, wpb=19926.3, bsz=256, num_updates=18800, lr=0.000257855, gnorm=2.716, clip=0, loss_scale=2048, train_wall=196, wall=46197
2022-07-06 05:34:25 | INFO | train_inner | epoch 017:    971 / 1122 loss=5.753, nll_loss=1.96, mask_ins=0.775, word_ins_ml=3.623, word_reposition=0.78, kpe=0.575, ppl=53.93, wps=7039, ups=0.35, wpb=19856.2, bsz=256, num_updates=18900, lr=0.000257172, gnorm=2.887, clip=0, loss_scale=2048, train_wall=244, wall=46479
2022-07-06 05:38:19 | INFO | train_inner | epoch 017:   1071 / 1122 loss=nan, nll_loss=1.884, mask_ins=0.767, word_ins_ml=3.555, word_reposition=0.781, kpe=nan, ppl=nan, wps=8538.1, ups=0.43, wpb=19959.4, bsz=256, num_updates=19000, lr=0.000256495, gnorm=2.625, clip=0, loss_scale=2048, train_wall=196, wall=46713
2022-07-06 05:39:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 05:40:17 | INFO | train | epoch 017 | loss nan | nll_loss 1.922 | mask_ins 0.774 | word_ins_ml 3.59 | word_reposition 0.781 | kpe nan | ppl nan | wps 8132.2 | ups 0.41 | wpb 19912.3 | bsz 255.8 | num_updates 19050 | lr 0.000256158 | gnorm 2.76 | clip 0 | loss_scale 1556 | train_wall 2244 | wall 46831
2022-07-06 05:41:22 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.28 | nll_loss 5.871 | mask_ins 1.671 | word_ins_ml 7.215 | word_reposition 1.573 | kpe 1.819 | ppl 4972.12 | wps 14601.1 | wpb 2279.4 | bsz 32 | num_updates 19050 | best_loss 11.892
2022-07-06 05:41:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 17 @ 19050 updates, score 12.28) (writing took 5.504182848148048 seconds)
2022-07-06 05:43:24 | INFO | train_inner | epoch 018:     50 / 1122 loss=5.67, nll_loss=1.899, mask_ins=0.769, word_ins_ml=3.569, word_reposition=0.78, kpe=0.552, ppl=50.92, wps=6458.3, ups=0.33, wpb=19674.3, bsz=253.8, num_updates=19100, lr=0.000255822, gnorm=2.914, clip=0, loss_scale=1267, train_wall=196, wall=47017
2022-07-06 05:47:17 | INFO | train_inner | epoch 018:    150 / 1122 loss=5.563, nll_loss=1.844, mask_ins=0.757, word_ins_ml=3.519, word_reposition=0.774, kpe=0.513, ppl=47.28, wps=8569.9, ups=0.43, wpb=19996.2, bsz=256, num_updates=19200, lr=0.000255155, gnorm=2.557, clip=0, loss_scale=1024, train_wall=195, wall=47251
2022-07-06 05:51:10 | INFO | train_inner | epoch 018:    250 / 1122 loss=5.625, nll_loss=1.892, mask_ins=0.767, word_ins_ml=3.562, word_reposition=0.772, kpe=0.524, ppl=49.34, wps=8582.7, ups=0.43, wpb=19994, bsz=256, num_updates=19300, lr=0.000254493, gnorm=2.655, clip=0, loss_scale=1024, train_wall=196, wall=47484
2022-07-06 05:55:03 | INFO | train_inner | epoch 018:    350 / 1122 loss=5.6, nll_loss=1.868, mask_ins=0.762, word_ins_ml=3.54, word_reposition=0.77, kpe=0.528, ppl=48.5, wps=8506.5, ups=0.43, wpb=19839.8, bsz=256, num_updates=19400, lr=0.000253837, gnorm=2.674, clip=0, loss_scale=1024, train_wall=196, wall=47717
2022-07-06 05:58:57 | INFO | train_inner | epoch 018:    450 / 1122 loss=5.608, nll_loss=1.875, mask_ins=0.76, word_ins_ml=3.546, word_reposition=0.775, kpe=0.528, ppl=48.78, wps=8554.7, ups=0.43, wpb=19975.1, bsz=256, num_updates=19500, lr=0.000253185, gnorm=2.604, clip=0, loss_scale=1024, train_wall=196, wall=47950
2022-07-06 05:59:31 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-06 06:02:52 | INFO | train_inner | epoch 018:    551 / 1122 loss=5.629, nll_loss=1.899, mask_ins=0.758, word_ins_ml=3.567, word_reposition=0.771, kpe=0.534, ppl=49.5, wps=8440.9, ups=0.43, wpb=19853.4, bsz=256, num_updates=19600, lr=0.000252538, gnorm=2.635, clip=0, loss_scale=583, train_wall=197, wall=48186
2022-07-06 06:06:45 | INFO | train_inner | epoch 018:    651 / 1122 loss=nan, nll_loss=1.876, mask_ins=0.766, word_ins_ml=3.548, word_reposition=0.778, kpe=nan, ppl=nan, wps=8516.5, ups=0.43, wpb=19881.9, bsz=256, num_updates=19700, lr=0.000251896, gnorm=2.765, clip=0, loss_scale=512, train_wall=196, wall=48419
2022-07-06 06:10:39 | INFO | train_inner | epoch 018:    751 / 1122 loss=nan, nll_loss=1.88, mask_ins=0.761, word_ins_ml=3.55, word_reposition=0.775, kpe=nan, ppl=nan, wps=8558, ups=0.43, wpb=20010.5, bsz=256, num_updates=19800, lr=0.000251259, gnorm=2.637, clip=0, loss_scale=512, train_wall=196, wall=48653
2022-07-06 06:14:33 | INFO | train_inner | epoch 018:    851 / 1122 loss=5.649, nll_loss=1.886, mask_ins=0.769, word_ins_ml=3.556, word_reposition=0.783, kpe=0.542, ppl=50.17, wps=8570.2, ups=0.43, wpb=20060.5, bsz=256, num_updates=19900, lr=0.000250627, gnorm=2.769, clip=0, loss_scale=512, train_wall=196, wall=48887
2022-07-06 06:18:27 | INFO | train_inner | epoch 018:    951 / 1122 loss=5.591, nll_loss=1.859, mask_ins=0.75, word_ins_ml=3.531, word_reposition=0.767, kpe=0.543, ppl=48.2, wps=8471.8, ups=0.43, wpb=19830.3, bsz=256, num_updates=20000, lr=0.00025, gnorm=2.69, clip=0, loss_scale=512, train_wall=196, wall=49121
2022-07-06 06:22:21 | INFO | train_inner | epoch 018:   1051 / 1122 loss=5.617, nll_loss=1.869, mask_ins=0.76, word_ins_ml=3.54, word_reposition=0.771, kpe=0.546, ppl=49.09, wps=8514, ups=0.43, wpb=19906.3, bsz=256, num_updates=20100, lr=0.000249377, gnorm=2.667, clip=0, loss_scale=896, train_wall=196, wall=49355
2022-07-06 06:25:52 | INFO | train | epoch 018 | loss nan | nll_loss 1.88 | mask_ins 0.762 | word_ins_ml 3.551 | word_reposition 0.774 | kpe nan | ppl nan | wps 8161.8 | ups 0.41 | wpb 19913.7 | bsz 255.8 | num_updates 20171 | lr 0.000248938 | gnorm 2.686 | clip 0 | loss_scale 790 | train_wall 2241 | wall 49566
2022-07-06 06:26:58 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.124 | nll_loss 5.836 | mask_ins 1.639 | word_ins_ml 7.169 | word_reposition 1.582 | kpe 1.735 | ppl 4463.76 | wps 14583 | wpb 2279.4 | bsz 32 | num_updates 20171 | best_loss 11.892
2022-07-06 06:27:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 18 @ 20171 updates, score 12.124) (writing took 5.395052959211171 seconds)
2022-07-06 06:28:11 | INFO | train_inner | epoch 019:     29 / 1122 loss=5.651, nll_loss=1.901, mask_ins=0.77, word_ins_ml=3.569, word_reposition=0.776, kpe=0.537, ppl=50.25, wps=5633.9, ups=0.29, wpb=19699.4, bsz=253.8, num_updates=20200, lr=0.000248759, gnorm=2.824, clip=0, loss_scale=1024, train_wall=241, wall=49704
2022-07-06 06:32:04 | INFO | train_inner | epoch 019:    129 / 1122 loss=5.53, nll_loss=1.845, mask_ins=0.756, word_ins_ml=3.519, word_reposition=0.759, kpe=0.497, ppl=46.21, wps=8450.8, ups=0.43, wpb=19740.1, bsz=256, num_updates=20300, lr=0.000248146, gnorm=2.65, clip=0, loss_scale=1024, train_wall=196, wall=49938
2022-07-06 06:35:57 | INFO | train_inner | epoch 019:    229 / 1122 loss=nan, nll_loss=1.835, mask_ins=0.753, word_ins_ml=3.51, word_reposition=0.762, kpe=nan, ppl=nan, wps=8530.6, ups=0.43, wpb=19869.2, bsz=256, num_updates=20400, lr=0.000247537, gnorm=2.614, clip=0, loss_scale=1024, train_wall=195, wall=50171
2022-07-06 06:39:50 | INFO | train_inner | epoch 019:    329 / 1122 loss=5.585, nll_loss=1.871, mask_ins=0.762, word_ins_ml=3.541, word_reposition=0.77, kpe=0.512, ppl=47.99, wps=8549.4, ups=0.43, wpb=19934.8, bsz=256, num_updates=20500, lr=0.000246932, gnorm=2.807, clip=0, loss_scale=1024, train_wall=195, wall=50404
2022-07-06 06:43:44 | INFO | train_inner | epoch 019:    429 / 1122 loss=5.562, nll_loss=1.857, mask_ins=0.754, word_ins_ml=3.529, word_reposition=0.769, kpe=0.509, ppl=47.24, wps=8494.7, ups=0.43, wpb=19808.4, bsz=256, num_updates=20600, lr=0.000246332, gnorm=2.748, clip=0, loss_scale=1669, train_wall=195, wall=50637
2022-07-06 06:47:37 | INFO | train_inner | epoch 019:    529 / 1122 loss=5.538, nll_loss=1.831, mask_ins=0.755, word_ins_ml=3.506, word_reposition=0.766, kpe=0.511, ppl=46.45, wps=8602.8, ups=0.43, wpb=20059.1, bsz=256, num_updates=20700, lr=0.000245737, gnorm=2.816, clip=0, loss_scale=2048, train_wall=195, wall=50870
2022-07-06 06:51:30 | INFO | train_inner | epoch 019:    629 / 1122 loss=5.532, nll_loss=1.828, mask_ins=0.748, word_ins_ml=3.503, word_reposition=0.767, kpe=0.514, ppl=46.25, wps=8573, ups=0.43, wpb=20010.3, bsz=256, num_updates=20800, lr=0.000245145, gnorm=2.632, clip=0, loss_scale=2048, train_wall=195, wall=51104
2022-07-06 06:55:23 | INFO | train_inner | epoch 019:    729 / 1122 loss=5.552, nll_loss=1.847, mask_ins=0.746, word_ins_ml=3.52, word_reposition=0.77, kpe=0.517, ppl=46.93, wps=8613.6, ups=0.43, wpb=20077.2, bsz=256, num_updates=20900, lr=0.000244558, gnorm=2.722, clip=0, loss_scale=2048, train_wall=195, wall=51337
2022-07-06 06:59:16 | INFO | train_inner | epoch 019:    829 / 1122 loss=5.557, nll_loss=1.845, mask_ins=0.752, word_ins_ml=3.518, word_reposition=0.766, kpe=0.521, ppl=47.08, wps=8568.9, ups=0.43, wpb=19927.2, bsz=256, num_updates=21000, lr=0.000243975, gnorm=2.703, clip=0, loss_scale=2048, train_wall=195, wall=51570
2022-07-06 07:03:09 | INFO | train_inner | epoch 019:    929 / 1122 loss=nan, nll_loss=1.834, mask_ins=0.752, word_ins_ml=3.508, word_reposition=0.76, kpe=nan, ppl=nan, wps=8511.7, ups=0.43, wpb=19856, bsz=256, num_updates=21100, lr=0.000243396, gnorm=2.665, clip=0, loss_scale=3092, train_wall=196, wall=51803
2022-07-06 07:07:02 | INFO | train_inner | epoch 019:   1029 / 1122 loss=5.539, nll_loss=1.839, mask_ins=0.744, word_ins_ml=3.512, word_reposition=0.76, kpe=0.523, ppl=46.5, wps=8571.5, ups=0.43, wpb=19989.6, bsz=256, num_updates=21200, lr=0.000242821, gnorm=2.57, clip=0, loss_scale=4096, train_wall=196, wall=52036
2022-07-06 07:08:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 07:10:39 | INFO | train | epoch 019 | loss nan | nll_loss 1.845 | mask_ins 0.753 | word_ins_ml 3.518 | word_reposition 0.766 | kpe nan | ppl nan | wps 8309.7 | ups 0.42 | wpb 19913.8 | bsz 255.8 | num_updates 21292 | lr 0.000242296 | gnorm 2.69 | clip 0 | loss_scale 2068 | train_wall 2192 | wall 52252
2022-07-06 07:11:44 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 11.87 | nll_loss 5.671 | mask_ins 1.62 | word_ins_ml 7.02 | word_reposition 1.565 | kpe 1.665 | ppl 3742.38 | wps 14581.9 | wpb 2279.4 | bsz 32 | num_updates 21292 | best_loss 11.87
2022-07-06 07:11:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_best.pt (epoch 19 @ 21292 updates, score 11.87) (writing took 9.450428447686136 seconds)
2022-07-06 07:12:12 | INFO | train_inner | epoch 020:      8 / 1122 loss=5.601, nll_loss=1.872, mask_ins=0.763, word_ins_ml=3.541, word_reposition=0.774, kpe=0.523, ppl=48.52, wps=6415.5, ups=0.32, wpb=19857.8, bsz=253.8, num_updates=21300, lr=0.000242251, gnorm=2.675, clip=0, loss_scale=2920, train_wall=197, wall=52346
2022-07-06 07:16:06 | INFO | train_inner | epoch 020:    108 / 1122 loss=5.469, nll_loss=1.818, mask_ins=0.739, word_ins_ml=3.494, word_reposition=0.759, kpe=0.477, ppl=44.29, wps=8502, ups=0.43, wpb=19859.6, bsz=256, num_updates=21400, lr=0.000241684, gnorm=2.622, clip=0, loss_scale=2048, train_wall=196, wall=52579
2022-07-06 07:20:45 | INFO | train_inner | epoch 020:    208 / 1122 loss=5.494, nll_loss=1.828, mask_ins=0.748, word_ins_ml=3.502, word_reposition=0.762, kpe=0.482, ppl=45.06, wps=7138.8, ups=0.36, wpb=19957.3, bsz=256, num_updates=21500, lr=0.000241121, gnorm=2.744, clip=0, loss_scale=2048, train_wall=242, wall=52859
2022-07-06 07:24:39 | INFO | train_inner | epoch 020:    308 / 1122 loss=5.545, nll_loss=1.851, mask_ins=0.765, word_ins_ml=3.522, word_reposition=0.774, kpe=0.484, ppl=46.67, wps=8551, ups=0.43, wpb=19980.3, bsz=256, num_updates=21600, lr=0.000240563, gnorm=2.694, clip=0, loss_scale=2048, train_wall=196, wall=53092
2022-07-06 07:28:28 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 07:28:35 | INFO | train_inner | epoch 020:    409 / 1122 loss=nan, nll_loss=1.815, mask_ins=0.745, word_ins_ml=3.49, word_reposition=0.765, kpe=nan, ppl=nan, wps=8470.7, ups=0.42, wpb=19989.2, bsz=256, num_updates=21700, lr=0.000240008, gnorm=2.613, clip=0, loss_scale=2007, train_wall=198, wall=53328
2022-07-06 07:32:28 | INFO | train_inner | epoch 020:    509 / 1122 loss=5.49, nll_loss=1.806, mask_ins=0.746, word_ins_ml=3.482, word_reposition=0.768, kpe=0.493, ppl=44.94, wps=8524.6, ups=0.43, wpb=19909.1, bsz=256, num_updates=21800, lr=0.000239457, gnorm=2.804, clip=0, loss_scale=1024, train_wall=195, wall=53562
2022-07-06 07:36:22 | INFO | train_inner | epoch 020:    609 / 1122 loss=5.494, nll_loss=1.824, mask_ins=0.745, word_ins_ml=3.498, word_reposition=0.758, kpe=0.494, ppl=45.08, wps=8544.9, ups=0.43, wpb=19956.5, bsz=256, num_updates=21900, lr=0.000238909, gnorm=2.713, clip=0, loss_scale=1024, train_wall=195, wall=53795
2022-07-06 07:40:15 | INFO | train_inner | epoch 020:    709 / 1122 loss=5.493, nll_loss=1.815, mask_ins=0.747, word_ins_ml=3.49, word_reposition=0.759, kpe=0.497, ppl=45.04, wps=8532.2, ups=0.43, wpb=19863, bsz=256, num_updates=22000, lr=0.000238366, gnorm=2.603, clip=0, loss_scale=1024, train_wall=195, wall=54028
2022-07-06 07:44:08 | INFO | train_inner | epoch 020:    809 / 1122 loss=5.472, nll_loss=1.793, mask_ins=0.747, word_ins_ml=3.47, word_reposition=0.759, kpe=0.496, ppl=44.39, wps=8524.6, ups=0.43, wpb=19887.9, bsz=256, num_updates=22100, lr=0.000237826, gnorm=2.762, clip=0, loss_scale=1024, train_wall=195, wall=54262
2022-07-06 07:48:02 | INFO | train_inner | epoch 020:    909 / 1122 loss=nan, nll_loss=1.784, mask_ins=0.746, word_ins_ml=3.462, word_reposition=0.757, kpe=nan, ppl=nan, wps=8552.5, ups=0.43, wpb=19987.3, bsz=256, num_updates=22200, lr=0.000237289, gnorm=2.608, clip=0, loss_scale=1024, train_wall=196, wall=54495
2022-07-06 07:51:55 | INFO | train_inner | epoch 020:   1009 / 1122 loss=5.507, nll_loss=1.829, mask_ins=0.744, word_ins_ml=3.502, word_reposition=0.757, kpe=0.505, ppl=45.47, wps=8512.2, ups=0.43, wpb=19882.9, bsz=256, num_updates=22300, lr=0.000236757, gnorm=2.701, clip=0, loss_scale=1966, train_wall=196, wall=54729
2022-07-06 07:55:48 | INFO | train_inner | epoch 020:   1109 / 1122 loss=5.519, nll_loss=1.837, mask_ins=0.747, word_ins_ml=3.509, word_reposition=0.759, kpe=0.505, ppl=45.84, wps=8548.1, ups=0.43, wpb=19925.9, bsz=256, num_updates=22400, lr=0.000236228, gnorm=2.686, clip=0, loss_scale=2048, train_wall=195, wall=54962
2022-07-06 07:56:18 | INFO | train | epoch 020 | loss nan | nll_loss 1.818 | mask_ins 0.747 | word_ins_ml 3.493 | word_reposition 0.762 | kpe nan | ppl nan | wps 8149.4 | ups 0.41 | wpb 19913.9 | bsz 255.8 | num_updates 22413 | lr 0.000236159 | gnorm 2.689 | clip 0 | loss_scale 1581 | train_wall 2240 | wall 54992
2022-07-06 07:57:23 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.91 | nll_loss 5.783 | mask_ins 1.579 | word_ins_ml 7.124 | word_reposition 1.427 | kpe 1.78 | ppl 3848.25 | wps 14654 | wpb 2279.4 | bsz 32 | num_updates 22413 | best_loss 11.87
2022-07-06 07:57:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 20 @ 22413 updates, score 11.91) (writing took 5.359881019219756 seconds)
2022-07-06 08:00:51 | INFO | train_inner | epoch 021:     87 / 1122 loss=5.415, nll_loss=1.775, mask_ins=0.745, word_ins_ml=3.455, word_reposition=0.756, kpe=0.46, ppl=42.68, wps=6538.7, ups=0.33, wpb=19808.1, bsz=253.8, num_updates=22500, lr=0.000235702, gnorm=2.687, clip=0, loss_scale=2048, train_wall=195, wall=55265
2022-07-06 08:04:44 | INFO | train_inner | epoch 021:    187 / 1122 loss=5.418, nll_loss=1.771, mask_ins=0.745, word_ins_ml=3.45, word_reposition=0.76, kpe=0.463, ppl=42.76, wps=8522.9, ups=0.43, wpb=19870.2, bsz=256, num_updates=22600, lr=0.00023518, gnorm=2.835, clip=0, loss_scale=2048, train_wall=195, wall=55498
2022-07-06 08:05:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 08:06:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 512.0
2022-07-06 08:08:42 | INFO | train_inner | epoch 021:    289 / 1122 loss=5.45, nll_loss=1.81, mask_ins=0.738, word_ins_ml=3.485, word_reposition=0.76, kpe=0.466, ppl=43.7, wps=8406.9, ups=0.42, wpb=20004, bsz=256, num_updates=22700, lr=0.000234662, gnorm=2.75, clip=0, loss_scale=964, train_wall=200, wall=55736
2022-07-06 08:13:24 | INFO | train_inner | epoch 021:    389 / 1122 loss=nan, nll_loss=1.778, mask_ins=0.746, word_ins_ml=3.456, word_reposition=0.753, kpe=nan, ppl=nan, wps=7086.8, ups=0.36, wpb=19933.2, bsz=256, num_updates=22800, lr=0.000234146, gnorm=2.688, clip=0, loss_scale=512, train_wall=243, wall=56017
2022-07-06 08:17:18 | INFO | train_inner | epoch 021:    489 / 1122 loss=nan, nll_loss=1.803, mask_ins=0.743, word_ins_ml=3.479, word_reposition=0.757, kpe=nan, ppl=nan, wps=8554.4, ups=0.43, wpb=20016.1, bsz=256, num_updates=22900, lr=0.000233635, gnorm=2.728, clip=0, loss_scale=512, train_wall=196, wall=56251
2022-07-06 08:21:11 | INFO | train_inner | epoch 021:    589 / 1122 loss=5.434, nll_loss=1.782, mask_ins=0.747, word_ins_ml=3.459, word_reposition=0.752, kpe=0.476, ppl=43.22, wps=8528.2, ups=0.43, wpb=19915.3, bsz=256, num_updates=23000, lr=0.000233126, gnorm=2.715, clip=0, loss_scale=512, train_wall=196, wall=56485
2022-07-06 08:25:05 | INFO | train_inner | epoch 021:    689 / 1122 loss=5.393, nll_loss=1.765, mask_ins=0.732, word_ins_ml=3.444, word_reposition=0.742, kpe=0.474, ppl=42.02, wps=8494.9, ups=0.43, wpb=19825.6, bsz=256, num_updates=23100, lr=0.000232621, gnorm=2.72, clip=0, loss_scale=512, train_wall=196, wall=56718
2022-07-06 08:28:57 | INFO | train_inner | epoch 021:    789 / 1122 loss=5.406, nll_loss=1.767, mask_ins=0.729, word_ins_ml=3.446, word_reposition=0.755, kpe=0.476, ppl=42.4, wps=8585.7, ups=0.43, wpb=19983.3, bsz=256, num_updates=23200, lr=0.000232119, gnorm=2.604, clip=0, loss_scale=748, train_wall=195, wall=56951
2022-07-06 08:32:51 | INFO | train_inner | epoch 021:    889 / 1122 loss=5.431, nll_loss=1.765, mask_ins=0.747, word_ins_ml=3.443, word_reposition=0.758, kpe=0.483, ppl=43.14, wps=8493.9, ups=0.43, wpb=19810, bsz=256, num_updates=23300, lr=0.000231621, gnorm=2.752, clip=0, loss_scale=1024, train_wall=195, wall=57184
2022-07-06 08:36:44 | INFO | train_inner | epoch 021:    989 / 1122 loss=5.414, nll_loss=1.763, mask_ins=0.734, word_ins_ml=3.442, word_reposition=0.755, kpe=0.484, ppl=42.64, wps=8565.2, ups=0.43, wpb=20036.1, bsz=256, num_updates=23400, lr=0.000231125, gnorm=2.654, clip=0, loss_scale=1024, train_wall=196, wall=57418
2022-07-06 08:40:38 | INFO | train_inner | epoch 021:   1089 / 1122 loss=5.463, nll_loss=1.81, mask_ins=0.741, word_ins_ml=3.484, word_reposition=0.751, kpe=0.488, ppl=44.12, wps=8504.2, ups=0.43, wpb=19832.9, bsz=256, num_updates=23500, lr=0.000230633, gnorm=2.731, clip=0, loss_scale=1024, train_wall=195, wall=57651
2022-07-06 08:41:54 | INFO | train | epoch 021 | loss nan | nll_loss 1.781 | mask_ins 0.74 | word_ins_ml 3.458 | word_reposition 0.754 | kpe nan | ppl nan | wps 8151.6 | ups 0.41 | wpb 19914 | bsz 255.8 | num_updates 23533 | lr 0.000230471 | gnorm 2.719 | clip 0 | loss_scale 982 | train_wall 2243 | wall 57728
2022-07-06 08:42:59 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 12.298 | nll_loss 5.835 | mask_ins 1.622 | word_ins_ml 7.177 | word_reposition 1.658 | kpe 1.841 | ppl 5034.61 | wps 14643 | wpb 2279.4 | bsz 32 | num_updates 23533 | best_loss 11.87
2022-07-06 08:43:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 21 @ 23533 updates, score 12.298) (writing took 5.34241917449981 seconds)
2022-07-06 08:45:41 | INFO | train_inner | epoch 022:     67 / 1122 loss=5.369, nll_loss=1.752, mask_ins=0.728, word_ins_ml=3.433, word_reposition=0.752, kpe=0.456, ppl=41.34, wps=6515.9, ups=0.33, wpb=19750.8, bsz=253.8, num_updates=23600, lr=0.000230144, gnorm=2.772, clip=0, loss_scale=1024, train_wall=195, wall=57954
2022-07-06 08:49:34 | INFO | train_inner | epoch 022:    167 / 1122 loss=5.349, nll_loss=1.743, mask_ins=0.727, word_ins_ml=3.424, word_reposition=0.754, kpe=0.444, ppl=40.75, wps=8533.7, ups=0.43, wpb=19923.4, bsz=256, num_updates=23700, lr=0.000229658, gnorm=2.623, clip=0, loss_scale=1372, train_wall=196, wall=58188
2022-07-06 08:53:29 | INFO | train_inner | epoch 022:    267 / 1122 loss=5.341, nll_loss=1.74, mask_ins=0.725, word_ins_ml=3.422, word_reposition=0.747, kpe=0.448, ppl=40.54, wps=8520.8, ups=0.43, wpb=20009.1, bsz=256, num_updates=23800, lr=0.000229175, gnorm=2.722, clip=0, loss_scale=2048, train_wall=197, wall=58423
2022-07-06 08:57:22 | INFO | train_inner | epoch 022:    367 / 1122 loss=5.364, nll_loss=1.759, mask_ins=0.725, word_ins_ml=3.439, word_reposition=0.745, kpe=0.454, ppl=41.18, wps=8574.6, ups=0.43, wpb=19997.2, bsz=256, num_updates=23900, lr=0.000228695, gnorm=2.679, clip=0, loss_scale=2048, train_wall=196, wall=58656
2022-07-06 09:01:16 | INFO | train_inner | epoch 022:    467 / 1122 loss=5.393, nll_loss=1.777, mask_ins=0.73, word_ins_ml=3.453, word_reposition=0.754, kpe=0.455, ppl=42.02, wps=8577.8, ups=0.43, wpb=20023.9, bsz=256, num_updates=24000, lr=0.000228218, gnorm=2.775, clip=0, loss_scale=2048, train_wall=196, wall=58889
2022-07-06 09:05:56 | INFO | train_inner | epoch 022:    567 / 1122 loss=5.374, nll_loss=1.759, mask_ins=0.727, word_ins_ml=3.438, word_reposition=0.751, kpe=0.458, ppl=41.46, wps=7094.4, ups=0.36, wpb=19913.4, bsz=256, num_updates=24100, lr=0.000227744, gnorm=2.852, clip=0, loss_scale=2048, train_wall=243, wall=59170
2022-07-06 09:09:50 | INFO | train_inner | epoch 022:    667 / 1122 loss=5.382, nll_loss=1.767, mask_ins=0.729, word_ins_ml=3.445, word_reposition=0.749, kpe=0.459, ppl=41.71, wps=8564.5, ups=0.43, wpb=20011.1, bsz=256, num_updates=24200, lr=0.000227273, gnorm=2.72, clip=0, loss_scale=2499, train_wall=196, wall=59404
2022-07-06 09:10:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 09:13:46 | INFO | train_inner | epoch 022:    768 / 1122 loss=5.378, nll_loss=1.758, mask_ins=0.731, word_ins_ml=3.437, word_reposition=0.749, kpe=0.462, ppl=41.59, wps=8447.1, ups=0.42, wpb=19935, bsz=256, num_updates=24300, lr=0.000226805, gnorm=2.717, clip=0, loss_scale=2616, train_wall=198, wall=59640
2022-07-06 09:17:40 | INFO | train_inner | epoch 022:    868 / 1122 loss=5.386, nll_loss=1.762, mask_ins=0.728, word_ins_ml=3.44, word_reposition=0.751, kpe=0.466, ppl=41.82, wps=8472.4, ups=0.43, wpb=19777, bsz=256, num_updates=24400, lr=0.000226339, gnorm=2.711, clip=0, loss_scale=2048, train_wall=196, wall=59873
2022-07-06 09:21:33 | INFO | train_inner | epoch 022:    968 / 1122 loss=nan, nll_loss=1.761, mask_ins=0.728, word_ins_ml=3.439, word_reposition=0.75, kpe=nan, ppl=nan, wps=8544.6, ups=0.43, wpb=19949.3, bsz=256, num_updates=24500, lr=0.000225877, gnorm=2.691, clip=0, loss_scale=2048, train_wall=196, wall=60107
2022-07-06 09:25:27 | INFO | train_inner | epoch 022:   1068 / 1122 loss=nan, nll_loss=1.734, mask_ins=0.73, word_ins_ml=3.415, word_reposition=0.749, kpe=nan, ppl=nan, wps=8483.6, ups=0.43, wpb=19817.8, bsz=256, num_updates=24600, lr=0.000225417, gnorm=2.692, clip=0, loss_scale=2048, train_wall=196, wall=60340
2022-07-06 09:27:32 | INFO | train | epoch 022 | loss nan | nll_loss 1.757 | mask_ins 0.728 | word_ins_ml 3.436 | word_reposition 0.751 | kpe nan | ppl nan | wps 8152.6 | ups 0.41 | wpb 19912.8 | bsz 255.8 | num_updates 24654 | lr 0.00022517 | gnorm 2.714 | clip 0 | loss_scale 2018 | train_wall 2244 | wall 60466
2022-07-06 09:28:37 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.999 | nll_loss 5.79 | mask_ins 1.599 | word_ins_ml 7.137 | word_reposition 1.451 | kpe 1.811 | ppl 4092.68 | wps 14644.8 | wpb 2279.4 | bsz 32 | num_updates 24654 | best_loss 11.87
2022-07-06 09:28:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 22 @ 24654 updates, score 11.999) (writing took 5.175104829482734 seconds)
2022-07-06 09:30:29 | INFO | train_inner | epoch 023:     46 / 1122 loss=5.368, nll_loss=1.761, mask_ins=0.729, word_ins_ml=3.439, word_reposition=0.75, kpe=0.45, ppl=41.31, wps=6518.1, ups=0.33, wpb=19728.8, bsz=253.8, num_updates=24700, lr=0.000224961, gnorm=2.696, clip=0, loss_scale=2048, train_wall=195, wall=60643
2022-07-06 09:34:23 | INFO | train_inner | epoch 023:    146 / 1122 loss=5.266, nll_loss=1.699, mask_ins=0.714, word_ins_ml=3.385, word_reposition=0.74, kpe=0.427, ppl=38.47, wps=8576, ups=0.43, wpb=20038.8, bsz=256, num_updates=24800, lr=0.000224507, gnorm=2.576, clip=0, loss_scale=3297, train_wall=196, wall=60877
2022-07-06 09:38:17 | INFO | train_inner | epoch 023:    246 / 1122 loss=5.308, nll_loss=1.725, mask_ins=0.727, word_ins_ml=3.407, word_reposition=0.74, kpe=0.433, ppl=39.6, wps=8545.1, ups=0.43, wpb=19962.1, bsz=256, num_updates=24900, lr=0.000224055, gnorm=2.575, clip=0, loss_scale=4096, train_wall=196, wall=61110
2022-07-06 09:42:11 | INFO | train_inner | epoch 023:    346 / 1122 loss=nan, nll_loss=1.757, mask_ins=0.729, word_ins_ml=3.435, word_reposition=0.751, kpe=nan, ppl=nan, wps=8535.7, ups=0.43, wpb=19980.2, bsz=256, num_updates=25000, lr=0.000223607, gnorm=2.819, clip=0, loss_scale=4096, train_wall=196, wall=61344
2022-07-06 09:42:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 09:46:07 | INFO | train_inner | epoch 023:    447 / 1122 loss=5.305, nll_loss=1.724, mask_ins=0.718, word_ins_ml=3.406, word_reposition=0.747, kpe=0.434, ppl=39.54, wps=8422.6, ups=0.42, wpb=19873.8, bsz=256, num_updates=25100, lr=0.000223161, gnorm=2.592, clip=0, loss_scale=2190, train_wall=198, wall=61580
2022-07-06 09:46:09 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1024.0
2022-07-06 09:50:02 | INFO | train_inner | epoch 023:    548 / 1122 loss=nan, nll_loss=1.743, mask_ins=0.733, word_ins_ml=3.423, word_reposition=0.75, kpe=nan, ppl=nan, wps=8467.5, ups=0.43, wpb=19922.6, bsz=256, num_updates=25200, lr=0.000222718, gnorm=2.727, clip=0, loss_scale=1024, train_wall=197, wall=61816
2022-07-06 09:54:07 | INFO | train_inner | epoch 023:    648 / 1122 loss=5.271, nll_loss=1.698, mask_ins=0.715, word_ins_ml=3.382, word_reposition=0.734, kpe=0.44, ppl=38.62, wps=8162.5, ups=0.41, wpb=19969.5, bsz=256, num_updates=25300, lr=0.000222277, gnorm=2.584, clip=0, loss_scale=1024, train_wall=207, wall=62060
2022-07-06 09:58:36 | INFO | train_inner | epoch 023:    748 / 1122 loss=5.321, nll_loss=1.734, mask_ins=0.717, word_ins_ml=3.415, word_reposition=0.744, kpe=0.445, ppl=39.96, wps=7375.7, ups=0.37, wpb=19852.6, bsz=256, num_updates=25400, lr=0.000221839, gnorm=2.601, clip=0, loss_scale=1024, train_wall=231, wall=62329
2022-07-06 10:02:29 | INFO | train_inner | epoch 023:    848 / 1122 loss=5.356, nll_loss=1.748, mask_ins=0.728, word_ins_ml=3.426, word_reposition=0.752, kpe=0.449, ppl=40.94, wps=8523.9, ups=0.43, wpb=19905.1, bsz=256, num_updates=25500, lr=0.000221404, gnorm=2.795, clip=0, loss_scale=1024, train_wall=196, wall=62563
2022-07-06 10:06:23 | INFO | train_inner | epoch 023:    948 / 1122 loss=5.354, nll_loss=1.748, mask_ins=0.726, word_ins_ml=3.427, word_reposition=0.751, kpe=0.451, ppl=40.9, wps=8601.6, ups=0.43, wpb=20062.1, bsz=256, num_updates=25600, lr=0.000220971, gnorm=2.703, clip=0, loss_scale=1024, train_wall=196, wall=62796
2022-07-06 10:10:16 | INFO | train_inner | epoch 023:   1048 / 1122 loss=5.34, nll_loss=1.755, mask_ins=0.717, word_ins_ml=3.433, word_reposition=0.74, kpe=0.45, ppl=40.52, wps=8475.3, ups=0.43, wpb=19760.2, bsz=256, num_updates=25700, lr=0.000220541, gnorm=2.679, clip=0, loss_scale=1935, train_wall=195, wall=63029
2022-07-06 10:13:08 | INFO | train | epoch 023 | loss nan | nll_loss 1.733 | mask_ins 0.722 | word_ins_ml 3.414 | word_reposition 0.745 | kpe nan | ppl nan | wps 8153.3 | ups 0.41 | wpb 19914.5 | bsz 255.8 | num_updates 25774 | lr 0.000220224 | gnorm 2.669 | clip 0 | loss_scale 2070 | train_wall 2242 | wall 63201
2022-07-06 10:14:13 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 12.052 | nll_loss 5.715 | mask_ins 1.617 | word_ins_ml 7.059 | word_reposition 1.567 | kpe 1.809 | ppl 4245.97 | wps 14642 | wpb 2279.4 | bsz 32 | num_updates 25774 | best_loss 11.87
2022-07-06 10:14:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 23 @ 25774 updates, score 12.052) (writing took 5.301960023120046 seconds)
2022-07-06 10:15:18 | INFO | train_inner | epoch 024:     26 / 1122 loss=5.312, nll_loss=1.726, mask_ins=0.725, word_ins_ml=3.407, word_reposition=0.738, kpe=0.441, ppl=39.72, wps=6489.7, ups=0.33, wpb=19638.8, bsz=253.8, num_updates=25800, lr=0.000220113, gnorm=2.691, clip=0, loss_scale=2048, train_wall=195, wall=63332
2022-07-06 10:19:11 | INFO | train_inner | epoch 024:    126 / 1122 loss=nan, nll_loss=1.675, mask_ins=0.716, word_ins_ml=3.362, word_reposition=0.737, kpe=nan, ppl=nan, wps=8521, ups=0.43, wpb=19836.7, bsz=256, num_updates=25900, lr=0.000219687, gnorm=2.623, clip=0, loss_scale=2048, train_wall=195, wall=63565
2022-07-06 10:23:05 | INFO | train_inner | epoch 024:    226 / 1122 loss=5.228, nll_loss=1.679, mask_ins=0.709, word_ins_ml=3.365, word_reposition=0.735, kpe=0.418, ppl=37.48, wps=8523.2, ups=0.43, wpb=19904, bsz=256, num_updates=26000, lr=0.000219265, gnorm=2.738, clip=0, loss_scale=2048, train_wall=196, wall=63798
2022-07-06 10:26:58 | INFO | train_inner | epoch 024:    326 / 1122 loss=5.227, nll_loss=1.677, mask_ins=0.707, word_ins_ml=3.363, word_reposition=0.737, kpe=0.42, ppl=37.45, wps=8541.3, ups=0.43, wpb=19890.2, bsz=256, num_updates=26100, lr=0.000218844, gnorm=2.706, clip=0, loss_scale=2048, train_wall=195, wall=64031
2022-07-06 10:30:51 | INFO | train_inner | epoch 024:    426 / 1122 loss=nan, nll_loss=1.69, mask_ins=0.725, word_ins_ml=3.375, word_reposition=0.747, kpe=nan, ppl=nan, wps=8569, ups=0.43, wpb=20004.9, bsz=256, num_updates=26200, lr=0.000218426, gnorm=2.568, clip=0, loss_scale=3625, train_wall=196, wall=64265
2022-07-06 10:31:54 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 10:34:48 | INFO | train_inner | epoch 024:    527 / 1122 loss=5.263, nll_loss=1.702, mask_ins=0.712, word_ins_ml=3.385, word_reposition=0.738, kpe=0.428, ppl=38.41, wps=8449.3, ups=0.42, wpb=20012.9, bsz=256, num_updates=26300, lr=0.00021801, gnorm=2.685, clip=0, loss_scale=2575, train_wall=198, wall=64501
2022-07-06 10:38:42 | INFO | train_inner | epoch 024:    627 / 1122 loss=5.224, nll_loss=1.653, mask_ins=0.718, word_ins_ml=3.342, word_reposition=0.735, kpe=0.428, ppl=37.37, wps=8504.2, ups=0.43, wpb=19875.7, bsz=256, num_updates=26400, lr=0.000217597, gnorm=2.656, clip=0, loss_scale=2048, train_wall=196, wall=64735
2022-07-06 10:42:35 | INFO | train_inner | epoch 024:    727 / 1122 loss=5.311, nll_loss=1.744, mask_ins=0.715, word_ins_ml=3.423, word_reposition=0.742, kpe=0.431, ppl=39.7, wps=8533.3, ups=0.43, wpb=19910.8, bsz=256, num_updates=26500, lr=0.000217186, gnorm=2.683, clip=0, loss_scale=2048, train_wall=196, wall=64969
2022-07-06 10:46:40 | INFO | train_inner | epoch 024:    827 / 1122 loss=5.258, nll_loss=1.68, mask_ins=0.72, word_ins_ml=3.366, word_reposition=0.743, kpe=0.43, ppl=38.26, wps=8149.2, ups=0.41, wpb=19979.9, bsz=256, num_updates=26600, lr=0.000216777, gnorm=2.616, clip=0, loss_scale=2048, train_wall=207, wall=65214
2022-07-06 10:51:09 | INFO | train_inner | epoch 024:    927 / 1122 loss=5.291, nll_loss=1.702, mask_ins=0.728, word_ins_ml=3.385, word_reposition=0.741, kpe=0.438, ppl=39.16, wps=7431.3, ups=0.37, wpb=19993.2, bsz=256, num_updates=26700, lr=0.000216371, gnorm=2.722, clip=0, loss_scale=2048, train_wall=231, wall=65483
2022-07-06 10:55:03 | INFO | train_inner | epoch 024:   1027 / 1122 loss=5.313, nll_loss=1.713, mask_ins=0.73, word_ins_ml=3.394, word_reposition=0.748, kpe=0.44, ppl=39.75, wps=8509.8, ups=0.43, wpb=19879.3, bsz=256, num_updates=26800, lr=0.000215967, gnorm=2.668, clip=0, loss_scale=3338, train_wall=196, wall=65716
2022-07-06 10:58:44 | INFO | train | epoch 024 | loss nan | nll_loss 1.694 | mask_ins 0.718 | word_ins_ml 3.378 | word_reposition 0.74 | kpe nan | ppl nan | wps 8158.8 | ups 0.41 | wpb 19913.8 | bsz 255.8 | num_updates 26895 | lr 0.000215585 | gnorm 2.679 | clip 0 | loss_scale 2524 | train_wall 2243 | wall 65937
2022-07-06 10:59:49 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 12.182 | nll_loss 5.714 | mask_ins 1.617 | word_ins_ml 7.057 | word_reposition 1.56 | kpe 1.947 | ppl 4646.64 | wps 14635.9 | wpb 2279.4 | bsz 32 | num_updates 26895 | best_loss 11.87
2022-07-06 10:59:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ../checkpoints_bert_transformer_kpe_uncased/checkpoint_last.pt (epoch 24 @ 26895 updates, score 12.182) (writing took 5.838526316918433 seconds)
2022-07-06 11:00:06 | INFO | train_inner | epoch 025:      5 / 1122 loss=5.281, nll_loss=1.704, mask_ins=0.715, word_ins_ml=3.387, word_reposition=0.74, kpe=0.439, ppl=38.88, wps=6539.9, ups=0.33, wpb=19856.3, bsz=253.8, num_updates=26900, lr=0.000215565, gnorm=2.794, clip=0, loss_scale=4096, train_wall=195, wall=66020
2022-07-06 11:04:00 | INFO | train_inner | epoch 025:    105 / 1122 loss=5.232, nll_loss=1.697, mask_ins=0.707, word_ins_ml=3.38, word_reposition=0.739, kpe=0.406, ppl=37.58, wps=8561.7, ups=0.43, wpb=19983.4, bsz=256, num_updates=27000, lr=0.000215166, gnorm=2.766, clip=0, loss_scale=4096, train_wall=195, wall=66253
2022-07-06 11:07:53 | INFO | train_inner | epoch 025:    205 / 1122 loss=5.191, nll_loss=1.664, mask_ins=0.706, word_ins_ml=3.351, word_reposition=0.73, kpe=0.404, ppl=36.52, wps=8516.5, ups=0.43, wpb=19899, bsz=256, num_updates=27100, lr=0.000214768, gnorm=2.55, clip=0, loss_scale=4096, train_wall=196, wall=66487
2022-07-06 11:11:47 | INFO | train_inner | epoch 025:    305 / 1122 loss=5.223, nll_loss=1.678, mask_ins=0.715, word_ins_ml=3.363, word_reposition=0.738, kpe=0.407, ppl=37.36, wps=8587.5, ups=0.43, wpb=20034, bsz=256, num_updates=27200, lr=0.000214373, gnorm=2.725, clip=0, loss_scale=4096, train_wall=195, wall=66720
2022-07-06 11:12:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2022-07-06 11:15:43 | INFO | train_inner | epoch 025:    406 / 1122 loss=5.228, nll_loss=1.69, mask_ins=0.71, word_ins_ml=3.374, word_reposition=0.733, kpe=0.411, ppl=37.47, wps=8471.1, ups=0.42, wpb=19991.8, bsz=256, num_updates=27300, lr=0.00021398, gnorm=2.592, clip=0, loss_scale=2149, train_wall=197, wall=66956
Traceback (most recent call last):
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/data/yukangliang/实验/BertKpeEditorWithAdaptor/fairseq_cli/train.py", line 369, in cli_main
    torch.multiprocessing.spawn(
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 130, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/data/yukangliang/anaconda3/envs/BertKpeEditorWithAdaptor/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
